"""
Base classes for conversational agents.

Every agent inherits from 'Agent' and overrides 'answer_stream'. The toolkit
ships three concrete implementations: 'RAG' for retrieval-augmented generation,
'Router' for LLM-based query routing across multiple agents, and 'ToolAgent'
for ReAct-style agentic loops with tool calling.
"""

from abc import ABC, abstractmethod
from typing import Any, AsyncGenerator, Sequence

from pydantic import BaseModel, Field

from conversational_toolkit.llms.base import LLM, LLMMessage, Roles
from conversational_toolkit.vectorstores.base import ChunkRecord


class QueryWithContext(BaseModel):
    """A user query paired with the prior conversation history passed to an agent."""

    query: str
    history: list[LLMMessage]


class AgentAnswer(LLMMessage):
    """
    Structured response produced by every agent.

    Extends 'LLMMessage' with agent-specific metadata so the streaming
    infrastructure can treat intermediate content chunks and the final answer
    uniformly. 'sources' carries the retrieved document chunks that informed the
    answer; 'follow_up_questions' holds suggestions generated by the agent;
    'step_by_step_thinking' exposes the chain-of-thought reasoning when available.
    """

    step_by_step_thinking: str = ""
    sources: Sequence[ChunkRecord] = Field(default_factory=lambda: [])
    follow_up_questions: Sequence[str] = Field(default_factory=lambda: [])


class Agent(ABC):
    """
    Abstract base class for all conversational agents.

    Agents turn a 'QueryWithContext' into a stream of 'AgentAnswer' objects.
    The async-generator interface ('answer_stream') is the primary API so partial
    responses can be forwarded to the client before the full answer is ready.

    The ABC provides shared helpers ('build_tool_answer', 'answer',
    '_answer_post_processing') that all subclasses benefit from, while still
    enforcing that every concrete agent implements the core streaming method.

    Attributes:
        description: Human-readable text used by 'Router' to decide which agent
            should handle a given query.
        system_prompt: Instruction text prepended to every LLM conversation.
        llm: The language model used for generation.
        max_steps: Maximum agentic loop iterations, relevant for 'ToolAgent'.
    """

    def __init__(self, system_prompt: str, llm: LLM, description: str = "", max_steps: int = 20):
        self.description = description
        self.system_prompt = system_prompt
        self.llm = llm
        self.max_steps = max_steps

    def build_tool_answer(self, tool_call_id: str, function_name: str, function_response: dict[str, Any]) -> LLMMessage:
        """Format a tool execution result as an 'LLMMessage' with role 'TOOL'.

        This is the standard way to feed tool outputs back into the LLM
        conversation in the OpenAI function-calling format.
        """
        return LLMMessage(
            tool_call_id=tool_call_id,
            role=Roles.TOOL,
            name=function_name,
            content=str(function_response),
        )

    async def answer(self, query_with_context: QueryWithContext) -> AgentAnswer:
        """Non-streaming convenience wrapper around 'answer_stream'.

        Consumes the entire stream and returns only the last yielded
        'AgentAnswer'. Raises 'ValueError' if the stream yields nothing.
        """
        response_message = None
        async for message in self.answer_stream(query_with_context):
            response_message = message

        if response_message is None:
            raise ValueError("No response received from the agent.")

        return response_message

    @abstractmethod
    def answer_stream(self, query_with_context: QueryWithContext) -> AsyncGenerator[AgentAnswer, None]:
        """Stream 'AgentAnswer' objects for the given query.

        Implementations yield partial answers as content is generated so the API
        layer can forward them to the client in real time. The last yielded value
        must contain the complete answer with all sources and follow-up questions.
        """
        pass

    async def _answer_post_processing(self, answer: AgentAnswer) -> AgentAnswer:
        """Optional hook called on each streamed answer chunk before it is yielded.

        Override in a subclass to transform answers (e.g. add metadata, filter
        content). The default implementation is a no-op pass-through.
        """
        return answer
