{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# Ingestion and the Vector Database\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is looking more closely at the vector database, how to query it and especially its metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98d7fd",
   "metadata": {},
   "source": [
    "Minimal preparations, assuming the /backend/data_vs.db has been written already. If not, execute the steps at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75210c80",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Forbidden).)."
     ]
    }
   ],
   "source": [
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from pathlib import Path\n",
    "\n",
    "VS_PATH = Path(\"/home/roger/Documents/DEV/sme-kt-zh-collaboration-rag/backend/data_vs.db\")\n",
    "\n",
    "vs = ChromaDBVectorStore(db_path=str(VS_PATH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8516056",
   "metadata": {},
   "source": [
    "#Query Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4abe16e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Failed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable. (Forbidden).)."
     ]
    }
   ],
   "source": [
    "results = vs.collection.get(include=[\"metadatas\"])\n",
    "for meta in results[\"metadatas\"][:10]:\n",
    "    print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d746ec1",
   "metadata": {},
   "source": [
    "or formatted a little nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef10dc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = vs.collection.get(include=[\"metadatas\"])\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"id\": chunk_id, **meta}\n",
    "    for chunk_id, meta in zip(results[\"ids\"], results[\"metadatas\"])\n",
    "])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd570da",
   "metadata": {},
   "source": [
    "## Updating Chunk Metadata After Ingestion\n",
    "\n",
    "ChromaDB allows you to add or modify metadata fields on already-stored chunks\n",
    "using `collection.update()`. This is useful for backfilling information you didn't\n",
    "have at ingestion time — for example, a review status, a quality score, or (as we\n",
    "will see later) a file hash for deduplication.\n",
    "\n",
    "> **Important:** `collection.update()` replaces the entire metadata dict for each\n",
    "> chunk. Always merge the existing metadata with your new field using `{**meta, \"new_key\": value}`\n",
    "> to avoid wiping existing fields.\n",
    "\n",
    "The cells below add a dummy `\"demo\"` field to every chunk as a minimal example,\n",
    "then verify the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba17752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 374 chunks\n"
     ]
    }
   ],
   "source": [
    "# Get all existing IDs and metadata\n",
    "results = vs.collection.get(include=[\"metadatas\"])\n",
    "\n",
    "# Merge existing metadata with the new field\n",
    "updated_metadatas = [\n",
    "    {**meta, \"demo\": \"test\"}\n",
    "    for meta in results[\"metadatas\"]\n",
    "]\n",
    "\n",
    "vs.collection.update(ids=results[\"ids\"], metadatas=updated_metadatas)\n",
    "print(f\"Updated {len(results['ids'])} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2e1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mime_type</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>demo</th>\n",
       "      <th>source_file</th>\n",
       "      <th>chapters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td># Supplier Sustainability Requirements</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>## 1. Purpose and Scope</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>## 2. Evidence Standards</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>## 3. Requirements by Category</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>### 3.1 All Suppliers and Products</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mime_type                                   title  \\\n",
       "0  text/markdown  # Supplier Sustainability Requirements   \n",
       "1  text/markdown                 ## 1. Purpose and Scope   \n",
       "2  text/markdown                ## 2. Evidence Standards   \n",
       "3  text/markdown          ## 3. Requirements by Category   \n",
       "4  text/markdown      ### 3.1 All Suppliers and Products   \n",
       "\n",
       "                                source  demo  \\\n",
       "0  ART_internal_procurement_policy.pdf  test   \n",
       "1  ART_internal_procurement_policy.pdf  test   \n",
       "2  ART_internal_procurement_policy.pdf  test   \n",
       "3  ART_internal_procurement_policy.pdf  test   \n",
       "4  ART_internal_procurement_policy.pdf  test   \n",
       "\n",
       "                           source_file  \\\n",
       "0  ART_internal_procurement_policy.pdf   \n",
       "1  ART_internal_procurement_policy.pdf   \n",
       "2  ART_internal_procurement_policy.pdf   \n",
       "3  ART_internal_procurement_policy.pdf   \n",
       "4  ART_internal_procurement_policy.pdf   \n",
       "\n",
       "                                            chapters  \n",
       "0         [\"# Supplier Sustainability Requirements\"]  \n",
       "1  [\"# Supplier Sustainability Requirements\", \"##...  \n",
       "2  [\"# Supplier Sustainability Requirements\", \"##...  \n",
       "3  [\"# Supplier Sustainability Requirements\", \"##...  \n",
       "4  [\"# Supplier Sustainability Requirements\", \"##...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = vs.collection.get(ids=results[\"ids\"][:5], include=[\"metadatas\"])\n",
    "pd.DataFrame(sample[\"metadatas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24973fa",
   "metadata": {},
   "source": [
    "## Idempotence of the ingestion / Deduplication via File Hashing\n",
    "\n",
    "A common problem in document ingestion pipelines is **accidentally re-ingesting\n",
    "the same file twice** — for example when adding new\n",
    "documents to the data folder again, or in case of duplicates with different filenames in the document corpus. This bloats the vector\n",
    "store with duplicate chunks and degrades retrieval quality.\n",
    "\n",
    "### The idea\n",
    "\n",
    "Instead of tracking filenames (which can change), we compute a **SHA-256 hash\n",
    "of the file's raw bytes**. This hash is a unique fingerprint of the file's\n",
    "content — if the content hasn't changed, the hash won't change either.\n",
    "\n",
    "The strategy has three steps:\n",
    "\n",
    "1. **Hash** — Before ingesting a file, compute its SHA-256 hash.\n",
    "2. **Check** — Query the vector store for any chunk that already carries that\n",
    "   hash as metadata. If one exists, the file is already ingested — skip it.\n",
    "3. **Stamp** — If the file is new, add the hash as a `\"file_hash\"` metadata\n",
    "   field on every chunk before inserting them. Future runs can then detect it.\n",
    "\n",
    "This approach correctly handles renamed files (same hash → skip) and detects\n",
    "modified files (different hash → re-ingest). Disadvantage: the filenames and folder path of the duplicates will be lost.\n",
    "\n",
    "The cells below demonstrate all three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Hash the file:\n",
    "\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def file_hash(path: str) -> str:\n",
    "    return hashlib.sha256(Path(path).read_bytes()).hexdigest()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe93dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2. Stamp it onto chunks before ingestion (this is where you'd extend load_chunks):\n",
    "\n",
    "\n",
    "for chunk in file_chunks:\n",
    "    chunk.metadata[\"file_hash\"] = file_hash(str(file_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3453571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3. Check for duplicates before ingesting a file:\n",
    "\n",
    "\n",
    "def already_ingested(collection, hash_value: str) -> bool:\n",
    "    results = collection.get(where={\"file_hash\": hash_value}, limit=1)\n",
    "    return len(results[\"ids\"]) > 0\n",
    "Then the guard becomes:\n",
    "\n",
    "\n",
    "hash_value = file_hash(str(file_path))\n",
    "if already_ingested(vs.collection, hash_value):\n",
    "    print(f\"Skipping {file_path.name} — already in store\")\n",
    "else:\n",
    "    # embed and insert chunks\n",
    "# The key thing: ChromaDB's where= filter on .get() lets you query by any metadata field, so the hash becomes a good lookup key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c3061",
   "metadata": {},
   "source": [
    "## Proposed Changes to feature0_baseline_rag.py\n",
    "\n",
    "Two functions need to be updated to support incremental, hash-based ingestion.\n",
    "\n",
    "**`load_chunks()`** gets a small addition: a `file_hash()` helper is introduced,\n",
    "and its result is stamped onto every chunk's metadata as `\"file_hash\"` at load\n",
    "time. All chunks from the same file share the same hash value.\n",
    "\n",
    "**`build_vector_store()`** replaces the all-or-nothing empty-store check with a\n",
    "per-file check. Incoming chunks are grouped by their `\"file_hash\"`. For each\n",
    "group, the store is queried — if a chunk with that hash already exists, the whole\n",
    "file is skipped. Only genuinely new files are embedded and inserted. This makes\n",
    "the pipeline fully incremental: drop new files into `data/` and re-run without\n",
    "touching what is already stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(path: Path) -> str:\n",
    "    \"\"\"SHA-256 fingerprint of a file's raw bytes.\"\"\"\n",
    "    return hashlib.sha256(path.read_bytes()).hexdigest()\n",
    "\n",
    "\n",
    "def load_chunks(max_files: int | None = None, existing_hashes: set[str] | None = None) -> list[Chunk]:\n",
    "    \"\"\"Load documents from DATA_DIR and split them into chunks.\n",
    "\n",
    "    Supported formats:\n",
    "        .pdf: converted to Markdown via pymupdf4llm, split on headings\n",
    "        .xlsx, .xls: one chunk per sheet (Markdown table)\n",
    "\n",
    "    Unsupported formats (e.g. standalone images) are logged as warnings and skipped.\n",
    "    Images embedded inside PDFs are not extracted as text by default!\n",
    "\n",
    "    Pass 'max_files' to cap the total number of files processed. Useful for quick\n",
    "    iteration during development before scaling to all files.\n",
    "    \"\"\"\n",
    "    all_chunks: list[Chunk] = []\n",
    "    all_files = sorted(f for f in DATA_DIR.iterdir() if f.is_file())\n",
    "\n",
    "    if max_files is not None:\n",
    "        all_files = all_files[:max_files]\n",
    "        print(len(all_files))\n",
    "\n",
    "    for f in all_files:\n",
    "        ext = f.suffix.lower()\n",
    "        if ext not in _CHUNKERS:\n",
    "            if ext in _IMAGE_EXTENSIONS:\n",
    "                logger.warning(f\"Skipping image file (not supported): {f.name}\")\n",
    "            else:\n",
    "                logger.warning(f\"Skipping unsupported file type {ext!r}: {f.name}\")\n",
    "\n",
    "    supported_files = [f for f in all_files if f.suffix.lower() in _CHUNKERS]\n",
    "    logger.info(f\"Chunking {len(supported_files)} files from {DATA_DIR}\")\n",
    "\n",
    "    seen_hashes: set[str] = set()\n",
    "    for file_path in supported_files:\n",
    "        hash_value = file_hash(file_path)\n",
    "        if existing_hashes and hash_value in existing_hashes:\n",
    "            logger.info(f\"Skipping {file_path.name!r} — already in store (hash={hash_value[:8]}…)\")\n",
    "            continue\n",
    "        if hash_value in seen_hashes:\n",
    "            logger.warning(f\"Skipping {file_path.name!r} — duplicate content in current batch (hash={hash_value[:8]}…)\")\n",
    "            continue\n",
    "        seen_hashes.add(hash_value)\n",
    "        chunker = _CHUNKERS[file_path.suffix.lower()]\n",
    "        try:\n",
    "            file_chunks = chunker.make_chunks(str(file_path))\n",
    "            for chunk in file_chunks:\n",
    "                chunk.metadata[\"file_hash\"] = hash_value\n",
    "                chunk.metadata[\"source_file\"] = file_path.name\n",
    "                chunk.metadata[\"source\"] = file_path.name\n",
    "                chunk.metadata[\"title\"] = chunk.title\n",
    "            all_chunks.extend(file_chunks)\n",
    "            logger.debug(f\"  {file_path.name}: {len(file_chunks)} chunks\")\n",
    "        except Exception as exc:\n",
    "            logger.warning(f\"Skipping {file_path.name}: {exc}\")\n",
    "\n",
    "    logger.info(f\"Done, {len(all_chunks)} chunks total\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_hashes(db_path: Path = VS_PATH) -> set[str]:\n",
    "    \"\"\"Return file hashes already present in the vector store.\"\"\"\n",
    "    if not db_path.exists():\n",
    "        return set()\n",
    "    vs = ChromaDBVectorStore(db_path=str(db_path))\n",
    "    result = vs.collection.get(include=[\"metadatas\"])\n",
    "    return {m[\"file_hash\"] for m in result[\"metadatas\"] if \"file_hash\" in m}\n",
    "\n",
    "\n",
    "async def build_vector_store(\n",
    "    chunks: list[Chunk],\n",
    "    embedding_model: SentenceTransformerEmbeddings,\n",
    "    db_path: Path = VS_PATH,\n",
    "    reset: bool = False,\n",
    ") -> ChromaDBVectorStore:\n",
    "    \"\"\"Embed 'chunks' and persist them in a ChromaDB vector store.\n",
    "\n",
    "    Set 'reset=True' to delete and rebuild the store from scratch. Leave\n",
    "    'reset=False' (default) to reuse an existing store, embedding all documents\n",
    "    takes time; skipping it on subsequent runs saves time.\n",
    "    \"\"\"\n",
    "    if reset and db_path.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(db_path)\n",
    "        logger.info(f\"Deleted existing vector store at {db_path}\")\n",
    "\n",
    "    vector_store = ChromaDBVectorStore(db_path=str(db_path))\n",
    "\n",
    "    # Group chunks by file hash for per-file deduplication\n",
    "    chunks_by_hash: dict[str, list[Chunk]] = {}\n",
    "    for chunk in chunks:\n",
    "        h = chunk.metadata.get(\"file_hash\", \"unknown\")\n",
    "        chunks_by_hash.setdefault(h, []).append(chunk)\n",
    "\n",
    "    # Warn about same-content files in the current batch\n",
    "    for hash_value, file_chunks in chunks_by_hash.items():\n",
    "        sources = list(dict.fromkeys(c.metadata.get(\"source_file\", \"?\") for c in file_chunks))\n",
    "        if len(sources) > 1:\n",
    "            logger.warning(f\"Duplicate content detected across files (hash={hash_value[:8]}…): {sources} — only ingesting once.\")\n",
    "\n",
    "    new_chunks: list[Chunk] = []\n",
    "    skipped_files = 0\n",
    "    skipped_chunks = 0\n",
    "    for hash_value, file_chunks in chunks_by_hash.items():\n",
    "        existing = vector_store.collection.get(where={\"file_hash\": hash_value}, limit=1)\n",
    "        if existing[\"ids\"]:\n",
    "            source = file_chunks[0].metadata.get(\"source_file\", \"?\")\n",
    "            logger.info(f\"Skipping {source!r} — already in store (hash={hash_value[:8]}…)\")\n",
    "            skipped_files += 1\n",
    "            skipped_chunks += len(file_chunks)\n",
    "        else:\n",
    "            new_chunks.extend(file_chunks)\n",
    "    logger.info(\n",
    "        f\"Deduplication: {skipped_files} file(s) / {skipped_chunks} chunk(s) skipped, \"\n",
    "        f\"{len(chunks_by_hash) - skipped_files} file(s) / {len(new_chunks)} chunk(s) to embed.\"\n",
    "    )\n",
    "\n",
    "    if not new_chunks:\n",
    "        logger.info(\"All files already in store — nothing to embed.\")\n",
    "        return vector_store\n",
    "\n",
    "    logger.info(f\"Embedding {len(new_chunks)} new chunks with {embedding_model.model_name!r} …\")\n",
    "    embeddings = await embedding_model.get_embeddings([c.content for c in new_chunks])\n",
    "    logger.info(f\"Embedding matrix: shape={embeddings.shape}  dtype={embeddings.dtype}\")\n",
    "    await vector_store.insert_chunks(chunks=new_chunks, embedding=embeddings)\n",
    "    logger.info(f\"Done! Vector store written to {db_path}\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37244449",
   "metadata": {},
   "source": [
    "In the Notebook, we need to give the existing_hashes to the load_chunks function. Also the calculation can lead to a division by 0, which needs to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78599c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from DATA_DIR and split them into chunks.\n",
    "existing_hashes = get_existing_hashes()\n",
    "chunks = load_chunks(max_files=None, existing_hashes=existing_hashes)\n",
    "# Print a statistical summary and sampled content for visual inspection.\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Print size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "if char_lengths:\n",
    "    print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "    print(f\"Over 1024-char limit (≈256 tok embedding limit): {over_limit} / {len(chunks)}\")\n",
    "else:\n",
    "    print(\"All files already in store — no new chunks loaded.\")\n",
    "print(f\"Over 1024-char limit (≈256 tok embedding limit): {over_limit} / {len(chunks)}\")\n",
    "print(\"\\nSuccessfully loaded and chunked the documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24f150",
   "metadata": {},
   "source": [
    "Helper to release the database, to prevent locks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61f489aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mvs\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'vs' is not defined"
     ]
    }
   ],
   "source": [
    "del vs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME RAG",
   "language": "python",
   "name": "sme-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
