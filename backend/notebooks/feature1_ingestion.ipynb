{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# Ingestion and the Vector Database\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is looking more closely at the vector database, how to query it and especially its metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98d7fd",
   "metadata": {},
   "source": [
    "Minimal preparations, assuming the /backend/data_vs.db has been written already. If not, execute the steps at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75210c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory:/home/roger/Documents/DEV/sme-kt-zh-collaboration-rag\n",
      "Path Variable:/home/roger/Documents/DEV/sme-kt-zh-collaboration-rag/backend/data_vs.db\n"
     ]
    }
   ],
   "source": [
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from pathlib import Path\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2️⃣ Build the absolute path to the DB file\n",
    "# ----------------------------------------------------------------------\n",
    "VS_PATH = PROJECT_ROOT / \"backend\" / \"data_vs.db\"\n",
    "print(f\"Current Working Directory:{PROJECT_ROOT}\")\n",
    "print(f\"Path Variable:{VS_PATH}\")\n",
    "vs = ChromaDBVectorStore(db_path=str(VS_PATH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8516056",
   "metadata": {},
   "source": [
    "#Query Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4abe16e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '## **Water-Activated Tape**', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'mime_type': 'text/markdown', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'chapters': '[\"## **Water-Activated Tape**\"]'}\n",
      "{'mime_type': 'text/markdown', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'chapters': '[\"## **Water-Activated Tape**\", \"###### from\"]', 'title': '###### from'}\n",
      "{'chapters': '[\"## **Water-Activated Tape**\", \"###### from\", \"###### **Environmental** **Product** **Declaration**\"]', 'mime_type': 'text/markdown', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'title': '###### **Environmental** **Product** **Declaration**', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9'}\n",
      "{'title': '###### 1', 'chapters': '[\"## **Water-Activated Tape**\", \"###### from\", \"###### **Environmental** **Product** **Declaration**\", \"###### 1\"]', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'mime_type': 'text/markdown', 'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf'}\n",
      "{'mime_type': 'text/markdown', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'chapters': '[\"## **Water-Activated Tape**\", \"## **EPD Programme Information**\"]', 'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'title': '## **EPD Programme Information**'}\n",
      "{'mime_type': 'text/markdown', 'title': '###### 2', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'chapters': '[\"## **Water-Activated Tape**\", \"## **EPD Programme Information**\", \"###### 2\"]', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf'}\n",
      "{'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'title': '###### 3', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'chapters': '[\"## **Water-Activated Tape**\", \"## **EPD Programme Information**\", \"###### 2\", \"###### 3\"]', 'mime_type': 'text/markdown'}\n",
      "{'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'title': '###### 4', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'chapters': '[\"## **Water-Activated Tape**\", \"## **EPD Programme Information**\", \"###### 2\", \"###### 3\", \"###### 4\"]', 'mime_type': 'text/markdown'}\n",
      "{'title': '## **IPG Company Information**', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'mime_type': 'text/markdown', 'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'chapters': '[\"## **Water-Activated Tape**\", \"## **IPG Company Information**\"]', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf'}\n",
      "{'file_hash': '642d4738ef34f1d134ad583d5979b07967c6b062ec3da52317c3b31e7f4cdee9', 'title': '## **Our Locations**', 'mime_type': 'text/markdown', 'chapters': '[\"## **Water-Activated Tape**\", \"## **Our Locations**\"]', 'source': 'EPD_tape_IPG_wateractivated (Copy 1).pdf', 'source_file': 'EPD_tape_IPG_wateractivated (Copy 1).pdf'}\n"
     ]
    }
   ],
   "source": [
    "results = vs.collection.get(include=[\"metadatas\"])\n",
    "for meta in results[\"metadatas\"][:10]:\n",
    "    print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d746ec1",
   "metadata": {},
   "source": [
    "or formatted a little nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef10dc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mime_type</th>\n",
       "      <th>source_file</th>\n",
       "      <th>chapters</th>\n",
       "      <th>source</th>\n",
       "      <th>file_hash</th>\n",
       "      <th>title</th>\n",
       "      <th>rows</th>\n",
       "      <th>sheet</th>\n",
       "      <th>columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45d61dd4-931f-416b-87a3-ee7f99900215</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>[\"## **Water-Activated Tape**\"]</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...</td>\n",
       "      <td>## **Water-Activated Tape**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ddfadcf6-c973-4d96-88a9-5aa8b408c069</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>[\"## **Water-Activated Tape**\", \"###### from\"]</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...</td>\n",
       "      <td>###### from</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3bd95a23-a5d4-462c-bae3-39b258a5df49</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>[\"## **Water-Activated Tape**\", \"###### from\",...</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...</td>\n",
       "      <td>###### **Environmental** **Product** **Declara...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a8e63ce3-cdb4-4931-ac1a-8425b2d778eb</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>[\"## **Water-Activated Tape**\", \"###### from\",...</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...</td>\n",
       "      <td>###### 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78641a49-7ae9-40e2-8e9d-357729c858cc</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>[\"## **Water-Activated Tape**\", \"## **EPD Prog...</td>\n",
       "      <td>EPD_tape_IPG_wateractivated (Copy 1).pdf</td>\n",
       "      <td>642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...</td>\n",
       "      <td>## **EPD Programme Information**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>2d1463b6-47ee-4a20-9f50-7567b4e289f5</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>[\"# tesapack\\u00ae Eco &amp; Ultra Strong ecoLogo\\...</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...</td>\n",
       "      <td>### **Eigenschaften / Leistungswerte**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>98b90ff3-3d1e-4457-b568-879e1b0a8550</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>[\"# tesapack\\u00ae Eco &amp; Ultra Strong ecoLogo\\...</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...</td>\n",
       "      <td># tesapack® Eco &amp; Ultra Strong ecoLogo®</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>4bef34b6-93be-4c1f-aa1b-32920254d9df</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>[\"# tesapack\\u00ae Eco &amp; Ultra Strong ecoLogo\\...</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...</td>\n",
       "      <td>## Produkt Information</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>39f2a524-3391-4216-bea0-68d026ed2ad0</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>[\"# tesapack\\u00ae Eco &amp; Ultra Strong ecoLogo\\...</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...</td>\n",
       "      <td>### Haftungsausschluss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>f79c1bcd-b05b-4f02-81d7-06f9b67d6f7b</td>\n",
       "      <td>text/markdown</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>[\"# tesapack\\u00ae Eco &amp; Ultra Strong ecoLogo\\...</td>\n",
       "      <td>SPEC_tape_tesa_tesapack58297.pdf</td>\n",
       "      <td>300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...</td>\n",
       "      <td>### Haftungsausschluss</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id      mime_type  \\\n",
       "0    45d61dd4-931f-416b-87a3-ee7f99900215  text/markdown   \n",
       "1    ddfadcf6-c973-4d96-88a9-5aa8b408c069  text/markdown   \n",
       "2    3bd95a23-a5d4-462c-bae3-39b258a5df49  text/markdown   \n",
       "3    a8e63ce3-cdb4-4931-ac1a-8425b2d778eb  text/markdown   \n",
       "4    78641a49-7ae9-40e2-8e9d-357729c858cc  text/markdown   \n",
       "..                                    ...            ...   \n",
       "369  2d1463b6-47ee-4a20-9f50-7567b4e289f5  text/markdown   \n",
       "370  98b90ff3-3d1e-4457-b568-879e1b0a8550  text/markdown   \n",
       "371  4bef34b6-93be-4c1f-aa1b-32920254d9df  text/markdown   \n",
       "372  39f2a524-3391-4216-bea0-68d026ed2ad0  text/markdown   \n",
       "373  f79c1bcd-b05b-4f02-81d7-06f9b67d6f7b  text/markdown   \n",
       "\n",
       "                                  source_file  \\\n",
       "0    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "1    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "2    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "3    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "4    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "..                                        ...   \n",
       "369          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "370          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "371          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "372          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "373          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "\n",
       "                                              chapters  \\\n",
       "0                      [\"## **Water-Activated Tape**\"]   \n",
       "1       [\"## **Water-Activated Tape**\", \"###### from\"]   \n",
       "2    [\"## **Water-Activated Tape**\", \"###### from\",...   \n",
       "3    [\"## **Water-Activated Tape**\", \"###### from\",...   \n",
       "4    [\"## **Water-Activated Tape**\", \"## **EPD Prog...   \n",
       "..                                                 ...   \n",
       "369  [\"# tesapack\\u00ae Eco & Ultra Strong ecoLogo\\...   \n",
       "370  [\"# tesapack\\u00ae Eco & Ultra Strong ecoLogo\\...   \n",
       "371  [\"# tesapack\\u00ae Eco & Ultra Strong ecoLogo\\...   \n",
       "372  [\"# tesapack\\u00ae Eco & Ultra Strong ecoLogo\\...   \n",
       "373  [\"# tesapack\\u00ae Eco & Ultra Strong ecoLogo\\...   \n",
       "\n",
       "                                       source  \\\n",
       "0    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "1    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "2    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "3    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "4    EPD_tape_IPG_wateractivated (Copy 1).pdf   \n",
       "..                                        ...   \n",
       "369          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "370          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "371          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "372          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "373          SPEC_tape_tesa_tesapack58297.pdf   \n",
       "\n",
       "                                             file_hash  \\\n",
       "0    642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...   \n",
       "1    642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...   \n",
       "2    642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...   \n",
       "3    642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...   \n",
       "4    642d4738ef34f1d134ad583d5979b07967c6b062ec3da5...   \n",
       "..                                                 ...   \n",
       "369  300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...   \n",
       "370  300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...   \n",
       "371  300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...   \n",
       "372  300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...   \n",
       "373  300b4decc8927c6edf4fe0036de78d7080bc4b6c3bc0e5...   \n",
       "\n",
       "                                                 title  rows sheet  columns  \n",
       "0                          ## **Water-Activated Tape**   NaN   NaN      NaN  \n",
       "1                                          ###### from   NaN   NaN      NaN  \n",
       "2    ###### **Environmental** **Product** **Declara...   NaN   NaN      NaN  \n",
       "3                                             ###### 1   NaN   NaN      NaN  \n",
       "4                     ## **EPD Programme Information**   NaN   NaN      NaN  \n",
       "..                                                 ...   ...   ...      ...  \n",
       "369             ### **Eigenschaften / Leistungswerte**   NaN   NaN      NaN  \n",
       "370            # tesapack® Eco & Ultra Strong ecoLogo®   NaN   NaN      NaN  \n",
       "371                             ## Produkt Information   NaN   NaN      NaN  \n",
       "372                             ### Haftungsausschluss   NaN   NaN      NaN  \n",
       "373                             ### Haftungsausschluss   NaN   NaN      NaN  \n",
       "\n",
       "[374 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = vs.collection.get(include=[\"metadatas\"])\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"id\": chunk_id, **meta}\n",
    "    for chunk_id, meta in zip(results[\"ids\"], results[\"metadatas\"])\n",
    "])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd570da",
   "metadata": {},
   "source": [
    "## Updating Chunk Metadata After Ingestion\n",
    "\n",
    "ChromaDB allows you to add or modify metadata fields on already-stored chunks\n",
    "using `collection.update()`. This is useful for backfilling information you didn't\n",
    "have at ingestion time — for example, a review status, a quality score, or (as we\n",
    "will see later) a file hash for deduplication.\n",
    "\n",
    "> **Important:** `collection.update()` replaces the entire metadata dict for each\n",
    "> chunk. Always merge the existing metadata with your new field using `{**meta, \"new_key\": value}`\n",
    "> to avoid wiping existing fields.\n",
    "\n",
    "The cells below add a dummy `\"demo\"` field to every chunk as a minimal example,\n",
    "then verify the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ba17752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 374 chunks\n"
     ]
    }
   ],
   "source": [
    "# Get all existing IDs and metadata\n",
    "results = vs.collection.get(include=[\"metadatas\"])\n",
    "\n",
    "# Merge existing metadata with the new field\n",
    "updated_metadatas = [\n",
    "    {**meta, \"demo\": \"test\"}\n",
    "    for meta in results[\"metadatas\"]\n",
    "]\n",
    "\n",
    "vs.collection.update(ids=results[\"ids\"], metadatas=updated_metadatas)\n",
    "print(f\"Updated {len(results['ids'])} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2e1389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mime_type</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>demo</th>\n",
       "      <th>source_file</th>\n",
       "      <th>chapters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td># Supplier Sustainability Requirements</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>## 1. Purpose and Scope</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>## 2. Evidence Standards</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>## 3. Requirements by Category</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text/markdown</td>\n",
       "      <td>### 3.1 All Suppliers and Products</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>test</td>\n",
       "      <td>ART_internal_procurement_policy.pdf</td>\n",
       "      <td>[\"# Supplier Sustainability Requirements\", \"##...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mime_type                                   title  \\\n",
       "0  text/markdown  # Supplier Sustainability Requirements   \n",
       "1  text/markdown                 ## 1. Purpose and Scope   \n",
       "2  text/markdown                ## 2. Evidence Standards   \n",
       "3  text/markdown          ## 3. Requirements by Category   \n",
       "4  text/markdown      ### 3.1 All Suppliers and Products   \n",
       "\n",
       "                                source  demo  \\\n",
       "0  ART_internal_procurement_policy.pdf  test   \n",
       "1  ART_internal_procurement_policy.pdf  test   \n",
       "2  ART_internal_procurement_policy.pdf  test   \n",
       "3  ART_internal_procurement_policy.pdf  test   \n",
       "4  ART_internal_procurement_policy.pdf  test   \n",
       "\n",
       "                           source_file  \\\n",
       "0  ART_internal_procurement_policy.pdf   \n",
       "1  ART_internal_procurement_policy.pdf   \n",
       "2  ART_internal_procurement_policy.pdf   \n",
       "3  ART_internal_procurement_policy.pdf   \n",
       "4  ART_internal_procurement_policy.pdf   \n",
       "\n",
       "                                            chapters  \n",
       "0         [\"# Supplier Sustainability Requirements\"]  \n",
       "1  [\"# Supplier Sustainability Requirements\", \"##...  \n",
       "2  [\"# Supplier Sustainability Requirements\", \"##...  \n",
       "3  [\"# Supplier Sustainability Requirements\", \"##...  \n",
       "4  [\"# Supplier Sustainability Requirements\", \"##...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = vs.collection.get(ids=results[\"ids\"][:5], include=[\"metadatas\"])\n",
    "pd.DataFrame(sample[\"metadatas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24973fa",
   "metadata": {},
   "source": [
    "## Idempotence of the ingestion / Deduplication via File Hashing\n",
    "\n",
    "A common problem in document ingestion pipelines is **accidentally re-ingesting\n",
    "the same file twice** — for example when adding new\n",
    "documents to the data folder again, or in case of duplicates with different filenames in the document corpus. This bloats the vector\n",
    "store with duplicate chunks and degrades retrieval quality.\n",
    "\n",
    "### The idea\n",
    "\n",
    "Instead of tracking filenames (which can change), we compute a **SHA-256 hash\n",
    "of the file's raw bytes**. This hash is a unique fingerprint of the file's\n",
    "content — if the content hasn't changed, the hash won't change either.\n",
    "\n",
    "The strategy has three steps:\n",
    "\n",
    "1. **Hash** — Before ingesting a file, compute its SHA-256 hash.\n",
    "2. **Check** — Query the vector store for any chunk that already carries that\n",
    "   hash as metadata. If one exists, the file is already ingested — skip it.\n",
    "3. **Stamp** — If the file is new, add the hash as a `\"file_hash\"` metadata\n",
    "   field on every chunk before inserting them. Future runs can then detect it.\n",
    "\n",
    "This approach correctly handles renamed files (same hash → skip) and detects\n",
    "modified files (different hash → re-ingest). Disadvantage: the filenames and folder path of the duplicates will be lost.\n",
    "\n",
    "The cells below demonstrate all three steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Hash the file:\n",
    "\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def file_hash(path: str) -> str:\n",
    "    return hashlib.sha256(Path(path).read_bytes()).hexdigest()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe93dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#2. Stamp it onto chunks before ingestion (this is where you'd extend load_chunks):\n",
    "\n",
    "\n",
    "for chunk in file_chunks:\n",
    "    chunk.metadata[\"file_hash\"] = file_hash(str(file_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3453571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3. Check for duplicates before ingesting a file:\n",
    "\n",
    "\n",
    "def already_ingested(collection, hash_value: str) -> bool:\n",
    "    results = collection.get(where={\"file_hash\": hash_value}, limit=1)\n",
    "    return len(results[\"ids\"]) > 0\n",
    "Then the guard becomes:\n",
    "\n",
    "\n",
    "hash_value = file_hash(str(file_path))\n",
    "if already_ingested(vs.collection, hash_value):\n",
    "    print(f\"Skipping {file_path.name} — already in store\")\n",
    "else:\n",
    "    # embed and insert chunks\n",
    "# The key thing: ChromaDB's where= filter on .get() lets you query by any metadata field, so the hash becomes a good lookup key.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c3061",
   "metadata": {},
   "source": [
    "## Changes Made to `feature0_baseline_rag.py`\n",
    "\n",
    "Three functions were added or updated to support incremental, hash-based ingestion.\n",
    "\n",
    "**`file_hash()`** is a new helper that computes a SHA-256 fingerprint of a file's\n",
    "raw bytes. Crucially, the hash is based on file *content*, not the filename —\n",
    "renaming a file produces the same hash, and modifying it produces a different one.\n",
    "\n",
    "**`get_existing_hashes()`** is a new helper that reads the vector store metadata\n",
    "and returns the set of file hashes already persisted. It is called before\n",
    "`load_chunks()` so that file parsing can be skipped early for known files.\n",
    "\n",
    "**`load_chunks()`** now accepts an `existing_hashes` parameter and maintains an\n",
    "internal `seen_hashes` set as it iterates over files. This gives it two layers\n",
    "of deduplication: it skips files whose hash is already in the store (cross-run\n",
    "dedup), and it skips files with identical content to one already processed in the\n",
    "current batch (within-run dedup). The expensive PDF parsing step is therefore\n",
    "never wasted on a file that would be discarded anyway.\n",
    "\n",
    "**`build_vector_store()`** replaces the all-or-nothing empty-store check with a\n",
    "per-file check. Incoming chunks are grouped by `\"file_hash\"` and each group is\n",
    "checked against the store before embedding. It logs a summary of how many\n",
    "files and chunks were skipped versus embedded, and warns when two different\n",
    "filenames in the current batch carry identical content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_hash(path: Path) -> str:\n",
    "    \"\"\"SHA-256 fingerprint of a file's raw bytes.\"\"\"\n",
    "    return hashlib.sha256(path.read_bytes()).hexdigest()\n",
    "\n",
    "\n",
    "def get_existing_hashes(db_path: Path = VS_PATH) -> set[str]:\n",
    "    \"\"\"Return file hashes already present in the vector store.\"\"\"\n",
    "    if not db_path.exists():\n",
    "        return set()\n",
    "    vs = ChromaDBVectorStore(db_path=str(db_path))\n",
    "    result = vs.collection.get(include=[\"metadatas\"])\n",
    "    return {m[\"file_hash\"] for m in result[\"metadatas\"] if \"file_hash\" in m}\n",
    "\n",
    "\n",
    "def load_chunks(max_files: int | None = None, existing_hashes: set[str] | None = None) -> list[Chunk]:\n",
    "    \"\"\"Load documents from DATA_DIR and split them into chunks.\n",
    "\n",
    "    Supported formats:\n",
    "        .pdf: converted to Markdown via pymupdf4llm, split on headings\n",
    "        .xlsx, .xls: one chunk per sheet (Markdown table)\n",
    "\n",
    "    Unsupported formats (e.g. standalone images) are logged as warnings and skipped.\n",
    "    Images embedded inside PDFs are not extracted as text by default!\n",
    "\n",
    "    Pass 'max_files' to cap the total number of files processed. Useful for quick\n",
    "    iteration during development before scaling to all files.\n",
    "    \"\"\"\n",
    "    all_chunks: list[Chunk] = []\n",
    "    all_files = sorted(f for f in DATA_DIR.iterdir() if f.is_file())\n",
    "\n",
    "    if max_files is not None:\n",
    "        all_files = all_files[:max_files]\n",
    "        print(len(all_files))\n",
    "\n",
    "    for f in all_files:\n",
    "        ext = f.suffix.lower()\n",
    "        if ext not in _CHUNKERS:\n",
    "            if ext in _IMAGE_EXTENSIONS:\n",
    "                logger.warning(f\"Skipping image file (not supported): {f.name}\")\n",
    "            else:\n",
    "                logger.warning(f\"Skipping unsupported file type {ext!r}: {f.name}\")\n",
    "\n",
    "    supported_files = [f for f in all_files if f.suffix.lower() in _CHUNKERS]\n",
    "    logger.info(f\"Chunking {len(supported_files)} files from {DATA_DIR}\")\n",
    "\n",
    "    seen_hashes: set[str] = set()\n",
    "    for file_path in supported_files:\n",
    "        hash_value = file_hash(file_path)\n",
    "        if existing_hashes is not None and hash_value in existing_hashes:\n",
    "            logger.info(f\"Skipping {file_path.name!r} — already in store (hash={hash_value[:8]}…)\")\n",
    "            continue\n",
    "        if hash_value in seen_hashes:\n",
    "            logger.warning(f\"Skipping {file_path.name!r} — duplicate content in current batch (hash={hash_value[:8]}…)\")\n",
    "            continue\n",
    "        seen_hashes.add(hash_value)\n",
    "        chunker = _CHUNKERS[file_path.suffix.lower()]\n",
    "        try:\n",
    "            file_chunks = chunker.make_chunks(str(file_path))\n",
    "            for chunk in file_chunks:\n",
    "                chunk.metadata[\"file_hash\"] = hash_value\n",
    "                chunk.metadata[\"source_file\"] = file_path.name\n",
    "                chunk.metadata[\"source\"] = file_path.name\n",
    "                chunk.metadata[\"title\"] = chunk.title\n",
    "            all_chunks.extend(file_chunks)\n",
    "            logger.debug(f\"  {file_path.name}: {len(file_chunks)} chunks\")\n",
    "        except Exception as exc:\n",
    "            logger.warning(f\"Skipping {file_path.name}: {exc}\")\n",
    "\n",
    "    logger.info(f\"Done, {len(all_chunks)} chunks total\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def build_vector_store(\n",
    "    chunks: list[Chunk],\n",
    "    embedding_model: SentenceTransformerEmbeddings,\n",
    "    db_path: Path = VS_PATH,\n",
    "    reset: bool = False,\n",
    ") -> ChromaDBVectorStore:\n",
    "    \"\"\"Embed 'chunks' and persist them in a ChromaDB vector store.\n",
    "\n",
    "    Set 'reset=True' to delete and rebuild the store from scratch. Leave\n",
    "    'reset=False' (default) to reuse an existing store, embedding all documents\n",
    "    takes time; skipping it on subsequent runs saves time.\n",
    "    \"\"\"\n",
    "    if reset and db_path.exists():\n",
    "        import shutil\n",
    "        shutil.rmtree(db_path)\n",
    "        logger.info(f\"Deleted existing vector store at {db_path}\")\n",
    "\n",
    "    vector_store = ChromaDBVectorStore(db_path=str(db_path))\n",
    "    \n",
    "\n",
    "    # Group chunks by file hash for per-file deduplication\n",
    "    chunks_by_hash: dict[str, list[Chunk]] = {}\n",
    "    for chunk in chunks:\n",
    "        h = chunk.metadata.get(\"file_hash\", \"unknown\")\n",
    "        chunks_by_hash.setdefault(h, []).append(chunk)\n",
    "\n",
    "    # Warn about same-content files in the current batch\n",
    "    for hash_value, file_chunks in chunks_by_hash.items():\n",
    "        sources = list(dict.fromkeys(c.metadata.get(\"source_file\", \"?\") for c in file_chunks))\n",
    "        if len(sources) > 1:\n",
    "            logger.warning(f\"Duplicate content detected across files (hash={hash_value[:8]}…): {sources} — only ingesting once.\")\n",
    "\n",
    "    new_chunks: list[Chunk] = []\n",
    "    skipped_files = 0\n",
    "    skipped_chunks = 0\n",
    "    for hash_value, file_chunks in chunks_by_hash.items():\n",
    "        existing = vector_store.collection.get(where={\"file_hash\": hash_value}, limit=1)\n",
    "        if existing[\"ids\"]:\n",
    "            source = file_chunks[0].metadata.get(\"source_file\", \"?\")\n",
    "            logger.info(f\"Skipping {source!r} — already in store (hash={hash_value[:8]}…)\")\n",
    "            skipped_files += 1\n",
    "            skipped_chunks += len(file_chunks)\n",
    "        else:\n",
    "            new_chunks.extend(file_chunks)\n",
    "    logger.info(\n",
    "        f\"Deduplication: {skipped_files} file(s) / {skipped_chunks} chunk(s) skipped, \"\n",
    "        f\"{len(chunks_by_hash) - skipped_files} file(s) / {len(new_chunks)} chunk(s) to embed.\"\n",
    "    )\n",
    "\n",
    "    if not new_chunks:\n",
    "        logger.info(\"All files already in store — nothing to embed.\")\n",
    "        return vector_store\n",
    "\n",
    "    logger.info(f\"Embedding {len(new_chunks)} new chunks with {embedding_model.model_name!r} …\")\n",
    "    embeddings = await embedding_model.get_embeddings([c.content for c in new_chunks])\n",
    "    logger.info(f\"Embedding matrix: shape={embeddings.shape}  dtype={embeddings.dtype}\")\n",
    "    await vector_store.insert_chunks(chunks=new_chunks, embedding=embeddings)\n",
    "    logger.info(f\"Done! Vector store written to {db_path}\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37244449",
   "metadata": {},
   "source": [
    "In the Notebook, we need to pass the existing_hashes to the load_chunks function. Also the calculation can lead to a division by 0, which needs to be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78599c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from DATA_DIR and split them into chunks.\n",
    "existing_hashes = get_existing_hashes()\n",
    "chunks = load_chunks(max_files=None, existing_hashes=existing_hashes)\n",
    "# Print a statistical summary and sampled content for visual inspection.\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Print size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "if char_lengths:\n",
    "    print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "    print(f\"Over 1024-char limit (≈256 tok embedding limit): {over_limit} / {len(chunks)}\")\n",
    "else:\n",
    "    print(\"All files already in store — no new chunks loaded.\")\n",
    "print(f\"Over 1024-char limit (≈256 tok embedding limit): {over_limit} / {len(chunks)}\")\n",
    "print(\"\\nSuccessfully loaded and chunked the documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24f150",
   "metadata": {},
   "source": [
    "Helper to release the database, to prevent locks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61f489aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mvs\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'vs' is not defined"
     ]
    }
   ],
   "source": [
    "del vs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SME RAG",
   "language": "python",
   "name": "sme-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
