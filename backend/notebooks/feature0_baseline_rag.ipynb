{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# The Baseline RAG Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is your **starting point**. The pipeline is already built and working: run it, explore its outputs, question it, and find its limits.\n",
    "\n",
    "**How to use this notebook**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| ðŸ“– **Read** | The explanations use plain language, no coding background needed |\n",
    "| â–¶ï¸ **Run** | Execute cells top to bottom with Shift+Enter to see the pipeline in action |\n",
    "| ðŸ’¬ **Discuss** | Talk about the outputs with your peers, do they make sense? Would you trust them? |\n",
    "| ðŸ”§ **Experiment** | Modify queries, tweak parameters, break things on purpose |\n",
    "| ðŸš€ **Extend** | The Tasks section points to what you can take further |\n",
    "\n",
    "> You don't need to understand every line of code. Focus on what the system gets right and wrong and on thinking about how this would apply in your own context.\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines a search engine with an AI assistant. Instead of the AI making things up from memory, it first searches your documents and then answers based on what it finds. The answer can always be traced back to a source.\n",
    "\n",
    "```\n",
    "Your question  ->  Search your documents  ->  AI answers using only those documents\n",
    "```\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| **Baseline (this notebook)** | Working baseline prototype |\n",
    "| Feature Track 1 | How documents are split into searchable pieces |\n",
    "| Feature Track 2 | How to measure answer quality |\n",
    "| Feature Track 3 | Reliable, structured outputs |\n",
    "| Feature Track 4 | Better retrieval strategies |\n",
    "| Feature Track 5 | Multi-step agent workflows |\n",
    "| Feature Track 6 | Connecting to the chat frontend |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-why-rag-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why RAG? The Problem with a Standalone LLM\n",
    "\n",
    "### The Scenario\n",
    "**PrimePack AG** buys packaging materials (pallets, cardboard boxes, tape) from multiple suppliers. Sustainability claims are increasingly scrutinised by customers and regulators. Employees need to answer questions like:\n",
    "> *\"What is the GWP of the Logypal 1 pallet, and is the figure verified?\"*  \n",
    "> *\"Can we tell a customer that the tesa tape is PFAS-free?\"*  \n",
    "> *\"Which of our suppliers have a certified EPD?\"*\n",
    "\n",
    "### Why Not Just Ask ChatGPT?\n",
    "A general-purpose LLM has three fundamental problems for this task:\n",
    "\n",
    "| Problem | Why It Matters |\n",
    "|---|---|\n",
    "| **Internal document** | LLMs don't know about internal company documents. |\n",
    "| **Hallucination** | When asked about unknown products the LLM invents plausible-sounding but false figures. |\n",
    "| **No evidence trail** | Even when correct, a raw LLM answer cannot be traced back to a source document. |\n",
    "\n",
    "### The RAG Solution\n",
    "RAG adds a **retrieval step** between the user's question and the LLM:\n",
    "\n",
    "```\n",
    " Documents â”€â”€â–º Chunker â”€â”€â–º Embedder â”€â”€â–º Vector DB\n",
    "                                              â”‚\n",
    " User query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Embedder â”€â”€â”€â”€â”€â–º  Retriever â”€â”€â–º Top-k Chunks\n",
    "                                                                      â”‚\n",
    "                                                               LLM + Prompt\n",
    "                                                                      â”‚\n",
    "                                                               Answer + Sources\n",
    "```\n",
    "\n",
    "The LLM only sees documents that are **actually in the corpus**. The answer can be traced to specific source chunks. If the corpus does not contain the answer, the LLM is instructed to say so.\n",
    "\n",
    "### What RAG Does *Not* Fix\n",
    "RAG shifts the problem from hallucination to **retrieval quality**. If the right chunk is not retrieved, the answer will still be wrong (or absent). The later feature tracks address exactly this: better chunking, better retrieval, and better output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-concepts-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### Chunks\n",
    "\n",
    "A **chunk** is a short excerpt from a source document, a section of a PDF, one sheet of a spreadsheet, or one heading-delimited paragraph of a Markdown file. Chunks are the unit of indexing and retrieval.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: str           # unique identifier\n",
    "    title: str        # e.g. section heading\n",
    "    content: str      # the text that gets embedded\n",
    "    metadata: dict    # source_file, page, ...\n",
    "```\n",
    "\n",
    "### Embeddings\n",
    "An **embedding** converts text to a dense numeric vector (e.g. 384 dimensions). Semantically similar texts produce similar vectors. Here we use `all-MiniLM-L6-v2`, a compact local model that runs without an API key.\n",
    "\n",
    "### Vector Store (ChromaDB)\n",
    "A **vector store** persists chunk embeddings on disk and supports approximate nearest-neighbour search. Given a query embedding, it returns the `top_k` most similar chunks in milliseconds.\n",
    "\n",
    "### Retriever\n",
    "A **retriever** wraps a vector store and exposes a single `retrieve(query)` method. The baseline uses a `VectorStoreRetriever` with `top_k=5`.\n",
    "\n",
    "### RAG Agent\n",
    "The **RAG agent** combines a retriever and an LLM. Its `answer()` method:\n",
    "1. Embeds the query\n",
    "2. Retrieves the top-k chunks\n",
    "3. Formats chunks as XML `<source>` tags in the prompt\n",
    "4. Calls the LLM and returns the answer + cited sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` must be installed in editable mode (`pip install -e conversational-toolkit/ && pip install -e backend/`). For the **Ollama** backend, start `ollama serve` and pull the model (`ollama pull mistral-nemo:12b`). For **OpenAI**, set `OPENAI_API_KEY` in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "p0-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n",
      "Project root : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag\n",
      "Data dir     : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/data\n",
      "Vector store : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/backend/data_vs.db\n",
      "LLM backend  : openai\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import VectorStoreRetriever\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    load_chunks,\n",
    "    inspect_chunks,\n",
    "    build_vector_store,\n",
    "    inspect_retrieval,\n",
    "    build_agent,\n",
    "    build_llm,\n",
    "    ask,\n",
    "    DATA_DIR,\n",
    "    VS_PATH,\n",
    "    EMBEDDING_MODEL,\n",
    "    RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "# Choose your LLM backend: \"ollama\" (local, requires `ollama serve`) or \"openai\" (requires OPENAI_API_KEY)\n",
    "BACKEND = \"openai\"  # set this before running\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError(\n",
    "        'BACKEND is not set. Edit the line above and set it to \"ollama\", or \"openai\".\\n'\n",
    "        \"See Renku_README.md for setup instructions.\"\n",
    "    )\n",
    "\n",
    "ROOT = Path().resolve().parents[1]\n",
    "print(f\"Project root : {ROOT}\")\n",
    "print(f\"Data dir     : {DATA_DIR}\")\n",
    "print(f\"Vector store : {VS_PATH}\")\n",
    "print(f\"LLM backend  : {BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0mf2j4xq79v",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before the RAG Pipeline: The LLM on Its Own\n",
    "\n",
    "A **large language model (LLM)** is a neural network trained on billions of words of text. It can summarise documents, answer questions, and generate structured output, but only from knowledge baked into its weights during training. It has no direct access to your internal documents.\n",
    "\n",
    "Before building the RAG pipeline, let's interact with the LLM directly to understand what it can and cannot do on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3tx6k25ljdx",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "# Reuse the backend you chose in the Setup cell above\n",
    "llm_standalone = build_llm(backend=BACKEND)\n",
    "\n",
    "# A question the LLM can answer from general training data\n",
    "general_question = \"What does GWP stand for, and what unit is it typically measured in?\"\n",
    "\n",
    "response_general = await llm_standalone.generate(\n",
    "    [LLMMessage(role=Roles.USER, content=general_question)]\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Q: {general_question}\\n\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"A: {response_general.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vkh4o5519",
   "metadata": {},
   "source": [
    "The LLM answers that correctly, GWP is a well-known concept covered in its training data.\n",
    "\n",
    "Now ask something specific to PrimePack AG's product portfolio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sy3pgyhowo9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A product-specific question the LLM has never seen in training\n",
    "primepack_question = \"What is the Global Warming Potential (GWP) of the Logypal 1 pallet sold by PrimePack AG, and is the figure third-party verified? Provide the link to PrimePack AG's official website.\"\n",
    "\n",
    "response_pp = await llm_standalone.generate(\n",
    "    [LLMMessage(role=Roles.USER, content=primepack_question)]\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Q: {primepack_question}\\n\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"A: {response_pp.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99690afe",
   "metadata": {},
   "source": [
    "PrimePack AG is a fictional company, no training data exists for this product\n",
    "- If the model gave a specific figure or website link: it is hallucinated.\n",
    "- If the model said 'I don't know': that is honest, but still not useful.\n",
    "\n",
    "Either way, the LLM cannot provide the actual figure with a verifiable source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a69d9b",
   "metadata": {},
   "source": [
    "> **Task: Compare LLM Backends**\n",
    "> 1. **Switch backends.** Change BACKEND to \"ollama\" (or \"openai\" if you started with Ollama) and re-run the two question cells above. Does one model hallucinate a specific figure or website link while the other declines? How confident does each answer sound?\n",
    "> 2. **Change the question.** Replace the tesa question with something you could imagine being asked in a real supplier audit. Does the standalone LLM give you an answer you would trust?\n",
    "> 3. **Note the pattern.** Regardless of whether the model hallucinates or says \"I don't know\", ask: could you send this response to a customer? What is missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgvbri1fhj8",
   "metadata": {},
   "source": [
    "### Why do different models behave differently?\n",
    "\n",
    "**OpenAI models** (GPT-4o, GPT-4o-mini) are extensively trained with human feedback (Reinforcement Learning from Human Feedback(RLHF)) to decline when they lack reliable information. For a fictional company like PrimePack AG, with no public web presence, the model has learned to say \"I don't know\" rather than confabulate a specific figure.\n",
    "\n",
    "**Smaller local models** (Mistral, LLaMA 7â€“13 B) are typically less safety-fine-tuned. Without the reinforcement signal that penalises confident wrong answers, they are more likely to generate a plausible-sounding but fabricated answer.\n",
    "\n",
    "**The problem in either case:** \"I don't know\" and a hallucinated answer are equally useless to an employee who needs to respond to a CSRD audit. The correct response, *\"The verified GWP is X kg COâ‚‚e according to the 2023 EPD (source: EPD_pallet_relicyc_logypal1.pdf)\"*, requires access to the actual document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qw0u2tdx1kj",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing a Backend: OpenAI API vs Local Models\n",
    "\n",
    "Two LLM backends are available in this workshop. Both expose the same interface, switching requires changing a single variable.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| | **OpenAI API** (`gpt-4o-mini`, `gpt-4o`, â€¦) | **Ollama -> local** (`mistral-nemo:12b`, `llama3.2`, â€¦) |\n",
    "|---|---|---|\n",
    "| **Data security** | Queries and document chunks are sent to OpenAI's servers. You can request zero-data-retention. | Everything stays on-premise. Nothing leaves the machine. Suitable for confidential documents without any external data agreements. |\n",
    "| **Model capability** | State-of-the-art. Follows complex instructions reliably, structures output well, handles edge cases. `gpt-4o-mini` is the default for this workshop, it is much cheaper than `gpt-4o` with most of the capability for RAG tasks. | Smaller models (7â€“13 B parameters) are weaker on complex reasoning and strict rule-following. For straightforward retrieval and summarisation tasks the quality gap narrows considerably. |\n",
    "| **Cost** | Per-token billing. A typical RAG query costs a fraction of a cent. See the cost estimation section below. | No API cost: you pay for hardware (CPU/GPU) and electricity. |\n",
    "| **Latency** | ~1â€“2 s per query (network + model time) | CPU-only: ~20â€“60 s. GPU: ~2â€“5 s |\n",
    "| **Setup** | One API key, no local hardware required | `ollama serve` + model download |\n",
    "\n",
    "### Self-Hosting Larger Models\n",
    "\n",
    "The quality gap between a 12 B local model and GPT-4o can be substantially closed at larger model sizes:\n",
    "\n",
    "- **LLaMA 3.1 70 B, Mistral Large 2, Qwen 2.5 72 B**: run on GPUs. Quality can approach GPT-4o on structured tasks like RAG.\n",
    "- **Quantised models (GGUF / GPTQ):** Reduce memory requirements by 50â€“75% with a modest quality trade-off, making larger models accessible on smaller hardware.\n",
    "- **Production stacks:** `vLLM` and `llama.cpp` server provide OpenAI-compatible APIs with batching and much higher throughput than `ollama` alone. \n",
    "\n",
    "**For this workshop** `gpt-4o-mini` (OpenAI) and `mistral-nemo:12b` (Ollama) are both sufficient to demonstrate the full RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clh6qsmd1uk",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cost Estimation\n",
    "\n",
    "API costs scale with the number of tokens processed. **Input tokens** (your system prompt and retrieved document chunks) are cheaper than **output tokens** (the model's generated answer).\n",
    "\n",
    "OpenAI API pricing for all models can be found [here](https://developers.openai.com/api/docs/pricing/). As an example, prices for `gpt-4o-mini` are:\n",
    "\n",
    "| Token type | Price |\n",
    "|---|---|\n",
    "| Input | $0.15 / 1 M tokens |\n",
    "| Output | $0.60 / 1 M tokens |\n",
    "\n",
    "A rough rule of thumb: **1 token â‰ˆ 4 characters** of English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vz7aliuggbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost estimation for gpt-4o-mini\n",
    "INPUT_PRICE_PER_TOKEN = 0.15 / 1_000_000  # USD\n",
    "OUTPUT_PRICE_PER_TOKEN = 0.60 / 1_000_000  # USD\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Rough estimate: 1 token â‰ˆ 4 characters.\"\"\"\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "\n",
    "def estimate_cost(input_text: str, output_text: str) -> dict:\n",
    "    input_tok = estimate_tokens(input_text)\n",
    "    output_tok = estimate_tokens(output_text)\n",
    "    cost = input_tok * INPUT_PRICE_PER_TOKEN + output_tok * OUTPUT_PRICE_PER_TOKEN\n",
    "    return {\"input_tokens\": input_tok, \"output_tokens\": output_tok, \"cost_usd\": cost}\n",
    "\n",
    "\n",
    "# Simulate a typical RAG query: system prompt + 5 retrieved chunks + user question --> short generated answer\n",
    "example_input = (\n",
    "    \"You are a helpful AI assistant specialised in sustainability for PrimePack AG. \"\n",
    "    \"Answer only using the provided document excerpts. Cite your sources.\\n\\n\"\n",
    "    \"Source: EPD_pallet_relicyc_logypal1.pdf\\n\"\n",
    "    \"The Logypal 1 pallet has a declared GWP of 3.2 kg CO\\u2082e per functional unit (A1\\u2013A3), \"\n",
    "    \"verified by an independent third-party auditor under ISO\\u202014044.\\n\\n\"\n",
    "    \"Source: ART_product_catalog.md\\n\"\n",
    "    \"The Logypal 1 (Product ID: 20-100) is a recycled-plastic pallet supplied by Relicyc. \"\n",
    "    \"It is listed as the primary pallet for heavy-duty use in the PrimePack AG portfolio.\\n\\n\"\n",
    "    \"[... 3 more retrieved chunks ...]\\n\\n\"\n",
    "    \"Q: What is the GWP of the Logypal 1 pallet, and is it verified?\"\n",
    ")\n",
    "example_output = (\n",
    "    \"The Logypal 1 pallet has a GWP of 3.2\\u202fkg\\u202fCO\\u2082e per functional unit (A1\\u2013A3), \"\n",
    "    \"according to the third-party verified EPD (EPD_pallet_relicyc_logypal1.pdf). \"\n",
    "    \"The figure has been independently audited under ISO\\u202014044.\"\n",
    ")\n",
    "\n",
    "info = estimate_cost(example_input, example_output)\n",
    "print(f\"Input  : ~{info['input_tokens']:>5,} tokens (prompt + chunks + question)\")\n",
    "print(f\"Output : ~{info['output_tokens']:>5,} tokens (generated answer)\")\n",
    "print(f\"Cost   : ${info['cost_usd']:.6f} per query\")\n",
    "print()\n",
    "print(f\"At 1,000 queries / day     ->  ${info['cost_usd'] * 1_000:>8.4f} / day\")\n",
    "print(f\"At 10,000 queries / day    ->  ${info['cost_usd'] * 10_000:>8.4f} / day\")\n",
    "print(f\"At 1,000,000 queries / day ->  ${info['cost_usd'] * 1_000_000:>8.2f} / day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b0c8a",
   "metadata": {},
   "source": [
    "The largest cost driver is context length. More retrieved chunks = more input tokens.\n",
    "top_k=5 (~2,000 input tokens) is a reasonable starting point for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4rue9d6isf",
   "metadata": {},
   "source": [
    "> **Consider:** For your use case, how many queries would the system handle per day? At what volume does the per-query cost become meaningful? Would data-security requirements push you towards a local model even at lower throughput?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fe3d0",
   "metadata": {},
   "source": [
    "---\n",
    "# RAG Pipeline\n",
    "\n",
    "Now that we have seen what an LLM can and cannot do on its own, we are ready to build the retrieval layer that makes it genuinely useful. The following five steps walk through the full RAG pipeline end-to-end, from loading the documents all the way to a sourced answer.\n",
    "\n",
    "```\n",
    " Documents â”€â”€â–º Chunker â”€â”€â–º Embedder â”€â”€â–º Vector DB\n",
    "                                              â”‚\n",
    " User query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Embedder â”€â”€â”€â”€â”€â–º  Retriever â”€â”€â–º Top-k Chunks\n",
    "                                                                      â”‚\n",
    "                                                               LLM + Prompt\n",
    "                                                                      â”‚\n",
    "                                                               Answer + Sources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load and Chunk Documents\n",
    "\n",
    "The `load_chunks()` function walks `data/` and dispatches each file to the right chunker:\n",
    "\n",
    "| Extension | Chunker | Strategy |\n",
    "|---|---|---|\n",
    "| `.pdf` | `PDFChunker` | Convert to Markdown via `pymupdf4llm`, split on `#` headings |\n",
    "| `.xlsx`, `.xls` | `ExcelChunker` | One chunk per sheet, serialised as a Markdown table |\n",
    "| `.md`, `.txt` | `MarkdownChunker` | Split on `#` headings |\n",
    "\n",
    "The result is a flat `list[Chunk]`, the same structure regardless of the original format.\n",
    "\n",
    "You can use `max_files=5` here for speed. Remove the limit (or set `None`) to load the full corpus.\n",
    "\n",
    "> **Feature Track 1** explores the importance of ingestion and alternative chunking strategies in more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "p0-step1-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:14:31.762 | WARNING  | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:196 - Skipping unsupported file type '': .DS_Store\n",
      "2026-02-22 16:14:31.763 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:199 - Chunking 35 files from /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/data\n",
      "2026-02-22 16:14:31.961 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   ART_internal_procurement_policy.pdf: 12 chunks\n",
      "2026-02-22 16:14:32.109 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   ART_logylight_incomplete_datasheet.pdf: 6 chunks\n",
      "2026-02-22 16:14:32.176 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   ART_product_catalog.pdf: 7 chunks\n",
      "2026-02-22 16:14:32.254 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   ART_relicyc_logypal1_datasheet_2021.pdf: 5 chunks\n",
      "2026-02-22 16:14:32.255 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   ART_response_inquiry_frische_felder.md: 6 chunks\n",
      "2026-02-22 16:14:32.335 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   ART_supplier_brochure_CPR_wood_pallet.pdf: 6 chunks\n",
      "2026-02-22 16:14:32.418 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   ART_supplier_brochure_tesa_ECO.pdf: 8 chunks\n",
      "2026-02-22 16:14:33.923 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EPD_cardboard_grupak_corrugated.pdf: 35 chunks\n",
      "2026-02-22 16:14:35.040 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EPD_cardboard_redbox_cartonpallet.pdf: 11 chunks\n",
      "2026-02-22 16:14:35.873 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EPD_pallet_CPR_noe.pdf: 11 chunks\n",
      "2026-02-22 16:14:37.130 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EPD_pallet_relicyc_logypal1.pdf: 17 chunks\n",
      "2026-02-22 16:14:38.606 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EPD_pallet_stabilplastik_ep08.pdf: 2 chunks\n",
      "2026-02-22 16:14:40.361 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EPD_tape_IPG_hotmelt.pdf: 30 chunks\n",
      "2026-02-22 16:14:42.731 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EPD_tape_IPG_wateractivated.pdf: 63 chunks\n",
      "2026-02-22 16:14:42.733 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   EVALUATION_qa_ground_truth.md: 7 chunks\n",
      "2026-02-22 16:14:46.462 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_ch_eu_sustainability_obligations.pdf: 1 chunks\n",
      "2026-02-22 16:14:49.383 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_ecologo_catalogue.pdf: 10 chunks\n",
      "2026-02-22 16:14:52.341 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_eu_climate_reporting_guidelines.pdf: 2 chunks\n",
      "2026-02-22 16:14:58.616 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_eu_csrd.pdf: 1 chunks\n",
      "2026-02-22 16:15:08.590 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_ghg_protocol_corporate_standard.pdf: 1 chunks\n",
      "2026-02-22 16:15:19.105 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_ghg_protocol_corporate_value_chain.pdf: 29 chunks\n",
      "2026-02-22 16:15:27.838 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_ghg_protocol_product_lca.pdf: 32 chunks\n",
      "2026-02-22 16:15:27.923 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   REF_iso14024_conformance_statement.pdf: 1 chunks\n",
      "2026-02-22 16:15:28.104 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_pallet_CPR_plastic.pdf: 2 chunks\n",
      "2026-02-22 16:15:28.189 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_pallet_CPR_recycled_plastic.pdf: 1 chunks\n",
      "2026-02-22 16:15:28.312 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_pallet_CPR_wood.pdf: 2 chunks\n",
      "2026-02-22 16:15:28.530 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_pallet_relicyc_1208MR.pdf: 2 chunks\n",
      "2026-02-22 16:15:28.752 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_pallet_relicyc_logypal1.pdf: 3 chunks\n",
      "2026-02-22 16:15:29.045 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_pallet_stabilplastik_ep08.pdf: 1 chunks\n",
      "2026-02-22 16:15:29.526 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_tape_CST_synthetic_rubber.pdf: 16 chunks\n",
      "2026-02-22 16:15:29.708 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_tape_WAT_260.pdf: 1 chunks\n",
      "2026-02-22 16:15:30.205 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_tape_WAT_central_brand.pdf: 16 chunks\n",
      "2026-02-22 16:15:35.734 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_tape_tesa_sustainability_report_2024.pdf: 15 chunks\n",
      "2026-02-22 16:15:35.820 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   SPEC_tape_tesa_tesapack58297.pdf: 11 chunks\n",
      "2026-02-22 16:15:35.825 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:211 -   product_overview.xlsx: 1 chunks\n",
      "2026-02-22 16:15:35.825 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:215 - Done, 374 chunks total\n",
      "2026-02-22 16:15:35.825 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:227 - ------ Chunk inspection -------\n",
      "2026-02-22 16:15:35.826 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:228 - Total chunks: 374; Source files: 35\n",
      "2026-02-22 16:15:35.826 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - ART_internal_procurement_policy.pdf: 12 chunks\n",
      "2026-02-22 16:15:35.827 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - ART_logylight_incomplete_datasheet.pdf: 6 chunks\n",
      "2026-02-22 16:15:35.827 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - ART_product_catalog.pdf: 7 chunks\n",
      "2026-02-22 16:15:35.828 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - ART_relicyc_logypal1_datasheet_2021.pdf: 5 chunks\n",
      "2026-02-22 16:15:35.829 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - ART_response_inquiry_frische_felder.md: 6 chunks\n",
      "2026-02-22 16:15:35.829 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - ART_supplier_brochure_CPR_wood_pallet.pdf: 6 chunks\n",
      "2026-02-22 16:15:35.829 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - ART_supplier_brochure_tesa_ECO.pdf: 8 chunks\n",
      "2026-02-22 16:15:35.829 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EPD_cardboard_grupak_corrugated.pdf: 35 chunks\n",
      "2026-02-22 16:15:35.830 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EPD_cardboard_redbox_cartonpallet.pdf: 11 chunks\n",
      "2026-02-22 16:15:35.831 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EPD_pallet_CPR_noe.pdf: 11 chunks\n",
      "2026-02-22 16:15:35.831 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EPD_pallet_relicyc_logypal1.pdf: 17 chunks\n",
      "2026-02-22 16:15:35.832 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EPD_pallet_stabilplastik_ep08.pdf: 2 chunks\n",
      "2026-02-22 16:15:35.832 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EPD_tape_IPG_hotmelt.pdf: 30 chunks\n",
      "2026-02-22 16:15:35.833 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EPD_tape_IPG_wateractivated.pdf: 63 chunks\n",
      "2026-02-22 16:15:35.833 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - EVALUATION_qa_ground_truth.md: 7 chunks\n",
      "2026-02-22 16:15:35.836 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_ch_eu_sustainability_obligations.pdf: 1 chunks\n",
      "2026-02-22 16:15:35.844 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_ecologo_catalogue.pdf: 10 chunks\n",
      "2026-02-22 16:15:35.849 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_eu_climate_reporting_guidelines.pdf: 2 chunks\n",
      "2026-02-22 16:15:35.853 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_eu_csrd.pdf: 1 chunks\n",
      "2026-02-22 16:15:35.854 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_ghg_protocol_corporate_standard.pdf: 1 chunks\n",
      "2026-02-22 16:15:35.856 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_ghg_protocol_corporate_value_chain.pdf: 29 chunks\n",
      "2026-02-22 16:15:35.861 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_ghg_protocol_product_lca.pdf: 32 chunks\n",
      "2026-02-22 16:15:35.862 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - REF_iso14024_conformance_statement.pdf: 1 chunks\n",
      "2026-02-22 16:15:35.863 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_pallet_CPR_plastic.pdf: 2 chunks\n",
      "2026-02-22 16:15:35.864 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_pallet_CPR_recycled_plastic.pdf: 1 chunks\n",
      "2026-02-22 16:15:35.866 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_pallet_CPR_wood.pdf: 2 chunks\n",
      "2026-02-22 16:15:35.868 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_pallet_relicyc_1208MR.pdf: 2 chunks\n",
      "2026-02-22 16:15:35.872 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_pallet_relicyc_logypal1.pdf: 3 chunks\n",
      "2026-02-22 16:15:35.873 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_pallet_stabilplastik_ep08.pdf: 1 chunks\n",
      "2026-02-22 16:15:35.875 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_tape_CST_synthetic_rubber.pdf: 16 chunks\n",
      "2026-02-22 16:15:35.876 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_tape_WAT_260.pdf: 1 chunks\n",
      "2026-02-22 16:15:35.877 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_tape_WAT_central_brand.pdf: 16 chunks\n",
      "2026-02-22 16:15:35.877 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_tape_tesa_sustainability_report_2024.pdf: 15 chunks\n",
      "2026-02-22 16:15:35.878 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - SPEC_tape_tesa_tesapack58297.pdf: 11 chunks\n",
      "2026-02-22 16:15:35.878 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:230 - product_overview.xlsx: 1 chunks\n",
      "2026-02-22 16:15:35.878 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:231 - Sample (first 5):\n",
      "2026-02-22 16:15:35.879 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:234 - Source and title: [ART_internal_procurement_policy.pdf] '# Supplier Sustainability Requirements'\n",
      "2026-02-22 16:15:35.879 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:235 - Chunk content: '# Supplier Sustainability Requirements\\n\\nVersion: 1.2 | Approved by CEO (Andrea Frei) | Effective: 1 January 2024 Classification: Internal use only, do not share externally without management approval'\n",
      "2026-02-22 16:15:35.879 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:234 - Source and title: [ART_internal_procurement_policy.pdf] '## 1. Purpose and Scope'\n",
      "2026-02-22 16:15:35.879 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:235 - Chunk content: '## 1. Purpose and Scope\\n\\n This document establishes the minimum sustainability requirements for all packaging product suppliers from whom PrimePack AG procures goods for resale or distribution. It app'\n",
      "2026-02-22 16:15:35.879 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:234 - Source and title: [ART_internal_procurement_policy.pdf] '## 2. Evidence Standards'\n",
      "2026-02-22 16:15:35.880 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:235 - Chunk content: '## 2. Evidence Standards\\n\\n PrimePack AG classifies supplier sustainability claims into four evidence levels:\\n\\n\\n\\n\\n\\n\\n\\n|Level|Label|Definition|Customer communication| |---|---|---|---| |A|Verified|Third-'\n",
      "2026-02-22 16:15:35.880 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:234 - Source and title: [ART_internal_procurement_policy.pdf] '## 3. Requirements by Category'\n",
      "2026-02-22 16:15:35.880 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:235 - Chunk content: '## 3. Requirements by Category'\n",
      "2026-02-22 16:15:35.880 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:234 - Source and title: [ART_internal_procurement_policy.pdf] '### 3.1 All Suppliers and Products'\n",
      "2026-02-22 16:15:35.880 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:inspect_chunks:235 - Chunk content: '### 3.1 All Suppliers and Products\\n\\n â—\\u200b Respond to sustainability questionnaires within 30 days of request. â—\\u200b Provide a REACH compliance declaration for all products (no SVHC above 0.1% w/w).\\n\\n â—\\u200b No'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunks total       : 374\n",
      "Mean length (chars): 4091\n",
      "Over 1024-char limit (â‰ˆ256 tok embedding limit): 102 / 374\n",
      "\n",
      "Successfully loaded and chunked the documents!\n"
     ]
    }
   ],
   "source": [
    "# Load documents from DATA_DIR and split them into chunks.\n",
    "chunks = load_chunks(max_files=None)\n",
    "# Print a statistical summary and sampled content for visual inspection.\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Print size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "print(f\"Over 1024-char limit (â‰ˆ256 tok embedding limit): {over_limit} / {len(chunks)}\")\n",
    "print(\"\\nSuccessfully loaded and chunked the documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-inspect-md",
   "metadata": {},
   "source": [
    "### What a Chunk Looks Like\n",
    "\n",
    "Each chunk carries a `title` (the heading), the raw text `content`, and a `metadata` dict\n",
    "with the source file name. This metadata is returned alongside the answer so the user can\n",
    "trace every claim back to its origin document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-inspect-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 3 representative chunks\n",
    "for c in chunks[:3]:\n",
    "    print(f\"--- [{c.metadata.get('source_file', '?')}] ---\")\n",
    "    print(f\"Title  : {c.title!r}\")\n",
    "    print(f\"Length : {len(c.content)} chars\")\n",
    "    print(f\"Preview: {c.content[:200].strip()!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Embed Chunks and Build the Vector Store\n",
    "\n",
    "`SentenceTransformerEmbeddings` converts every chunk's `content` to a 384-dimensional vector using `all-MiniLM-L6-v2`. The resulting matrix (shape `[n_chunks, 384]`) is inserted into a persistent `ChromaDBVectorStore`.\n",
    "\n",
    "**On subsequent runs**, leave `reset=False` (the default) to skip re-embedding, it takes time and the store on disk is already correct. Pass `reset=True` only when the corpus or chunking strategy changes.\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Models\n",
    "Two model families are implemented in the toolkit. The choice affects retrieval quality, cost, context limit, and data security.\n",
    "\n",
    "| | `all-MiniLM-L6-v2` (default) | Other SentenceTransformer models | `text-embedding-3-small` (OpenAI) |\n",
    "|---|---|---|---|\n",
    "| **Dimensions** | 384 | 384â€“1024 (model-dependent) | 1024 (toolkit setting) |\n",
    "| **Context limit** | 256 tokens | 256â€“8 192 (model-dependent) | 8 191 tokens |\n",
    "| **Cost** | Free, local | Free, local | ~$0.02 / 1 M tokens |\n",
    "| **Data security** | Fully local | Fully local | Sent to OpenAI |\n",
    "| **Quality** | Good for short, focused text | Varies; some match OpenAI | State-of-the-art |\n",
    "| **Setup** | No API key | No API key | `OPENAI_API_KEY` required |\n",
    "\n",
    "**SentenceTransformer: free alternatives from HuggingFace**\n",
    "Any model from HuggingFace that is compatible with the `sentence-transformers` library works with our `SentenceTransformerEmbeddings` by passing a different `model_name`. Browse quality rankings on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "Hardware matters: larger models are slower on CPU. A useful rule of thumb: models under ~150 MB run comfortably on CPU; larger models benefit from a GPU.\n",
    "\n",
    "| Model | Size | CPU speed | Max input tokens (truncation limit) | Notes |\n",
    "|---|---|---|---|---|\n",
    "| `all-MiniLM-L6-v2` | 90 MB | Very fast | 256 | Default; good for short technical text |\n",
    "| `sentence-transformers/all-mpnet-base-v2` | 420 MB | Moderate | 384 | Better English quality, same 512-token limit |\n",
    "| `BAAI/bge-m3` | 2.3 GB | Very slow, GPU recommended | 8192 | Best multilingual quality; 8192-token limit |\n",
    "| `thenlper/gte-large` | 1.3 GB | Slow, GPU recommended | 512 | Strong English quality |\n",
    "\n",
    "> On Renku (CPU-only sessions), the top two rows are practical choices. \n",
    "\n",
    "**OpenAI embeddings**\n",
    "`OpenAIEmbeddings` from `conversational_toolkit.embeddings.openai` calls the OpenAI API. The toolkit requests 1024 dimensions using OpenAI's Matryoshka dimension reduction, a technique that allows truncating full embeddings to a smaller size with minimal quality loss. Requires `OPENAI_API_KEY`.\n",
    "\n",
    "```python\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c159c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vector Store\n",
    "\n",
    "A vector store persists chunk embeddings on disk and provides approximate nearest-neighbour search. Two implementations are in the toolkit.\n",
    "\n",
    "**`ChromaDBVectorStore`** (used in this notebook)\n",
    "- Embedded database: no separate server process, data stored as files on disk at `VS_PATH`.\n",
    "- Survives session restarts, which is why `reset=False` is safe by default.\n",
    "- Uses L2 distance for search. For unit-length vectors (which both embedding models produce) this gives the same ranking as cosine similarity -> different numbers, identical top-k order.\n",
    "- Well-suited for corpora up to ~100 k chunks. Not designed for concurrent writes or multi-user access.\n",
    "\n",
    "**`PGVectorStore`** (also in the toolkit)\n",
    "- PostgreSQL with the `pgvector` extension. Requires a running Postgres instance.\n",
    "- Uses cosine similarity natively (also supports other).\n",
    "- Supports rich metadata filtering, concurrent reads and writes, and standard SQL queries alongside vector search.\n",
    "- The right choice when you already have a Postgres infrastructure, need concurrent access, or want to combine vector search with relational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:15:48.495 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-22 16:15:48.593 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_vector_store:264 - Embedding 374 chunks with 'sentence-transformers/all-MiniLM-L6-v2' ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:15:50.213 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (374, 384)\n",
      "2026-02-22 16:15:50.213 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_vector_store:268 - Embedding matrix: shape=(374, 384)  dtype=float32\n",
      "2026-02-22 16:15:50.437 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_vector_store:271 - Done! Vector store written to /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/backend/data_vs.db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store ready.\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Set reset=True to rebuild the store from scratch\n",
    "vector_store = await build_vector_store(\n",
    "    chunks, embedding_model, db_path=VS_PATH, reset=False\n",
    ")\n",
    "print(\"Vector store ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-embed-md",
   "metadata": {},
   "source": [
    "### Similarity in Embedding Space\n",
    "\n",
    "Embeddings that are close in vector space share semantic meaning. The cell below embeds several sentences and measures their cosine similarity: a value between -1 (opposite) and 1 (identical). You can change the sentences to see the impact on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-embed-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = \"carbon footprint of a pallet\"\n",
    "sentence2 = \"GWP value for the Logypal 1\"\n",
    "sentence3 = \"PFAS-free tape declaration\"\n",
    "sentence4 = \"the annual report of a software firm\"\n",
    "\n",
    "\n",
    "async def cosine_similarity(a: str, b: str) -> float:\n",
    "    vecs = await embedding_model.get_embeddings([a, b])\n",
    "    return float(\n",
    "        np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (sentence1, sentence2),\n",
    "    (sentence1, sentence3),\n",
    "    (sentence1, sentence4),\n",
    "]\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for a, b in pairs:\n",
    "    sim = await cosine_similarity(a, b)\n",
    "    print(f\"{sim:.3f}  -->  {a!r}  vs  {b!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqsb3dt213",
   "metadata": {},
   "source": [
    "### Comparing Embedding Models on our Documents\n",
    "\n",
    "A quick way to compare models without running a full evaluation: pick a query, a relevant chunk, and an irrelevant chunk, then measure the **cosine similarity gap** -> how much more similar the relevant chunk is to the query than the irrelevant one. A larger gap means the model discriminates better between useful and noise results, which translates directly to higher retrieval precision.\n",
    "\n",
    "The cell below runs this for three CPU-friendly models (and OpenAI if the key is set). The HuggingFace models are downloaded on first use (~400 MB each, takes a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45sgmln8q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "\n",
    "# Test query and two chunks from the corpus\n",
    "query = \"What is the carbon footprint of the Logypal 1 pallet?\"\n",
    "\n",
    "chunk_relevant = \"Logypal 1 â€” GWP: 3.2 kg CO2e per functional unit (A1-A3). Figure verified by independent third-party auditor under ISO 14044.\"\n",
    "chunk_irrelevant = \"PrimePack AG Supplier Code of Conduct. All suppliers must comply with applicable environmental regulations and report annually on progress.\"\n",
    "\n",
    "# Models to compare (CPU-friendly by default)\n",
    "models_to_compare: dict = {\n",
    "    \"all-MiniLM-L6-v2        (90 MB, 384-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"all-MiniLM-L6-v2\"\n",
    "    ),\n",
    "    \"all-mpnet-base-v2       (420 MB, 768-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    ),\n",
    "    \"multilingual-MiniLM-L12 (470 MB, 384-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    ),\n",
    "}\n",
    "# Add OpenAI if key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    models_to_compare[\"text-embedding-3-small  (API, 1024-dim)\"] = OpenAIEmbeddings(\n",
    "        \"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "# Run comparison\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query     : {query!r}\")\n",
    "print(f\"Relevant  : {chunk_relevant[:70]!r}...\")\n",
    "print(f\"Irrelevant: {chunk_irrelevant[:70]!r}...\")\n",
    "print()\n",
    "print(f\"{'Model':<44}  {'sim(relevant)':>13}  {'sim(irrelevant)':>15}  {'gap':>6}\")\n",
    "print(\"-\" * 84)\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    vecs = await model.get_embeddings([query, chunk_relevant, chunk_irrelevant])\n",
    "    sim_rel = cosine_sim(vecs[0], vecs[1])\n",
    "    sim_irr = cosine_sim(vecs[0], vecs[2])\n",
    "    print(f\"{name:<44}  {sim_rel:>13.3f}  {sim_irr:>15.3f}  {sim_rel - sim_irr:>6.3f}\")\n",
    "\n",
    "print(\"\\nGap = sim(relevant) - sim(irrelevant). Larger gap -> better discrimination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7624e3",
   "metadata": {},
   "source": [
    "**Disclaimer**: This is a quick example, not a rigorous evaluation. A single (query, chunk) pair can be misleading â€” one model may score higher here and worse on a different example. Reliable model selection requires testing across many diverse queries and aggregating a metric like MRR or NDCG. Feature Track 2 shows how to do this systematically.\n",
    "\n",
    "> Task: Probe the comparison with your own examples\n",
    "> 1. Replace query, chunk_relevant, and chunk_irrelevant with other combinations \n",
    "> 2. Try a query where your phrasing differs (e.g. \"carbon emissions\" instead of \"GWP\"). Does any model bridge the gap better?\n",
    "> 3. Try a query in German. Does paraphrase-multilingual-MiniLM-L12-v2 show a larger gap than the English-only default?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Inspect Retrieval (Before the LLM Sees Anything)\n",
    "\n",
    "This is the **most important diagnostic step** in the whole pipeline:\n",
    "\n",
    "> If the retrieved chunks are wrong, the final answer will be wrong regardless of how good the LLM is.\n",
    "\n",
    "`inspect_retrieval()` runs the query through the embedding model, fetches the top-k most similar chunks from ChromaDB, and prints them with scores. Use this to verify that relevant documents are in the index, tune `top_k`, compare different query phrasings, and identify retrieval gaps before blaming the LLM.\n",
    "\n",
    "The **similarity score** is the L2 distance, range [0,4], lower = more similar. L2 distance is used becuase it works for any vectors, normalised or not. Cosine similarity only makes sense for direction (magnitude doesn't matter), so it requires that vectors be unit-length to be meaningful. L2 makes no such assumption, making it the safer general default. ChromaDB defaults to L2 because it's simpler to compute and works even if vector magnitudes vary. Since our embedding model always produces equal-length vectors, we get cosine-equivalent ranking. The score numbers look different, but the top-5 results would be identical either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "results = await inspect_retrieval(\n",
    "    QUERY, vector_store, embedding_model, top_k=RETRIEVER_TOP_K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed900dfc",
   "metadata": {},
   "source": [
    "> Task: Inspect the retrieved chunks\n",
    "> 1. Are all 5 chunks about the Logypal 1, or do other products appear? If so, what does that tell you about how the vector store handles similar-sounding queries?\n",
    "> 2. Change the query to a synonym or a different phrasing (e.g. \"composition of the Logypal 1\" or \"what is the Logypal 1 made of\"). Do the retrieved chunks change? Do the scores shift?\n",
    "> 3. Try reducing top_k to 2 or increasing it to 10 by passing a different retriever to inspect_retrieval. Is more context always better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-md",
   "metadata": {},
   "source": [
    "### Retrieval for a Product Outside the Portfolio\n",
    "\n",
    "The PrimePack AG product catalog defines the portfolio boundary. The **Lara Pallet** is not in the catalog, it does not exist. Watch which chunks are returned and what scores they have. A **higher** minimum score (large L2 distance) signals *weaker semantic match*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-fail-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_OOK = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "results_ook = await inspect_retrieval(\n",
    "    QUERY_OOK, vector_store, embedding_model, top_k=RETRIEVER_TOP_K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-obs-md",
   "metadata": {},
   "source": [
    "> **Observation:** The retriever always returns the *closest* chunks it can find, it has no concept of \"no match\". For an unknown product the L2 distances are **higher** (the closest chunks are still about other pallets), but without a guard the LLM receives those chunks anyway and may silently answer about the wrong product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step4-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Build the RAG Agent\n",
    "\n",
    "`build_agent()` assembles the three components:\n",
    "\n",
    "```\n",
    "VectorStoreRetriever\n",
    "    â””â”€ ChromaDBVectorStore (on disk, persists across runs)\n",
    "    â””â”€ SentenceTransformerEmbeddings\n",
    "\n",
    "RAG Agent\n",
    "    â”œâ”€ LLM (Ollama / OpenAI)\n",
    "    â”œâ”€ Retriever\n",
    "    â””â”€ System prompt\n",
    "```\n",
    "\n",
    "### The System Prompt\n",
    "\n",
    "The system prompt is the is a key lever for controlling LLM behaviour. It is prepended to every conversation and defines the rules the model must follow:\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant specialised in sustainability and product compliance\n",
    "for PrimePack AG. \n",
    "\n",
    "You will receive document excerpts relevant to the user's question. Produce the best possible answer using only the information in those excerpts.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "p0-step4-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:16:29.946 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_llm:135 - LLM backend: OpenAI (gpt-4o-mini)\n",
      "2026-02-22 16:16:29.995 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.3; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\n",
      "2026-02-22 16:16:29.996 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_agent:332 - RAG agent ready (top_k=5  query_expansion=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG agent assembled.\n"
     ]
    }
   ],
   "source": [
    "llm = build_llm(backend=BACKEND)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful AI assistant specialised in sustainability and product compliance for PrimePack AG.\\n\\n\"\n",
    "    \"You will receive document excerpts relevant to the user's question. Produce the best possible answer using only the information in those excerpts.\"\n",
    ")\n",
    "agent = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,  # 0 = no expansion; see Feature Track 3 for more\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step5-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Ask a Question\n",
    "\n",
    "`ask()` sends the query to the agent and returns the answer string. The internal flow is:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks\n",
    "3. Build the prompt: `<system>` + `<sources>` XML block + user question\n",
    "4. Generate the answer with the LLM\n",
    "5. Return the answer and a list of cited source chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-queries-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Probing Failure Modes\n",
    "\n",
    "The dataset was designed with three deliberate challenges. Run the queries below and observe the answers.\n",
    "\n",
    "### a) Out-of-Portfolio Query\n",
    "\n",
    "The **Lara Pallet** does not exist. A good RAG must say so instead of describing a different pallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "p0-query-ook-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:16:36.337 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:349 - Query: 'What materials is the Lara pallet made from?'\n",
      "2026-02-22 16:16:36.427 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Query: 'What materials is the Lara pallet made from?'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:16:38.833 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:352 - Answer:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lara pallet is made from a combination of recycled materials, specifically a mix of polypropylene and high-density polyethylene (PEHD). It is produced using secondary raw materials, making it a sustainable option compared to traditional wooden pallets. The pallet is designed to be lightweight, resistant to mold and humidity, and fully recyclable at the end of its life.\n",
      "Sources (5):\n",
      "  'ART_relicyc_logypal1_datasheet_2021.pdf'  |  '## Overview'\n",
      "  'ART_supplier_brochure_CPR_wood_pallet.pdf'  |  '## Sustainable by Nature'\n",
      "  'EPD_pallet_relicyc_logypal1.pdf'  |  '# PRODUCT INFORMATION'\n",
      "  'SPEC_pallet_CPR_plastic.pdf'  |  '## 1200 x 800 mm'\n",
      "  'SPEC_pallet_CPR_recycled_plastic.pdf'  |  '# **NOÃˆ pallet made of recycled plastic â€“ cod. PR12** 1200x800 mm'\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"What materials is the Lara pallet made from?\"\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_ook = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-gap-md",
   "metadata": {},
   "source": [
    "### b) Missing Data (LogyLight Pallet)\n",
    "\n",
    "The LogyLight datasheet marks all LCA fields as *\"not yet available\"*. The correct answer is that we don't have the data, not a fabricated figure or saying that there is no infromation on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "p0-query-gap-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:16:44.343 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:349 - Query: 'What is the GWP of the LogyLight pallet?'\n",
      "2026-02-22 16:16:44.414 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Query: 'What is the GWP of the LogyLight pallet?'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:16:47.036 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:352 - Answer:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sources provided do not contain specific information about the Global Warming Potential (GWP) of the LogyLight pallet. They discuss other products, such as the CPR Wooden Pallet and the Logypal 1, including their GWP figures, but there is no mention of the GWP for the LogyLight pallet.\n",
      "\n",
      "To obtain the GWP for the LogyLight pallet, you may need to consult Relicyc's official documentation or contact them directly for the most accurate and up-to-date information.\n",
      "Sources (5):\n",
      "  'ART_logylight_incomplete_datasheet.pdf'  |  '## Product Overview'\n",
      "  'EVALUATION_qa_ground_truth.md'  |  '## Q2: What is the GWP of the CPR Wooden Pallet (32-101)?'\n",
      "  'ART_relicyc_logypal1_datasheet_2021.pdf'  |  '## Overview'\n",
      "  'EVALUATION_qa_ground_truth.md'  |  '## Q3: What is the GWP of the Relicyc Logypal 1?'\n",
      "  'ART_supplier_brochure_CPR_wood_pallet.pdf'  |  '## Climate Impact'\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"What is the GWP of the LogyLight pallet?\"\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_gap = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-conflict-md",
   "metadata": {},
   "source": [
    "### c) Conflicting Evidence (Relicyc GWP Figures)\n",
    "\n",
    "The 2021 Relicyc datasheet reports **4.1 kg COâ‚‚e** per pallet. The 2023 EPD (third-party verified) reports a different, more recent figure. The RAG should flag the conflict and prefer the verified, more recent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "p0-query-conflict-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:17:59.255 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:349 - Query: 'What is the GWP of the Logypal 1 pallet, and how reliable is the figure?'\n",
      "2026-02-22 16:17:59.403 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Query: 'What is the GWP of the Logypal 1 pallet, and how reliable is the figure?'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:18:03.624 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:352 - Answer:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Global Warming Potential (GWP) of the Logypal 1 pallet is reported to be **4.1 kg COâ‚‚e per pallet** for a 50-trip lifetime, according to an internal, non-verified Life Cycle Assessment (LCA) from 2021. However, a more recent and authoritative source, a third-party verified Environmental Product Declaration (EPD) published in 2023 (Relicyc EPD No. S-P-10482), provides updated figures that differ from the 2021 LCA. The 2023 EPD is considered the reliable source, and the 2021 figure should no longer be cited.\n",
      "\n",
      "In summary, while the 4.1 kg COâ‚‚e figure exists, it is based on outdated data and methodology. The reliability of the GWP figure from the 2023 EPD is higher, making it the preferred reference for accurate and verified information.\n",
      "Sources (5):\n",
      "  'ART_relicyc_logypal1_datasheet_2021.pdf'  |  '## Overview'\n",
      "  'EVALUATION_qa_ground_truth.md'  |  '## Q2: What is the GWP of the CPR Wooden Pallet (32-101)?'\n",
      "  'ART_logylight_incomplete_datasheet.pdf'  |  '## Product Overview'\n",
      "  'EVALUATION_qa_ground_truth.md'  |  '## Q3: What is the GWP of the Relicyc Logypal 1?'\n",
      "  'SPEC_pallet_CPR_plastic.pdf'  |  '## 1200 x 800 mm'\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"What is the GWP of the Logypal 1 pallet, and how reliable is the figure?\"\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_conflict = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-unverified-md",
   "metadata": {},
   "source": [
    "### d) Unverified Supplier Claim (Tesa ECO Tape)\n",
    "\n",
    "The tesa supplier brochure claims **68% COâ‚‚ reduction** compared to conventional tape. This is a self-declared marketing claim, there is no independent EPD. The RAG should report the claim but flag that it is unverified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "p0-query-unverified-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:18:49.719 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:349 - Query: 'How much lower is the carbon footprint of tesa ECO tape compared to standard tape?'\n",
      "2026-02-22 16:18:49.789 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Query: 'How much lower is the carbon footprint of tesa ECO tape compared to standard tape?'\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-22 16:18:52.835 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:ask:352 - Answer:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tesa ECO tape (tesapack ECO) claims a **68% reduction in COâ‚‚ emissions per roll** compared to its conventional baseline product from 2019. However, this figure is based on an internal comparative assessment and has not been independently verified through a third-party Environmental Product Declaration (EPD) or an ISO 14044 lifecycle assessment.\n",
      "\n",
      "In contrast, standard tapes do not have a specified carbon footprint reduction percentage provided in the sources. Therefore, while we know that tesa ECO tape has a 68% lower carbon footprint than its own conventional product, we cannot directly compare it to standard tape without specific data on the carbon footprint of those standard tapes. \n",
      "\n",
      "If you need more precise comparisons, you may want to look for EPDs or lifecycle assessments for the specific standard tapes you are interested in.\n",
      "Sources (5):\n",
      "  'ART_response_inquiry_frische_felder.md'  |  '### Response to Section 1: Tape Products'\n",
      "  'ART_supplier_brochure_tesa_ECO.pdf'  |  '## A New Standard in Sustainable Sealing'\n",
      "  'ART_response_inquiry_frische_felder.md'  |  '## Incoming Customer Email'\n",
      "  'ART_supplier_brochure_tesa_ECO.pdf'  |  '### Climate performance:'\n",
      "  'REF_ecologo_catalogue.pdf'  |  '#### ~~tesa extra PowerÂ® Eco Repair ecoLogoÂ®~~'\n"
     ]
    }
   ],
   "source": [
    "QUERY = (\n",
    "    \"How much lower is the carbon footprint of tesa ECO tape compared to standard tape?\"\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_claim = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5948f40",
   "metadata": {},
   "source": [
    "> ðŸ’¬ **Discuss with your peers:** Look back at the four queries you just ran:\n",
    "> 1. Which failure modes did the RAG handle correctly, and which did it not?\n",
    "> 2. Did your LLM backend matter? If you used a different backend (OpenAI vs Ollama), compare answers. Did one backend refuse to speculate more often? Did one add caveats the other didn't?\n",
    "> 3. Which failure mode concerns you most in a real deployment?\n",
    "> 4. What is the downstream consequence? Think concretely: CSRD reporting errors, a false marketing claim to a customer, a supplier selected on unverified data.\n",
    ">\n",
    "> Can you think of other ways the system might fail that aren't shown here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-multiturn-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Turn Conversation\n",
    "\n",
    "All previous queries were standalone: one question, one answer. Real usage looks different: a user asks a follow-up question that references the previous answer (\"What about its end-of-life?\", \"Is that figure verified?\").\n",
    "\n",
    "The ask() function accepts a history argument, a list of prior LLMMessage objects, to support this. There is one subtlety: the retriever only sees the current query, not the conversation history. A follow-up like \"What about its recycled content?\" would embed the pronoun \"its\", which matches nothing in the corpus.\n",
    "\n",
    "To prevent this, when history is provided the agent first rewrites the query into a self-contained form before retrieval, for example \"What about its recycled content?\" becomes \"What is the recycled content of the Logypal 1 pallet?,\" and only then embeds and retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-multiturn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "history: list[LLMMessage] = []\n",
    "\n",
    "\n",
    "async def conversation_turn(query: str) -> str:\n",
    "    global history\n",
    "    answer = await agent.answer(QueryWithContext(query=query, history=history))\n",
    "    history.append(LLMMessage(role=Roles.USER, content=query))\n",
    "    history.append(LLMMessage(role=Roles.ASSISTANT, content=answer.content))\n",
    "    return answer.content\n",
    "\n",
    "\n",
    "QUERY1 = \"Which pallets in our portfolio have a third-party verified EPD?\"\n",
    "QUERY2 = \"What is the GWP figure reported in it for the Logypal 1?\"\n",
    "\n",
    "# Turn 1: ask about a specific product\n",
    "reply1 = await conversation_turn(QUERY1)\n",
    "print(\"---------------------------\")\n",
    "print(f\"User: {QUERY1}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Assistant: {reply1}\\n\")\n",
    "\n",
    "# Turn 2: follow-up using a pronoun â€” the agent should resolve \"it\" before retrieval\n",
    "reply2 = await conversation_turn(QUERY2)\n",
    "print(\"---------------------------\")\n",
    "print(f\"User: {QUERY2}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Assistant: {reply2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-arch-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running the Full Pipeline in One Call\n",
    "\n",
    "We have now gone through the pipeline step by step. For convenience, the `run_pipeline()` function executes all five steps end-to-end. It is also what the `__main__` entry point calls.\n",
    "\n",
    "Use it for quick one-shot queries. Use the individual step functions above when you need\n",
    "to inspect intermediate results or iterate on a specific stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-arch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import run_pipeline\n",
    "\n",
    "answer = await run_pipeline(\n",
    "    backend=BACKEND,\n",
    "    query=\"What sustainability certifications do the pallets in the portfolio have?\",\n",
    "    reset_vs=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-exercises-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tasks & Discussion\n",
    "\n",
    "Work through these in small groups. You don't need to do them all, pick what interests you or what matches your background.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ Explore & Discuss\n",
    "\n",
    "**A. Build a test set together**\n",
    "Go through the data files (run the scratch cell below to see what's there) and write down questions where the answer *is* in the documents, but also questions about products that don't exist or data that isn't there, and questions where you suspect conflicting information. Example questions can be found in the EVALUATION_qa_ground_truth.md file. You can also create new files if you think this will help to test the RAG. You can then also run the test questions through the system to evaluate the current RAG. \n",
    "\n",
    "**B. Evaluate the outputs: would you trust them?**\n",
    "Look back at the failure mode answers in Section 4. For each one: would you have trusted this answer without knowing it might be wrong? What would a *good* response look like? What would need to change in the system to prevent this failure?\n",
    "\n",
    "**C. Think about your own context**\n",
    "If you were to deploy this in your organisation, what documents would go into the knowledge base? What questions should it answer well? Who would use it, and what are the consequences of a wrong answer?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Code Experiments\n",
    "\n",
    "**1. Retrieval inspection**\n",
    "Call `inspect_retrieval()` with several different queries. Look at the L2 scores: at roughly what value do retrieved chunks stop being relevant? Could you use this as a confidence threshold?\n",
    "\n",
    "**2. Top-k sensitivity**\n",
    "Change `top_k` from 5 to 1 and re-run a query from the failure mode section. Does the answer change? Try 10. Is more context always better, or does noise creep in?\n",
    "\n",
    "**3. System prompt ablation**\n",
    "Experiment with the `SYSTEM_PROMPT`. Add rules to avoid failure modes, for example about flagging conflicting information. Rebuild the agent and re-run different queries. Does the answer change?\n",
    "\n",
    "**4. Query phrasing experiment**\n",
    "Try these different phrasing for the same underlying question. For example, try `\"COâ‚‚ footprint Logypal 1\"`, `\"carbon emissions recycled pallet\"`, and `\"GWP A1-A3 EPD pallet\"`. Do the retrieved chunks differ? Which phrasing finds the most relevant result? You can also try queries in different languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-exercises-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch cell, run your experiments here\n",
    "async def quick_retrieve(query: str, top_k: int = 5):\n",
    "    retriever = VectorStoreRetriever(embedding_model, vector_store, top_k=top_k)\n",
    "    results = await retriever.retrieve(query)\n",
    "    print(f\"Query: {query!r}  (top_k={top_k})\")\n",
    "    for r in results:\n",
    "        src = r.metadata.get(\"source_file\", \"?\")\n",
    "        print(f\"  score={r.score:.4f}  {src}  {r.title!r}\")\n",
    "\n",
    "\n",
    "await quick_retrieve(\"PFAS-free tape declaration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Function |\n",
    "|---|---|\n",
    "| 1. Load & chunk | `load_chunks(max_files)` |\n",
    "| 2. Embed & index | `build_vector_store(chunks, emb, reset)` |\n",
    "| 3. Inspect retrieval | `inspect_retrieval(query, vs, emb, top_k)` |\n",
    "| 4. Build agent | `build_agent(vs, emb, llm, top_k, system_prompt, number_query_expansion)` |\n",
    "| 5. Generate answer | `ask(agent, query, history)` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
