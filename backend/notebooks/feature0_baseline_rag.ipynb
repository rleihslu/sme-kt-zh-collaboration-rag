{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# The Baseline RAG Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is your **starting point**. The pipeline is already built and working: run it, explore its outputs, question it, and find its limits.\n",
    "\n",
    "**How to use this notebook**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| ðŸ“– **Read** | The explanations use plain language, no coding background needed |\n",
    "| â–¶ï¸ **Run** | Execute cells top to bottom with Shift+Enter to see the pipeline in action |\n",
    "| ðŸ’¬ **Discuss** | Talk about the outputs with your peers, do they make sense? Would you trust them? |\n",
    "| ðŸ”§ **Experiment** | Modify queries, tweak parameters, break things on purpose |\n",
    "| ðŸš€ **Extend** | The Tasks section points to what you can take further |\n",
    "\n",
    "> You don't need to understand every line of code. Focus on what the system gets right and wrong and on thinking about how this would apply in your own context.\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines an AI assistant with a search capability across a corpus of documents. Instead of the AI making things up from memory, it first searches your documents and then answers based on what it finds. The answer can always be traced back to a source.\n",
    "\n",
    "```\n",
    "Your question  ->  Search your documents  ->  AI answers using only those documents\n",
    "```\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| **Baseline (this notebook)** | Working baseline prototype |\n",
    "| Feature Track 1 | How to measure answer quality |\n",
    "| Feature Track 2 | Reliable, structured outputs |\n",
    "| Feature Track 3 | Better retrieval strategies |\n",
    "| Feature Track 4 | Multi-step agent workflows |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-why-rag-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why RAG? The Problem with a Standalone LLM\n",
    "\n",
    "### The Scenario\n",
    "**PrimePack AG** buys packaging materials (pallets, cardboard boxes, tape) from multiple suppliers. Sustainability claims are increasingly scrutinised by customers and regulators. Employees need to answer questions like:\n",
    "> *\"What is the GWP of the Logypal 1 pallet, and is the figure verified?\"*  \n",
    "> *\"Can we tell a customer that the tesa tape is PFAS-free?\"*  \n",
    "> *\"Which of our suppliers have a certified EPD?\"*\n",
    "\n",
    "### Why Not Just Ask ChatGPT?\n",
    "A general-purpose LLM has three fundamental problems for this task:\n",
    "\n",
    "| Problem | Why It Matters |\n",
    "|---|---|\n",
    "| **Internal document** | LLMs don't know about internal company documents. |\n",
    "| **Hallucination** | When asked about unknown products the LLM invents plausible-sounding but false figures. |\n",
    "| **No evidence trail** | Even when correct, a raw LLM answer cannot be traced back to a source document. |\n",
    "\n",
    "### The RAG Solution\n",
    "RAG adds a **retrieval step** between the user's question and the LLM:\n",
    "\n",
    "```\n",
    " Documents â”€â”€â–º Chunker â”€â”€â–º Embedder â”€â”€â–º Vector DB\n",
    "                                              â”‚\n",
    " User query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Embedder â”€â”€â”€â”€â”€â–º  Retriever â”€â”€â–º Top-k Chunks\n",
    "                                                                      â”‚\n",
    "                                                               LLM + Prompt\n",
    "                                                                      â”‚\n",
    "                                                               Answer + Sources\n",
    "```\n",
    "\n",
    "The LLM only sees documents that are **actually in the corpus**. The answer can be traced to specific source chunks. If the corpus does not contain the answer, the LLM is instructed to say so.\n",
    "\n",
    "### What RAG Does *Not* Fix\n",
    "RAG shifts the problem from hallucination to **retrieval quality**. If the right chunk is not retrieved, the answer will still be wrong (or absent). The later feature tracks address exactly this: better chunking, better retrieval, and better output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-concepts-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### Chunks\n",
    "\n",
    "A **chunk** is a short excerpt from a source document, a section of a PDF, one sheet of a spreadsheet, or one heading-delimited paragraph of a Markdown file. Chunks are the unit of indexing and retrieval.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: str           # unique identifier\n",
    "    title: str        # e.g. section heading\n",
    "    content: str      # the text that gets embedded\n",
    "    metadata: dict    # source_file, page, ...\n",
    "```\n",
    "\n",
    "### Embeddings\n",
    "An **embedding** converts text to a dense numeric vector (e.g. 384 dimensions). Semantically similar texts produce similar vectors. Here we use `all-MiniLM-L6-v2`, a compact local model that runs without an API key.\n",
    "\n",
    "### Vector Store (ChromaDB)\n",
    "A **vector store** persists chunk embeddings on disk and supports approximate nearest-neighbour search. Given a query embedding, it returns the `top_k` most similar chunks in milliseconds.\n",
    "\n",
    "### Retriever\n",
    "A **retriever** wraps a vector store and exposes a single `retrieve(query)` method. The baseline uses a `VectorStoreRetriever` with `top_k=5`.\n",
    "\n",
    "### RAG Agent\n",
    "The **RAG agent** combines a retriever and an LLM. Its `answer()` method:\n",
    "1. Embeds the query\n",
    "2. Retrieves the top-k chunks\n",
    "3. Formats chunks as XML `<source>` tags in the prompt\n",
    "4. Calls the LLM and returns the answer + cited sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` must be installed in editable mode (`pip install -e conversational-toolkit/ && pip install -e backend/`). For the **Ollama** backend, start `ollama serve` and pull the model (`ollama pull mistral-nemo:12b`). For **OpenAI**, set `OPENAI_API_KEY` in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-setup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import VectorStoreRetriever\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    load_chunks,\n",
    "    inspect_chunks,\n",
    "    build_vector_store,\n",
    "    inspect_retrieval,\n",
    "    build_agent,\n",
    "    build_llm,\n",
    "    ask,\n",
    "    DATA_DIR,\n",
    "    VS_PATH,\n",
    "    EMBEDDING_MODEL,\n",
    "    RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "# Choose your LLM backend: \"ollama\" (local, requires `ollama serve`) or \"openai\" (requires OPENAI_API_KEY)\n",
    "BACKEND = \"ollama\"  # set this before running\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError(\n",
    "        'BACKEND is not set. Edit the line above and set it to \"ollama\", or \"openai\".\\n'\n",
    "        \"See Renku_README.md for setup instructions.\"\n",
    "    )\n",
    "\n",
    "ROOT = Path().resolve().parents[1]\n",
    "print(f\"Project root : {ROOT}\")\n",
    "print(f\"Data dir     : {DATA_DIR}\")\n",
    "print(f\"Vector store : {VS_PATH}\")\n",
    "print(f\"LLM backend  : {BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0mf2j4xq79v",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before the RAG Pipeline: The LLM on Its Own\n",
    "\n",
    "A **large language model (LLM)** is a neural network trained on billions of words of text. It can summarise documents, answer questions, and generate structured output, but only from knowledge baked into its weights during training. It has no direct access to your internal documents.\n",
    "\n",
    "Before building the RAG pipeline, let's interact with the LLM directly to understand what it can and cannot do on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3tx6k25ljdx",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "# Reuse the backend you chose in the Setup cell above\n",
    "llm_standalone = build_llm(backend=BACKEND)\n",
    "\n",
    "# A question the LLM can answer from general training data\n",
    "general_question = \"What does GWP stand for, and what unit is it typically measured in?\"\n",
    "\n",
    "response_general = await llm_standalone.generate(\n",
    "    [LLMMessage(role=Roles.USER, content=general_question)]\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Q: {general_question}\\n\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"A: {response_general.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vkh4o5519",
   "metadata": {},
   "source": [
    "The LLM answers that correctly, GWP is a well-known concept covered in its training data.\n",
    "\n",
    "Now ask something specific to PrimePack AG's product portfolio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sy3pgyhowo9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A product-specific question the LLM has never seen in training\n",
    "primepack_question = \"What is the Global Warming Potential (GWP) of the Logypal 1 pallet sold by PrimePack AG, and is the figure third-party verified? Provide the link to PrimePack AG's official website.\"\n",
    "\n",
    "response_pp = await llm_standalone.generate(\n",
    "    [LLMMessage(role=Roles.USER, content=primepack_question)]\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Q: {primepack_question}\\n\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"A: {response_pp.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99690afe",
   "metadata": {},
   "source": [
    "PrimePack AG is a fictional company, no training data exists for this product\n",
    "- If the model gave a specific figure or website link: it is hallucinated.\n",
    "- If the model said 'I don't know': that is honest, but still not useful.\n",
    "\n",
    "Either way, the LLM cannot provide the actual figure with a verifiable source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a69d9b",
   "metadata": {},
   "source": [
    "> **Task: Compare LLM Backends**\n",
    "> 1. **Switch backends.** Change BACKEND to \"ollama\" (or \"openai\" if you started with Ollama) and re-run the two question cells above. Does one model hallucinate a specific figure or website link while the other declines? How confident does each answer sound?\n",
    "> 2. **Change the question.** Replace the tesa question with something you could imagine being asked in a real supplier audit. Does the standalone LLM give you an answer you would trust?\n",
    "> 3. **Note the pattern.** Regardless of whether the model hallucinates or says \"I don't know\", ask: could you send this response to a customer? What is missing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgvbri1fhj8",
   "metadata": {},
   "source": [
    "### Why do different models behave differently?\n",
    "\n",
    "**OpenAI models** (GPT-4o, GPT-4o-mini) are extensively trained with human feedback (Reinforcement Learning from Human Feedback(RLHF)) to decline when they lack reliable information. For a fictional company like PrimePack AG, with no public web presence, the model has learned to say \"I don't know\" rather than confabulate a specific figure.\n",
    "\n",
    "**Smaller local models** (Mistral, LLaMA 7â€“13 B) are typically less safety-fine-tuned. Without the reinforcement signal that penalises confident wrong answers, they are more likely to generate a plausible-sounding but fabricated answer.\n",
    "\n",
    "**The problem in either case:** \"I don't know\" and a hallucinated answer are equally useless to an employee who needs to respond to a CSRD audit. The correct response, *\"The verified GWP is X kg COâ‚‚e according to the 2023 EPD (source: EPD_pallet_relicyc_logypal1.pdf)\"*, requires access to the actual document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qw0u2tdx1kj",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Choosing a Backend: OpenAI API vs Local Models\n",
    "\n",
    "Two LLM backends are available in this workshop. Both expose the same interface, switching requires changing a single variable.\n",
    "\n",
    "#### Comparison\n",
    "\n",
    "| | **OpenAI API** (`gpt-4o-mini`, `gpt-4o`, â€¦) | **Ollama -> local** (`mistral-nemo:12b`, `llama3.2`, â€¦) |\n",
    "|---|---|---|\n",
    "| **Data security** | Queries and document chunks are sent to OpenAI's servers. You can request zero-data-retention. | Everything stays on-premise. Nothing leaves the machine. Suitable for confidential documents without any external data agreements. |\n",
    "| **Model capability** | State-of-the-art. Follows complex instructions reliably, structures output well, handles edge cases. `gpt-4o-mini` is the default for this workshop, it is much cheaper than `gpt-4o` with most of the capability for RAG tasks. | Smaller models (7â€“13 B parameters) are weaker on complex reasoning and strict rule-following. For straightforward retrieval and summarisation tasks the quality gap narrows considerably. |\n",
    "| **Cost** | Per-token billing. A typical RAG query costs a fraction of a cent. See the cost estimation section below. | No API cost: you pay for hardware (CPU/GPU) and electricity. |\n",
    "| **Setup** | One API key, no local hardware required | `ollama serve` + model download |\n",
    "\n",
    "#### Self-Hosting Larger Models\n",
    "\n",
    "The quality gap between a 12 B local model and GPT-4o can be substantially closed at larger model sizes:\n",
    "\n",
    "- **LLaMA 3.1 70 B, Mistral Large 2, Qwen 2.5 72 B**: run on GPUs. Quality can approach GPT-4o on structured tasks like RAG.\n",
    "- **Quantised models (GGUF / GPTQ):** Reduce memory requirements with a modest quality trade-off, making larger models accessible on smaller hardware.\n",
    "- **Production stacks:** `vLLM` and `llama.cpp` server provide OpenAI-compatible APIs with batching and much higher throughput than `ollama` alone. \n",
    "\n",
    "**For this workshop** `gpt-4o-mini` (OpenAI) and `mistral-nemo:12b` (Ollama) are both sufficient to demonstrate the full RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clh6qsmd1uk",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Cost Estimation\n",
    "\n",
    "API costs scale with the number of tokens processed. **Input tokens** (your system prompt and retrieved document chunks) are cheaper than **output tokens** (the model's generated answer).\n",
    "\n",
    "OpenAI API pricing for all models can be found [here](https://developers.openai.com/api/docs/pricing/). As an example, prices for `gpt-4o-mini` are:\n",
    "\n",
    "| Token type | Price |\n",
    "|---|---|\n",
    "| Input | $0.15 / 1 M tokens |\n",
    "| Output | $0.60 / 1 M tokens |\n",
    "\n",
    "A rough rule of thumb: **1 token â‰ˆ 4 characters** of English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vz7aliuggbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sme_kt_zh_collaboration_rag.feature0_ingestion import estimate_tokens\n",
    "\n",
    "# Cost estimation for gpt-4o-mini\n",
    "INPUT_PRICE_PER_TOKEN = 0.15 / 1_000_000  # USD\n",
    "OUTPUT_PRICE_PER_TOKEN = 0.60 / 1_000_000  # USD\n",
    "\n",
    "\n",
    "def estimate_cost(input_text: str, output_text: str) -> dict:\n",
    "    input_tok = estimate_tokens(input_text)\n",
    "    output_tok = estimate_tokens(output_text)\n",
    "    cost = input_tok * INPUT_PRICE_PER_TOKEN + output_tok * OUTPUT_PRICE_PER_TOKEN\n",
    "    return {\"input_tokens\": input_tok, \"output_tokens\": output_tok, \"cost_usd\": cost}\n",
    "\n",
    "\n",
    "# Simulate a typical RAG query: system prompt + 5 retrieved chunks + user question --> short generated answer\n",
    "example_input = (\n",
    "    \"You are a helpful AI assistant specialised in sustainability for PrimePack AG. \"\n",
    "    \"Answer only using the provided document excerpts. Cite your sources.\\n\\n\"\n",
    "    \"Source: EPD_pallet_relicyc_logypal1.pdf\\n\"\n",
    "    \"The Logypal 1 pallet has a declared GWP of 3.2 kg CO\\u2082e per functional unit (A1\\u2013A3), \"\n",
    "    \"verified by an independent third-party auditor under ISO\\u202014044.\\n\\n\"\n",
    "    \"Source: ART_product_catalog.md\\n\"\n",
    "    \"The Logypal 1 (Product ID: 20-100) is a recycled-plastic pallet supplied by Relicyc. \"\n",
    "    \"It is listed as the primary pallet for heavy-duty use in the PrimePack AG portfolio.\\n\\n\"\n",
    "    \"[... 3 more retrieved chunks ...]\\n\\n\"\n",
    "    \"Q: What is the GWP of the Logypal 1 pallet, and is it verified?\"\n",
    ")\n",
    "example_output = (\n",
    "    \"The Logypal 1 pallet has a GWP of 3.2\\u202fkg\\u202fCO\\u2082e per functional unit (A1\\u2013A3), \"\n",
    "    \"according to the third-party verified EPD (EPD_pallet_relicyc_logypal1.pdf). \"\n",
    "    \"The figure has been independently audited under ISO\\u202014044.\"\n",
    ")\n",
    "\n",
    "info = estimate_cost(example_input, example_output)\n",
    "print(f\"Input  : ~{info['input_tokens']:>5,} tokens (prompt + chunks + question)\")\n",
    "print(f\"Output : ~{info['output_tokens']:>5,} tokens (generated answer)\")\n",
    "print(f\"Cost   : ${info['cost_usd']:.6f} per query\")\n",
    "print()\n",
    "print(f\"At 1,000 queries / day     ->  ${info['cost_usd'] * 1_000:>8.4f} / day\")\n",
    "print(f\"At 10,000 queries / day    ->  ${info['cost_usd'] * 10_000:>8.4f} / day\")\n",
    "print(f\"At 1,000,000 queries / day ->  ${info['cost_usd'] * 1_000_000:>8.2f} / day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b0c8a",
   "metadata": {},
   "source": [
    "The largest cost driver is context length. More retrieved chunks = more input tokens.\n",
    "top_k=5 (~2,000 input tokens) is a reasonable starting point for RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4rue9d6isf",
   "metadata": {},
   "source": [
    "> **Consider:** For your use case, how many queries would the system handle per day? At what volume does the per-query cost become meaningful? Would data-security requirements push you towards a local model even at lower throughput?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fe3d0",
   "metadata": {},
   "source": [
    "---\n",
    "# RAG Pipeline\n",
    "\n",
    "Now that we have seen what an LLM can and cannot do on its own, we are ready to build the retrieval layer that makes it genuinely useful. The following five steps walk through the full RAG pipeline end-to-end, from loading the documents all the way to a sourced answer.\n",
    "\n",
    "```\n",
    " Documents â”€â”€â–º Chunker â”€â”€â–º Embedder â”€â”€â–º Vector DB\n",
    "                                              â”‚\n",
    " User query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Embedder â”€â”€â”€â”€â”€â–º  Retriever â”€â”€â–º Top-k Chunks\n",
    "                                                                      â”‚\n",
    "                                                               LLM + Prompt\n",
    "                                                                      â”‚\n",
    "                                                               Answer + Sources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-md",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load and Chunk Documents\n",
    "\n",
    "The `load_chunks()` function walks `data/` and dispatches each file to the right chunker:\n",
    "\n",
    "| Extension | Chunker | Strategy |\n",
    "|---|---|---|\n",
    "| `.pdf` | `PDFChunker` | Convert to Markdown via `pymupdf4llm`, split on `#` headings |\n",
    "| `.xlsx`, `.xls` | `ExcelChunker` | One chunk per sheet, serialised as a Markdown table |\n",
    "| `.md`, `.txt` | `MarkdownChunker` | Split on `#` headings |\n",
    "\n",
    "The result is a flat `list[Chunk]`, the same structure regardless of the original format.\n",
    "\n",
    "You can use `max_files=5` here for speed. Remove the limit (or set `None`) to load the full corpus.\n",
    "\n",
    "> **Feature Track 1** explores the importance of ingestion and alternative chunking strategies in more depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from DATA_DIR and split them into chunks.\n",
    "chunks = load_chunks(max_files=None)\n",
    "# Print a statistical summary and sampled content for visual inspection.\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Print size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "print(f\"Over 1024-char limit (â‰ˆ256 tok embedding limit): {over_limit} / {len(chunks)}\")\n",
    "print(\"\\nSuccessfully loaded and chunked the documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-inspect-md",
   "metadata": {},
   "source": [
    "### What a Chunk Looks Like\n",
    "\n",
    "Each chunk carries a `title` (the heading), the raw text `content`, and a `metadata` dict\n",
    "with the source file name. This metadata is returned alongside the answer so the user can\n",
    "trace every claim back to its origin document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-inspect-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print 3 representative chunks\n",
    "for c in chunks[:3]:\n",
    "    print(f\"--- [{c.metadata.get('source_file', '?')}] ---\")\n",
    "    print(f\"Title  : {c.title!r}\")\n",
    "    print(f\"Length : {len(c.content)} chars\")\n",
    "    print(f\"Preview: {c.content[:200].strip()!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pymupdf4llm  # type: ignore[import-untyped]\n",
    "\n",
    "from conversational_toolkit.chunking.excel_chunker import ExcelChunker\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_ingestion import (\n",
    "    header_based_chunks,\n",
    "    fixed_size_chunks,\n",
    "    paragraph_aware_chunks,\n",
    "    analyze_chunks,\n",
    "    compare_strategies,\n",
    "    print_comparison_table,\n",
    "    char_histogram,\n",
    ")\n",
    "\n",
    "# We analyse a single document in detail â€” the Relicyc Logypal 1 EPD.\n",
    "# `chunks_header` is separate from `chunks` (the full corpus loaded above).\n",
    "sample_pdf = str(DATA_DIR / \"EPD_pallet_relicyc_logypal1.pdf\")\n",
    "chunks_header = header_based_chunks(sample_pdf)\n",
    "print(\n",
    "    f\"Header-based chunking: {len(chunks_header)} chunks from {Path(sample_pdf).name}\"\n",
    ")\n",
    "print(f\"Titles: {[c.title for c in chunks_header[:4]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-header-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ingestion & Chunking: Under the Hood\n",
    "\n",
    "The chunks you loaded above are the **output** of the ingestion pipeline. This section opens up the pipeline and works through the challenges practitioners face:\n",
    "\n",
    "1. **Parser choice**: which tool converts your PDF to text, and what gets lost\n",
    "2. **Layout quality**: heading detection, multi-column text, dropped content\n",
    "3. **Chunk size**: why it matters for retrieval and comprehension\n",
    "4. **Chunking strategies**: three approaches compared side by side\n",
    "5. **Tables**: in PDFs and spreadsheets\n",
    "6. **Images**: what happens to figures and diagrams\n",
    "7. **Custom chunkers**: extending the pipeline for your own document types\n",
    "8. **The embedding token limit**: one constraint to be aware of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-parser-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. PDF Parser Choice\n",
    "\n",
    "Before any chunking, a PDF must be converted to text. The choice of parser determines what survives (tables, images, headings, multi-column layouts) and how fast the pipeline runs.\n",
    "\n",
    "| Tool | What it is | Strength | Weakness | Tables (as) | Images (extraction) | Positions (bbox/coords) |\n",
    "|---|---|---|---|---|---|---|\n",
    "| **`PyMuPDF`** | Fast low-level PDF parser | Speed + layout coords | Raw output, DIY chunking | Text (heuristics)  | Native raw extraction | Yes (strong) |\n",
    "| **`PyMuPDF4LLM`** âœ“ | PyMuPDF -> Markdown chunks | LLM-ready, easy | Less flexible | Markdown | Limited / mostly references | Yes |\n",
    "| **`Docling`** | Structured document AI parser | Best layout & tables | Heavy, slower | Structured -> Markdown/JSON | Advanced extraction & understanding | Yes |\n",
    "| **`Unstructured`** | Multi-format ingestion pipeline | Many file types | Heavy deps, slower   | Element types (table-ish) | Raw extraction via elements   | Parser-dependent  |\n",
    "| **`PyPDF`** | Lightweight pure-Python reader | Simple & light | Poor layout quality | Text | Raw extraction only | Limited |\n",
    "\n",
    "**Why we use `pymupdf4llm`?** It is fast enough for interactive use, outputs clean Markdown with headings and tables already formatted, and requires no GPU. For production pipelines where table accuracy is critical it makes sense to explore other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-layout-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Layout Quality\n",
    "\n",
    "Converting a PDF to text is lossy. Understanding *what* gets lost and *why* is essential for building a reliable ingestion pipeline.\n",
    "\n",
    "**Common layout quality challenges:**\n",
    "\n",
    "| Challenge | What happens | Effect on retrieval |\n",
    "|---|---|---|\n",
    "| **Content before the first heading** | Everything before `# Heading 1` is silently dropped | Cover page metadata, product IDs, version numbers can disappear |\n",
    "| **Heading detection errors** | Lines that look like headings (e.g. short bold lines in a table) may become false heading boundaries | Chunks split at the wrong place; small noise chunks created |\n",
    "| **Multi-column layouts** | Column text is linearised left-to-right across the full page width | Sentences from different columns are merged, producing incoherent text |\n",
    "| **Tables** | Rendered as Markdown pipe tables, column headers may not appear in each row chunk | Query about a specific row may not match because the product name is only in the header |\n",
    "| **Images and figures** | Default: silently dropped | Figures, diagrams, and chart data are invisible to a text-only retriever |\n",
    "| **Headers/footers** | Page numbers and document titles may appear in the body text | Noise chunks of 2â€“3 words are created |\n",
    "\n",
    "The cell below inspects the raw Markdown for the Relicyc EPD. Read through it carefully, some of the quality issues above should be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-layout-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_markdown: str = pymupdf4llm.to_markdown(sample_pdf)  # type: ignore[assignment]\n",
    "\n",
    "heading_pattern = re.compile(r\"^(#{1,6}\\s.*)$\", re.MULTILINE)\n",
    "heading_matches = list(heading_pattern.finditer(raw_markdown))\n",
    "\n",
    "# Summary\n",
    "print(f\"Total characters : {len(raw_markdown)}\")\n",
    "print(f\"Total lines      : {raw_markdown.count(chr(10))}\")\n",
    "print(f\"Headings found   : {len(heading_matches)}\")\n",
    "\n",
    "# Heading map\n",
    "print(\"\\nHeading map:\")\n",
    "for m in heading_matches[:12]:\n",
    "    level = m.group(1).count(\"#\", 0, m.group(1).find(\" \"))\n",
    "    indent = \"  \" * (level - 1)\n",
    "    print(f\"  char {m.start():5d}  {indent}{m.group(1).strip()}\")\n",
    "if len(heading_matches) > 12:\n",
    "    print(f\"  ... ({len(heading_matches) - 12} more)\")\n",
    "\n",
    "# Dropped region\n",
    "first_heading_pos = heading_matches[0].start() if heading_matches else len(raw_markdown)\n",
    "dropped = raw_markdown[:first_heading_pos]\n",
    "print(f\"\\nDropped (before first heading): {len(dropped)} chars\")\n",
    "if dropped.strip():\n",
    "    print(dropped[:1500])\n",
    "    if len(dropped) > 1500:\n",
    "        print(f\"... [{len(dropped) - 1500} more chars]\")\n",
    "else:\n",
    "    print(\"(nothing dropped â€” document starts with a heading)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-layout-discuss-md",
   "metadata": {},
   "source": [
    "> ðŸ’¬ **Discuss:** Scroll through the heading map and dropped region above.\n",
    "> 1. Does the dropped content contain anything a user might query?\n",
    "> 2. Are there any headings that look like false positives (not real section titles)?\n",
    "> 3. For your own organisation's documents, which layout quality challenge concerns you most?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-chunksize-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Chunk Size: Why It Matters\n",
    "\n",
    "Chunk size affects retrieval quality in two opposite directions:\n",
    "\n",
    "| Chunk too **small** | Chunk too **large** |\n",
    "|---|---|\n",
    "| Loses surrounding context -> the chunk makes no sense in isolation | Mixes multiple topics -> the embedding averages over too many ideas |\n",
    "| May not mention the product name or entity the data is about | LLM receives noisy context -> relevant sentence is buried |\n",
    "| Many chunks -> slower index build, higher storage | Risk of exceeding the embedding model's token limit (more on this in the token-limit section below) |\n",
    "\n",
    "The histogram below shows the character-length distribution for the current chunking strategy. **~4 characters â‰ˆ 1 token** for English technical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-chunksize-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analyze_chunks(chunks_header, \"header_based\")\n",
    "print(stats)\n",
    "print()\n",
    "print(\"Character length distribution (1 bar â‰ˆ 1 bucket of lengths):\")\n",
    "print(char_histogram(chunks_header))\n",
    "\n",
    "print(\"\\nPrinting the first 3 chunks as examples:\")\n",
    "for chunk in chunks_header[:3]:\n",
    "    print(f\"--- {chunk.title or '(no title)'} ---\")\n",
    "    print(chunk.content[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-chunksize-interpret-md",
   "metadata": {},
   "source": [
    "**Reading the histogram:** Each bucket spans a range of character lengths. A bar in the `[100â€“524)` bucket means that many chunks land in that range.\n",
    "\n",
    "A good target for `all-MiniLM-L6-v2` (the local embedding model used here) is **under ~1000 characters, so under ~250 tokens**. Longer chunks risk truncation, we will demonstrate this experimentally in the token-limit section below.\n",
    "\n",
    "> **Step 2 preview:** Embedding models have a maximum input length. For `all-MiniLM-L6-v2` this is 256 tokens. Step 2 covers embeddings in more detail. Chunk size is one lever; embedding model choice is another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-strategies-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Chunking Strategies\n",
    "\n",
    "We can test two other strategies. The cells below run all three on the same PDF and print a side-by-side comparison table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-fixed-md",
   "metadata": {},
   "source": [
    "**Fixed-Size** cuts every `chunk_size` characters with an `overlap` for boundary context. Guarantees an upper bound on chunk length but may split mid-sentence or mid-table row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-fixed-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_fixed = fixed_size_chunks(sample_pdf, chunk_size=800, overlap=100)\n",
    "print(f\"Fixed-size: {len(chunks_fixed)} chunks\")\n",
    "print(analyze_chunks(chunks_fixed, \"fixed_size_800\"))\n",
    "print()\n",
    "print(char_histogram(chunks_fixed))\n",
    "\n",
    "print(\"\\nPrinting the first 3 chunks as examples:\")\n",
    "for chunk in chunks_fixed[:3]:\n",
    "    print(f\"--- {chunk.title or '(no title)'} ---\")\n",
    "    print(chunk.content[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-para-md",
   "metadata": {},
   "source": [
    "**Paragraph-Aware** respects double-newline boundaries and merges consecutive paragraphs until a `target_chars` ceiling is reached. Never splits mid-sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-para-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_para = paragraph_aware_chunks(sample_pdf, target_chars=600)\n",
    "print(f\"Paragraph-aware: {len(chunks_para)} chunks\")\n",
    "print(analyze_chunks(chunks_para, \"paragraph_600\"))\n",
    "print()\n",
    "print(char_histogram(chunks_para))\n",
    "\n",
    "print(\"\\nPrinting the first 3 chunks as examples:\")\n",
    "for chunk in chunks_para[:3]:\n",
    "    print(f\"--- {chunk.title or '(no title)'} ---\")\n",
    "    print(chunk.content[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-compare-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_strategies(sample_pdf)\n",
    "print_comparison_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603f2f5",
   "metadata": {},
   "source": [
    "> When would you use which chunking strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-tables-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Tables: PDFs and Spreadsheets\n",
    "\n",
    "**Tables in PDFs** are rendered as Markdown pipe tables. The problem: a table chunk full of numbers may not mention key information like the product name, weakening the semantic match for queries like *\"GWP of the Logypal 1\"*. If the heading chunk above the table does contain the product name, the retriever may return the heading chunk but miss the actual data.\n",
    "\n",
    "**Spreadsheets (`.xlsx`)** are chunked differently: `ExcelChunker` creates one chunk per sheet, serialising the sheet as a Markdown-like table. This preserves column structure but produces large chunks for wide sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-tables-pdf-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF tables\n",
    "table_chunks = [c for c in chunks_header if c.content.count(\"|\") >= 8]\n",
    "print(\n",
    "    f\"Chunks with tables (pipe heuristic): {len(table_chunks)} of {len(chunks_header)}\"\n",
    ")\n",
    "print()\n",
    "for tc in table_chunks[1:2]:\n",
    "    product_mentioned = any(\n",
    "        t in tc.content.lower() for t in [\"logypal\", \"relicyc\", \"32-103\"]\n",
    "    )\n",
    "    print(f\"Title              : {tc.title!r}\")\n",
    "    print(f\"Product name in chunk: {product_mentioned}\")\n",
    "    print(\"Content preview:\")\n",
    "    print(tc.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc300e",
   "metadata": {},
   "source": [
    "**The table content was silently dropped by the parser**\n",
    "\n",
    "The chunk does contain a table, but look closely at the extracted content:\n",
    "\n",
    "|Col1|Col2|Col3|Col4|Col5|Col6|Col7| \n",
    "|---|---|---|---|---|---|---| \n",
    "|||||||| |||||||| |||||||| |||||||| |||||||| |||||||| |||||||15|\n",
    "\n",
    "`pymupdf4llm` detected the table structure but could not read the cell content. The result is a skeleton of empty rows with auto-generated column names (`Col1`...`Col7`) and a single stray value (`15`). The actual header labels and data are gone. The table caption (`_Table 3: Content declaration of pallet Logypal 1_`) survived only because it is regular paragraph text, not part of the table element.\n",
    "\n",
    "**Why does this happen?**  \n",
    "PDF tables have no universal standard. Some PDFs encode tables as proper table structures; others draw them as lines and position text independently. When `pymupdf4llm` cannot reliably map text to cells merged cells, or text positioned outside the detected grid, it falls back to empty placeholders rather than guessing wrong values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-tables-excel-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel / spreadsheets\n",
    "xlsx_file = str(DATA_DIR / \"product_overview.xlsx\")\n",
    "xl_chunks = ExcelChunker().make_chunks(xlsx_file)\n",
    "print(f\"Excel chunks: {len(xl_chunks)} (one per sheet)\")\n",
    "for c in xl_chunks:\n",
    "    print(f\"  Sheet '{c.title}': {len(c.content)} chars\")\n",
    "    print(f\"  Preview: {c.content.strip()!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dcdcf0",
   "metadata": {},
   "source": [
    "> Inspect the table obtained form `product_overview.xlsx` with the ExcelChunker. Is it correct?\n",
    "\n",
    "|  | Product Category | ID | Product Name | Supplier | EPD |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "|  | Tape | 50-100 | Pressure-Sensitive Hot Melt Carton Sealing Tape | ipg | yes |\n",
    "|  |  | 50-101 | Water-Activated Tape | ipg | yes |\n",
    "|  |  | 50-102 | tesapack ECO & ULTRA STRONG ecoLogo | tesa | no |\n",
    "|  | Pallets | 32-100 | NoÃ© pallet | CPR System | yes |\n",
    "|  |  | 32-101 | Wooden pallet | CPR System | no |\n",
    "|  |  | 32-102 | Plastic pallet | CPR System | no |\n",
    "|  |  | 32-103 | Logypal 1 | Relicyc | yes |\n",
    "|  |  | 32-104 | LogyLight | Relicyc | no |\n",
    "|  |  | 32-105 | Plastic pallet EP 08 Â® | StabilPlastik | yes |\n",
    "|  | Cardboard boxes | 11-100 | Cartonpallet CMP Roserio | redbox | yes |\n",
    "|  |  | 11-101 | Corrugated cardboard packaging | Grupak | yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-images-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Images in PDFs\n",
    "\n",
    "By default `PDFChunker` uses `write_images=False`, **images are silently dropped**. The text-only Markdown output contains no trace of any figure or diagram. For sustainability documents this can be significant: LCA system boundaries, process flow diagrams, and certifications labels are often only communicated as images.\n",
    "\n",
    "With `write_images=True`, images are extracted to disk and a reference `![image_name](path)` is injected into the Markdown at the correct position. However: **the current embedding model cannot see images**, it embeds the reference string `\"![Figure 3](figure_3.png)\"`, which carries essentially no semantic content.\n",
    "\n",
    "| Possible approach | What the retriever sees |\n",
    "|---|---|\n",
    "| `write_images=False` (default) | Nothing, images dropped | \n",
    "| `write_images=True` | Image filename as text reference | \n",
    "| Vision LLM caption -> embed caption | Rich text description of the image |\n",
    "| Multimodal embedding model | Image vector + text vector fused |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-images-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count image references in the default (write_images=False) raw Markdown\n",
    "img_refs = re.findall(r\"!\\[.*?\\]\\(.*?\\)\", raw_markdown)\n",
    "print(f\"Image references in raw Markdown (write_images=False): {len(img_refs)}\")\n",
    "if img_refs:\n",
    "    print(\"References found:\")\n",
    "    for r in img_refs[:5]:\n",
    "        print(f\"  {r}\")\n",
    "else:\n",
    "    print(\"No image references -> all figures are dropped by default.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-custom-chunker-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Custom Chunkers\n",
    "\n",
    "Every chunker inherits from `Chunker`, a simple abstract base class with a single method:\n",
    "\n",
    "```python\n",
    "class Chunker(ABC):\n",
    "    @abstractmethod\n",
    "    def make_chunks(self, *args, **kwargs) -> list[Chunk]:\n",
    "        ...\n",
    "```\n",
    "\n",
    "If none of the built-in chunkers fit your document type, e.g. supplier data in a CSV, you can implement your own in a few lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-token-limit-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8. The Embedding Token Limit\n",
    "\n",
    "`all-MiniLM-L6-v2` truncates its input at **256 tokens** (~1,024 characters). Text beyond this is silently dropped *before* the embedding is computed, the retriever never sees the tail of long chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-token-limit-openai-md",
   "metadata": {},
   "source": [
    "> **Quick fix: embedding models with higher token limit** For example OpenAIs `text-embedding-3-small` accepts up to 8,191 tokens, eliminating the truncation problem entirely for this corpus. The `OpenAIEmbeddings` class in the toolkit is a drop-in replacement. The tradeoff is an API key, a per-call cost, and a network dependency. For a workshop on a laptop, the local model is fine; for a production system handling documents with large sections it might be worth switching.\n",
    ">\n",
    "> Step 2 covers embedding models in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-md",
   "metadata": {},
   "source": [
    "---\n",
    "After this deep-dive into parsing and chunking, let's go to the next step in the RAG pipeline:\n",
    "\n",
    "## Step 2: Embed Chunks and Build the Vector Store\n",
    "\n",
    "`SentenceTransformerEmbeddings` converts every chunk's `content` to a 384-dimensional vector using `all-MiniLM-L6-v2`. The resulting matrix (shape `[n_chunks, 384]`) is inserted into a persistent `ChromaDBVectorStore`.\n",
    "\n",
    "**On subsequent runs**, leave `reset=False` (the default) to skip re-embedding, it takes time and the store on disk is already correct. Pass `reset=True` only when the corpus or chunking strategy changes.\n",
    "\n",
    "---\n",
    "\n",
    "### Embedding Models\n",
    "Two model families are implemented in the toolkit. The choice affects retrieval quality, cost, context limit, and data security.\n",
    "\n",
    "| | `all-MiniLM-L6-v2` (default) | Other SentenceTransformer models | `text-embedding-3-small` (OpenAI) |\n",
    "|---|---|---|---|\n",
    "| **Dimensions** | 384 | 384â€“1024 (model-dependent) | 1024 (toolkit setting) |\n",
    "| **Context limit** | 256 tokens | 256â€“8 192 (model-dependent) | 8 191 tokens |\n",
    "| **Cost** | Free, local | Free, local | ~$0.02 / 1 M tokens |\n",
    "| **Data security** | Fully local | Fully local | Sent to OpenAI |\n",
    "| **Quality** | Good for short, focused text | Varies; some match OpenAI | State-of-the-art |\n",
    "| **Setup** | No API key | No API key | `OPENAI_API_KEY` required |\n",
    "\n",
    "**SentenceTransformer: free alternatives from HuggingFace**\n",
    "Any model from HuggingFace that is compatible with the `sentence-transformers` library works with our `SentenceTransformerEmbeddings` by passing a different `model_name`. Browse quality rankings on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "\n",
    "Hardware matters: larger models are slower on CPU. A useful rule of thumb: models under ~150 MB run comfortably on CPU; larger models benefit from a GPU.\n",
    "\n",
    "| Model | Size | CPU speed | Max input tokens (truncation limit) | Notes |\n",
    "|---|---|---|---|---|\n",
    "| `all-MiniLM-L6-v2` | 90 MB | Very fast | 256 | Default; good for short technical text |\n",
    "| `sentence-transformers/all-mpnet-base-v2` | 420 MB | Moderate | 384 | Better English quality, same 512-token limit |\n",
    "| `BAAI/bge-m3` | 2.3 GB | Very slow, GPU recommended | 8192 | Best multilingual quality; 8192-token limit |\n",
    "| `thenlper/gte-large` | 1.3 GB | Slow, GPU recommended | 512 | Strong English quality |\n",
    "\n",
    "> On Renku (CPU-only sessions), the top two rows are practical choices. \n",
    "\n",
    "**OpenAI embeddings**\n",
    "`OpenAIEmbeddings` from `conversational_toolkit.embeddings.openai` calls the OpenAI API. The toolkit requests 1024 dimensions using OpenAI's Matryoshka dimension reduction, a technique that allows truncating full embeddings to a smaller size with minimal quality loss. Requires `OPENAI_API_KEY`.\n",
    "\n",
    "```python\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "embedding_model = OpenAIEmbeddings(model_name=\"text-embedding-3-small\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c159c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vector Store\n",
    "\n",
    "A vector store persists chunk embeddings on disk and provides approximate nearest-neighbour search. Two implementations are in the toolkit.\n",
    "\n",
    "**`ChromaDBVectorStore`** (used in this notebook)\n",
    "- Embedded database: no separate server process, data stored as files on disk at `VS_PATH`.\n",
    "- Survives session restarts, which is why `reset=False` is safe by default.\n",
    "- Uses L2 distance for search. For unit-length vectors (which both embedding models produce) this gives the same ranking as cosine similarity -> different numbers, identical top-k order.\n",
    "- Well-suited for corpora up to ~100 k chunks. Not designed for concurrent writes or multi-user access.\n",
    "\n",
    "**`PGVectorStore`** (also in the toolkit)\n",
    "- PostgreSQL with the `pgvector` extension. Requires a running Postgres instance.\n",
    "- Uses cosine similarity natively (also supports other).\n",
    "- Supports rich metadata filtering, concurrent reads and writes, and standard SQL queries alongside vector search.\n",
    "- The right choice when you already have a Postgres infrastructure, need concurrent access, or want to combine vector search with relational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Set reset=True to rebuild the store from scratch\n",
    "vector_store = await build_vector_store(\n",
    "    chunks, embedding_model, db_path=VS_PATH, reset=False\n",
    ")\n",
    "print(\"Vector store ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-embed-md",
   "metadata": {},
   "source": [
    "### Similarity in Embedding Space\n",
    "\n",
    "Embeddings that are close in vector space share semantic meaning (for visualisation of embeddings in the vector space look at this [website](https://projector.tensorflow.org)). The cell below embeds several sentences and measures their cosine similarity: a value between -1 (opposite) and 1 (identical). You can change the sentences to see the impact on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-embed-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"carbon footprint of a pallet\"\n",
    "sentence2 = \"GWP value for the Logypal 1\"\n",
    "sentence3 = \"PFAS-free tape declaration\"\n",
    "sentence4 = \"the annual report of a software firm\"\n",
    "\n",
    "\n",
    "async def cosine_similarity(a: str, b: str) -> float:\n",
    "    vecs = await embedding_model.get_embeddings([a, b])\n",
    "    return float(\n",
    "        np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (sentence1, sentence2),\n",
    "    (sentence1, sentence3),\n",
    "    (sentence1, sentence4),\n",
    "]\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for a, b in pairs:\n",
    "    sim = await cosine_similarity(a, b)\n",
    "    print(f\"{sim:.3f}  -->  {a!r}  vs  {b!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqsb3dt213",
   "metadata": {},
   "source": [
    "### Comparing Embedding Models on our Documents\n",
    "\n",
    "A quick way to compare models without running a full evaluation: pick a query, a relevant chunk, and an irrelevant chunk, then measure the **cosine similarity gap** -> how much more similar the relevant chunk is to the query than the irrelevant one. A larger gap means the model discriminates better between useful and noise results, which translates directly to higher retrieval precision.\n",
    "\n",
    "The cell below runs this for three CPU-friendly models (and OpenAI if the key is set). The HuggingFace models are downloaded on first use (~400 MB each, takes a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45sgmln8q9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "\n",
    "# Test query and two chunks from the corpus\n",
    "query = \"What is the carbon footprint of the Logypal 1 pallet?\"\n",
    "\n",
    "chunk_relevant = \"Logypal 1 â€” GWP: 3.2 kg CO2e per functional unit (A1-A3). Figure verified by independent third-party auditor under ISO 14044.\"\n",
    "chunk_irrelevant = \"PrimePack AG Supplier Code of Conduct. All suppliers must comply with applicable environmental regulations and report annually on progress.\"\n",
    "\n",
    "# Models to compare (CPU-friendly by default)\n",
    "models_to_compare: dict = {\n",
    "    \"all-MiniLM-L6-v2 (90 MB, 384-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"all-MiniLM-L6-v2\"\n",
    "    ),\n",
    "    \"all-mpnet-base-v2 (420 MB, 768-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    ),\n",
    "    \"multilingual-MiniLM-L12 (470 MB, 384-dim)\": SentenceTransformerEmbeddings(\n",
    "        \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "    ),\n",
    "}\n",
    "# Add OpenAI if key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    models_to_compare[\"text-embedding-3-small  (API, 1024-dim)\"] = OpenAIEmbeddings(\n",
    "        \"text-embedding-3-small\"\n",
    "    )\n",
    "\n",
    "# Run comparison\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query     : {query!r}\")\n",
    "print(f\"Relevant  : {chunk_relevant[:70]!r}...\")\n",
    "print(f\"Irrelevant: {chunk_irrelevant[:70]!r}...\")\n",
    "print()\n",
    "print(f\"{'Model':<44}  {'sim(relevant)':>13}  {'sim(irrelevant)':>15}  {'gap':>6}\")\n",
    "print(\"-\" * 84)\n",
    "\n",
    "for name, model in models_to_compare.items():\n",
    "    vecs = await model.get_embeddings([query, chunk_relevant, chunk_irrelevant])\n",
    "    sim_rel = cosine_sim(vecs[0], vecs[1])\n",
    "    sim_irr = cosine_sim(vecs[0], vecs[2])\n",
    "    print(f\"{name:<44}  {sim_rel:>13.3f}  {sim_irr:>15.3f}  {sim_rel - sim_irr:>6.3f}\")\n",
    "\n",
    "print(\"\\nGap = sim(relevant) - sim(irrelevant). Larger gap -> better discrimination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7624e3",
   "metadata": {},
   "source": [
    "**Disclaimer**: This is a quick example, not a rigorous evaluation. A single (query, chunk) pair can be misleading -> one model may score higher here and worse on a different example. Reliable model selection requires testing across many diverse queries and aggregating a metric like MRR or NDCG. Feature Track 2 shows how to do this systematically.\n",
    "\n",
    "> Task: Probe the comparison with your own examples\n",
    "> 1. Replace query, chunk_relevant, and chunk_irrelevant with other combinations \n",
    "> 2. Try a query where your phrasing differs (e.g. \"carbon emissions\" instead of \"GWP\"). Does any model bridge the gap better?\n",
    "> 3. Try a query in German. Does paraphrase-multilingual-MiniLM-L12-v2 show a larger gap than the English-only default?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Inspect Retrieval (Before the LLM Sees Anything)\n",
    "\n",
    "This is the **most important diagnostic step** in the whole pipeline:\n",
    "\n",
    "> If the retrieved chunks are wrong, the final answer will be wrong regardless of how good the LLM is.\n",
    "\n",
    "`inspect_retrieval()` runs the query through the embedding model, fetches the top-k most similar chunks from ChromaDB, and prints them with scores. Use this to verify that relevant documents are in the index, tune `top_k`, compare different query phrasings, and identify retrieval gaps before blaming the LLM.\n",
    "\n",
    "The **similarity score** is the L2 distance, range [0,4], lower = more similar. L2 distance is used becuase it works for any vectors, normalised or not. Cosine similarity only makes sense for direction (magnitude doesn't matter), so it requires that vectors be unit-length to be meaningful. L2 makes no such assumption, making it the safer general default. ChromaDB defaults to L2 because it's simpler to compute and works even if vector magnitudes vary. When the embedding model produces equal-length vectors, we get cosine-equivalent ranking. The score numbers look different, but the top-5 results would be identical either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "results = await inspect_retrieval(\n",
    "    QUERY, vector_store, embedding_model, top_k=RETRIEVER_TOP_K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed900dfc",
   "metadata": {},
   "source": [
    "> Task: Inspect the retrieved chunks\n",
    "> 1. Are all 5 chunks about the Logypal 1, or do other products appear? If so, what does that tell you about how the vector store handles similar-sounding queries?\n",
    "> 2. Change the query to a synonym or a different phrasing (e.g. \"composition of the Logypal 1\" or \"what is the Logypal 1 made of\"). Do the retrieved chunks change? Do the scores shift?\n",
    "> 3. Try reducing top_k to 2 or increasing it to 10 by passing a different retriever to inspect_retrieval. Is more context always better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-md",
   "metadata": {},
   "source": [
    "### Retrieval for a Product Outside the Portfolio\n",
    "\n",
    "The PrimePack AG product catalog defines the portfolio boundary. The **Lara Pallet** is not in the catalog, it does not exist. Watch which chunks are returned and what scores they have. A **higher** minimum score (large L2 distance) signals *weaker semantic match*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-fail-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_OOK = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "results_ook = await inspect_retrieval(\n",
    "    QUERY_OOK, vector_store, embedding_model, top_k=RETRIEVER_TOP_K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-obs-md",
   "metadata": {},
   "source": [
    "> **Observation:** The retriever always returns the *closest* chunks it can find, it has no concept of \"no match\". For an unknown product the L2 distances are **higher** (the closest chunks are still about other pallets), but without a guard the LLM receives those chunks anyway and may silently answer about the wrong product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step4-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Build the RAG Agent\n",
    "\n",
    "`build_agent()` assembles the three components:\n",
    "\n",
    "```\n",
    "VectorStoreRetriever\n",
    "    â””â”€ ChromaDBVectorStore (on disk, persists across runs)\n",
    "    â””â”€ SentenceTransformerEmbeddings\n",
    "\n",
    "RAG Agent\n",
    "    â”œâ”€ LLM (Ollama / OpenAI)\n",
    "    â”œâ”€ Retriever\n",
    "    â””â”€ System prompt\n",
    "```\n",
    "\n",
    "### The System Prompt\n",
    "\n",
    "The system prompt is the is a key lever for controlling LLM behaviour. It is prepended to every conversation and defines the rules the model must follow:\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant specialised in sustainability and product compliance\n",
    "for PrimePack AG. \n",
    "\n",
    "You will receive document excerpts relevant to the user's question. Produce the best possible answer using only the information in those excerpts.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step4-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = build_llm(backend=BACKEND)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful AI assistant specialised in sustainability and product compliance for PrimePack AG.\\n\\n\"\n",
    "    \"You will receive document excerpts relevant to the user's question. Produce the best possible answer using only the information in those excerpts.\"\n",
    ")\n",
    "agent = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,  # 0 = no expansion; see Feature Track 3 for more\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step5-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Ask a Question\n",
    "\n",
    "`ask()` sends the query to the agent and returns the answer string. The internal flow is:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks\n",
    "3. Build the prompt: `<system>` + `<sources>` XML block + user question\n",
    "4. Generate the answer with the LLM\n",
    "5. Return the answer and a list of cited source chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step5-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-queries-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Probing Failure Modes\n",
    "\n",
    "The dataset was designed with three deliberate challenges. Run the queries below and observe the answers.\n",
    "\n",
    "### a) Out-of-Portfolio Query\n",
    "\n",
    "The **Lara Pallet** does not exist. A good RAG must say so instead of describing a different pallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-ook-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What materials is the Lara pallet made from?\"\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_ook = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-gap-md",
   "metadata": {},
   "source": [
    "### b) Missing Data (LogyLight Pallet)\n",
    "\n",
    "The LogyLight datasheet marks all LCA fields as *\"not yet available\"*. The correct answer is that we don't have the data, not a fabricated figure or saying that there is no infromation on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-gap-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What is the GWP of the LogyLight pallet?\"\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_gap = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-conflict-md",
   "metadata": {},
   "source": [
    "### c) Conflicting Evidence (Relicyc GWP Figures)\n",
    "\n",
    "The 2021 Relicyc datasheet reports **4.1 kg COâ‚‚e** per pallet. The 2023 EPD (third-party verified) reports a different, more recent figure. The RAG should flag the conflict and prefer the verified, more recent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-conflict-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What is the GWP of the Logypal 1 pallet, and how reliable is the figure?\"\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_conflict = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-unverified-md",
   "metadata": {},
   "source": [
    "### d) Unverified Supplier Claim (Tesa ECO Tape)\n",
    "\n",
    "The tesa supplier brochure claims **68% COâ‚‚ reduction** compared to conventional tape. This is a self-declared marketing claim, there is no independent EPD. The RAG should report the claim but flag that it is unverified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-unverified-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "    \"How much lower is the carbon footprint of tesa ECO tape compared to standard tape?\"\n",
    ")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Query: {QUERY!r}\")\n",
    "print(\"---------------------------\")\n",
    "answer_claim = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5948f40",
   "metadata": {},
   "source": [
    "> ðŸ’¬ **Discuss with your peers:** Look back at the four queries you just ran:\n",
    "> 1. Which failure modes did the RAG handle correctly, and which did it not?\n",
    "> 2. Did your LLM backend matter? If you used a different backend (OpenAI vs Ollama), compare answers. Did one backend refuse to speculate more often? Did one add caveats the other didn't?\n",
    "> 3. Which failure mode concerns you most in a real deployment?\n",
    "> 4. What is the downstream consequence? Think concretely: CSRD reporting errors, a false marketing claim to a customer, a supplier selected on unverified data.\n",
    ">\n",
    "> Can you think of other ways the system might fail that aren't shown here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-multiturn-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Turn Conversation\n",
    "\n",
    "All previous queries were standalone: one question, one answer. Real usage looks different: a user asks a follow-up question that references the previous answer (\"What about its end-of-life?\", \"Is that figure verified?\").\n",
    "\n",
    "The ask() function accepts a history argument, a list of prior LLMMessage objects, to support this. There is one subtlety: the retriever only sees the current query, not the conversation history. A follow-up like \"What about its recycled content?\" would embed the pronoun \"its\", which matches nothing in the corpus.\n",
    "\n",
    "To prevent this, when history is provided the agent first rewrites the query into a self-contained form before retrieval, for example \"What about its recycled content?\" becomes \"What is the recycled content of the Logypal 1 pallet?,\" and only then embeds and retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-multiturn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "history: list[LLMMessage] = []\n",
    "\n",
    "\n",
    "async def conversation_turn(query: str) -> str:\n",
    "    global history\n",
    "    answer = await agent.answer(QueryWithContext(query=query, history=history))\n",
    "    history.append(LLMMessage(role=Roles.USER, content=query))\n",
    "    history.append(LLMMessage(role=Roles.ASSISTANT, content=answer.content))\n",
    "    return answer.content\n",
    "\n",
    "\n",
    "QUERY1 = \"Which pallets in our portfolio have a third-party verified EPD?\"\n",
    "QUERY2 = \"What is the GWP figure reported in it for the Logypal 1?\"\n",
    "\n",
    "# Turn 1: ask about a specific product\n",
    "reply1 = await conversation_turn(QUERY1)\n",
    "print(\"---------------------------\")\n",
    "print(f\"User: {QUERY1}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Assistant: {reply1}\\n\")\n",
    "\n",
    "# Turn 2: follow-up using a pronoun â€” the agent should resolve \"it\" before retrieval\n",
    "reply2 = await conversation_turn(QUERY2)\n",
    "print(\"---------------------------\")\n",
    "print(f\"User: {QUERY2}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Assistant: {reply2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-arch-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Running the Full Pipeline in One Call\n",
    "\n",
    "We have now gone through the pipeline step by step. For convenience, the `run_pipeline()` function executes all five steps end-to-end. It is also what the `__main__` entry point calls.\n",
    "\n",
    "Use it for quick one-shot queries. Use the individual step functions above when you need\n",
    "to inspect intermediate results or iterate on a specific stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-arch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import run_pipeline\n",
    "\n",
    "answer = await run_pipeline(\n",
    "    backend=BACKEND,\n",
    "    query=\"What sustainability certifications do the pallets in the portfolio have?\",\n",
    "    reset_vs=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-exercises-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tasks & Discussion\n",
    "\n",
    "Work through these in small groups. You don't need to do them all, pick what interests you or what matches your background.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¬ Explore & Discuss\n",
    "\n",
    "**A. Build a test set together**\n",
    "Go through the data files (run the scratch cell below to see what's there) and write down questions where the answer *is* in the documents, but also questions about products that don't exist or data that isn't there, and questions where you suspect conflicting information. Example questions can be found in the EVALUATION_qa_ground_truth.md file. You can also create new files if you think this will help to test the RAG. You can then also run the test questions through the system to evaluate the current RAG. \n",
    "\n",
    "**B. Evaluate the outputs: would you trust them?**\n",
    "Look back at the failure mode answers in Section 4. For each one: would you have trusted this answer without knowing it might be wrong? What would a *good* response look like? What would need to change in the system to prevent this failure?\n",
    "\n",
    "**C. Think about your own context**\n",
    "If you were to deploy this in your organisation, what documents would go into the knowledge base? What questions should it answer well? Who would use it, and what are the consequences of a wrong answer?\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Code Experiments\n",
    "\n",
    "**1. Layout quality audit on a different PDF**\n",
    "Re-run the raw Markdown extraction and heading inspection from the ingestion section on a different PDF in the corpus. Are heading detection failures consistent across documents, or document-specific? How much content is dropped before the first heading?\n",
    "\n",
    "**2. Try a different PDF parser**\n",
    "Switch from the default `pymupdf4llm` engine to `markitdown` and compare the output for the same PDF:\n",
    "```python\n",
    "from conversational_toolkit.chunking import PDFChunker, MarkdownConverterEngine\n",
    "chunks_md = PDFChunker().make_chunks(sample_pdf, engine=MarkdownConverterEngine.MARKITDOWN)\n",
    "```\n",
    "Are the headings and table structures preserved differently? Which parser gives cleaner chunks for this document? You can also implement a new PDF parser.\n",
    "\n",
    "**3. Chunking strategy comparison**\n",
    "Throughout this notebook we used `header_based_chunks`, even though we saw that many chunks exceed the 256-token limit of `all-MiniLM-L6-v2`. Try one of these fixes: switch to `paragraph_aware_chunks(target_chars=600)` to keep chunks within the limit, or keep `header_based_chunks` but switch to the OpenAI embedding model (`text-embedding-3-small`). Rebuild the vector store and re-run the failure mode queries. Do the retrieved chunks change? Do the answers improve?\n",
    "\n",
    "**4. Images: what are we losing?**\n",
    "The ingestion section showed that image references are zero by default. Enable extraction and inspect the results:\n",
    "```python\n",
    "img_chunks = PDFChunker().make_chunks(sample_pdf, write_images=True, image_path=\"tmp_images/\")\n",
    "```\n",
    "Open the extracted files. Do they contain diagrams or figures a user might query? What would it take to make them searchable?\n",
    "\n",
    "**5. Retrieval inspection**\n",
    "Use `quick_retrieve()` in the scratch cell below with several different queries, both in-corpus and out-of-corpus. At roughly what L2 score do retrieved chunks stop being relevant? Could this threshold serve as an automatic â€œno relevant dataâ€ flag?\n",
    "\n",
    "**7. Top-k sensitivity**\n",
    "Change `top_k` from 5 to 1 and re-run one of the failure mode queries from Section 4. Does the answer change? Try 10. Does more context improve the answer, or does irrelevant noise creep in?\n",
    "\n",
    "**8. System prompt ablation**\n",
    "Edit `SYSTEM_PROMPT` in the agent setup cell to address one of the failure modes: for example, add a rule about flagging conflicting figures or refusing to answer for out-of-portfolio products. Rebuild the agent and re-run the relevant query. Does the behaviour change?\n",
    "\n",
    "**9. Query phrasing**\n",
    "Run the same underlying question in different phrasings through `quick_retrieve()`: for example `\"COâ‚‚ footprint Logypal 1\"`, `\"carbon emissions recycled pallet\"`, and `\"GWP A1-A3 EPD pallet\"`. Do the retrieved chunks differ? Which phrasing scores highest? Try a query in German or French.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-exercises-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scratch cell, run your experiments here\n",
    "async def quick_retrieve(query: str, top_k: int = 5):\n",
    "    retriever = VectorStoreRetriever(embedding_model, vector_store, top_k=top_k)\n",
    "    results = await retriever.retrieve(query)\n",
    "    print(f\"Query: {query!r}  (top_k={top_k})\")\n",
    "    for r in results:\n",
    "        src = r.metadata.get(\"source_file\", \"?\")\n",
    "        print(f\"  score={r.score:.4f}  {src}  {r.title!r}\")\n",
    "\n",
    "\n",
    "await quick_retrieve(\"PFAS-free tape declaration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Function |\n",
    "|---|---|\n",
    "| 1. Load & chunk | `load_chunks(max_files)` |\n",
    "| 2. Embed & index | `build_vector_store(chunks, emb, reset)` |\n",
    "| 3. Inspect retrieval | `inspect_retrieval(query, vs, emb, top_k)` |\n",
    "| 4. Build agent | `build_agent(vs, emb, llm, top_k, system_prompt, number_query_expansion)` |\n",
    "| 5. Generate answer | `ask(agent, query, history)` |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
