{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p0-title-md",
   "metadata": {},
   "source": [
    "# The Baseline RAG Pipeline\n",
    "\n",
    "**RAG Prototyping Workshop**\n",
    "\n",
    "---\n",
    "\n",
    "## What You Will Learn\n",
    "\n",
    "This notebook is the starting point for the workshop. It introduces the **key concepts** behind Retrieval-Augmented Generation (RAG) and walks through every step of the **baseline pipeline** that the later phases build upon.\n",
    "\n",
    "After working through this notebook you will be able to:\n",
    "- Explain why a standalone LLM is insufficient for grounded enterprise Q&A\n",
    "- Describe the five stages of a RAG pipeline (chunk -> embed -> store -> retrieve -> generate)\n",
    "- Run the full baseline pipeline against the PrimePack AG corpus\n",
    "- Use the retrieval inspection step as the primary debugging tool\n",
    "- Identify the three main failure modes this workshop addresses\n",
    "\n",
    "**Workshop Phases at a Glance**\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| **Baseline (this notebook)** | Key concepts + end-to-end baseline |\n",
    "| Feature Track 1 | Chunking strategies & document ingestion |\n",
    "| Feature Track 2 | Evaluation metrics (retrieval + generation) |\n",
    "| Feature Track 3 | Reliable & structured outputs |\n",
    "| Feature Track 4 | Advanced retrieval |\n",
    "| Feature Track 5 | Multi-step agent workflows |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-why-rag-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Why RAG? The Problem with a Standalone LLM\n",
    "\n",
    "### The Scenario\n",
    "**PrimePack AG** buys packaging materials (pallets, cardboard boxes, tape) from multiple suppliers. Sustainability claims are increasingly scrutinised by customers and regulators. Employees need to answer questions like:\n",
    "> *\"What is the GWP of the Logypal 1 pallet, and is the figure verified?\"*  \n",
    "> *\"Can we tell a customer that the tesa tape is PFAS-free?\"*  \n",
    "> *\"Which of our suppliers have a certified EPD?\"*\n",
    "\n",
    "### Why Not Just Ask ChatGPT?\n",
    "A general-purpose LLM has three fundamental problems for this task:\n",
    "\n",
    "| Problem | Why It Matters |\n",
    "|---|---|\n",
    "| **No product knowledge** | LLMs know nothing about Logypal 1, PrimePack's specific portfolio, or the individual supplier documents. |\n",
    "| **Hallucination** | When asked about unknown products the LLM invents plausible-sounding but false figures. |\n",
    "| **No evidence trail** | Even when correct, a raw LLM answer cannot be traced back to a source document. |\n",
    "\n",
    "### The RAG Solution\n",
    "RAG adds a **retrieval step** between the user's question and the LLM:\n",
    "\n",
    "```\n",
    " Documents ──► Chunker ──► Embedder ──► Vector DB\n",
    "                                              │\n",
    " User query ─────────────────► Embedder ─────►  Retriever ──► Top-k Chunks\n",
    "                                                                      │\n",
    "                                                               LLM + Prompt\n",
    "                                                                      │\n",
    "                                                               Answer + Sources\n",
    "```\n",
    "\n",
    "The LLM only sees documents that are **actually in the corpus**. The answer can be traced to specific source chunks. If the corpus does not contain the answer, the LLM is instructed to say so.\n",
    "\n",
    "### What RAG Does *Not* Fix\n",
    "RAG shifts the problem from hallucination to **retrieval quality**. If the right chunk is not retrieved, the answer will still be wrong (or absent). The later phases of this workshop address exactly this: better chunking, better retrieval, and better output structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-concepts-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Core Concepts\n",
    "\n",
    "### Chunks\n",
    "\n",
    "A **chunk** is a short excerpt from a source document, a section of a PDF, one sheet of a spreadsheet, or one heading-delimited paragraph of a Markdown file. Chunks are the unit of indexing and retrieval.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: str           # unique identifier\n",
    "    title: str        # e.g. section heading\n",
    "    content: str      # the text that gets embedded\n",
    "    metadata: dict    # source_file, page, ...\n",
    "```\n",
    "\n",
    "### Embeddings\n",
    "An **embedding** converts text to a dense numeric vector (e.g. 384 dimensions). Semantically similar texts produce similar vectors. Here we use `all-MiniLM-L6-v2`, a compact local model that runs without an API key.\n",
    "\n",
    "### Vector Store (ChromaDB)\n",
    "A **vector store** persists chunk embeddings on disk and supports approximate nearest-neighbour search. Given a query embedding, it returns the `top_k` most similar chunks in milliseconds.\n",
    "\n",
    "### Retriever\n",
    "A **retriever** wraps a vector store and exposes a single `retrieve(query)` method. The baseline uses a `VectorStoreRetriever` with `top_k=5`.\n",
    "\n",
    "### RAG Agent\n",
    "The **RAG agent** combines a retriever and an LLM. Its `answer()` method:\n",
    "1. Embeds the query\n",
    "2. Retrieves the top-k chunks\n",
    "3. Formats chunks as XML `<source>` tags in the prompt\n",
    "4. Calls the LLM and returns the answer + cited sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "- `conversational-toolkit` installed in editable mode (`pip install -e conversational-toolkit/`) (already done on Renku)\n",
    "- `backend` installed in editable mode (`pip install -e backend/`) (already done on Renku)\n",
    "- For the **Ollama** backend (default): `ollama serve` running + `ollama pull mistral-nemo:12b`\n",
    "- For the **OpenAI** and \"QWEN\" backend you need to set the API Keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n",
      "Project root : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag\n",
      "Data dir     : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/data\n",
      "Vector store : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/backend/data_vs.db\n",
      "LLM backend  : qwen\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from conversational_toolkit.agents.base import QueryWithContext\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.retriever.vectorstore_retriever import VectorStoreRetriever\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    load_chunks,\n",
    "    inspect_chunks,\n",
    "    build_vector_store,\n",
    "    inspect_retrieval,\n",
    "    build_agent,\n",
    "    build_llm,\n",
    "    ask,\n",
    "    DATA_DIR,\n",
    "    VS_PATH,\n",
    "    EMBEDDING_MODEL,\n",
    "    RETRIEVER_TOP_K,\n",
    ")\n",
    "\n",
    "# ── Choose your LLM backend ─────────────────────────────────────────────────\n",
    "# Set BACKEND to one of:\n",
    "#   \"ollama\"  — local model, requires running `ollama serve` (see Renku_README.md)\n",
    "#   \"openai\"  — cloud model, requires an OpenAI API key   (see Renku_README.md)\n",
    "#   \"qwen\"    — SDSC cloud model, requires an SDSC token  (see Renku_README.md)\n",
    "BACKEND = \"qwen\"  # set this before running\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError(\n",
    "        'BACKEND is not set. Edit the line above and set it to \"ollama\", \"openai\", or \"qwen\".\\n'\n",
    "        \"See Renku_README.md for setup instructions.\"\n",
    "    )\n",
    "\n",
    "ROOT = Path().resolve().parents[1]  # backend/notebooks/ → project root\n",
    "print(f\"Project root : {ROOT}\")\n",
    "print(f\"Data dir     : {DATA_DIR}\")\n",
    "print(f\"Vector store : {VS_PATH}\")\n",
    "print(f\"LLM backend  : {BACKEND}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Load and Chunk Documents\n",
    "\n",
    "The `load_chunks()` function walks `data/` and dispatches each file to the right chunker:\n",
    "\n",
    "| Extension | Chunker | Strategy |\n",
    "|---|---|---|\n",
    "| `.pdf` | `PDFChunker` | Convert to Markdown via `pymupdf4llm`, split on `#` headings |\n",
    "| `.xlsx`, `.xls` | `ExcelChunker` | One chunk per sheet, serialised as a Markdown table |\n",
    "| `.md`, `.txt` | `MarkdownChunker` | Split on `#` headings |\n",
    "\n",
    "The result is a flat `list[Chunk]`, the same structure regardless of the original format.\n",
    "\n",
    "You can use `max_files=5` here for speed. Remove the limit (or set `None`) to load the full corpus.\n",
    "\n",
    "> **Feature Track 1** explores alternative chunking strategies in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 15:30:13.369 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:196 - Chunking 35 files from /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/data\n",
      "2026-02-21 15:30:13.371 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   ART_internal_procurement_policy.md: 12 chunks\n",
      "2026-02-21 15:30:13.372 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   ART_logylight_incomplete_datasheet.md: 7 chunks\n",
      "2026-02-21 15:30:13.373 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   ART_product_catalog.md: 7 chunks\n",
      "2026-02-21 15:30:13.373 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   ART_relicyc_logypal1_old_datasheet_2021.md: 7 chunks\n",
      "2026-02-21 15:30:13.374 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   ART_response_inquiry_frische_felder.md: 6 chunks\n",
      "2026-02-21 15:30:13.374 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   ART_supplier_brochure_CPR_wood_pallet.md: 6 chunks\n",
      "2026-02-21 15:30:13.375 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   ART_supplier_brochure_tesa_ECO.md: 6 chunks\n",
      "2026-02-21 15:30:14.893 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   EPD_cardboard_grupak_corrugated.pdf: 35 chunks\n",
      "2026-02-21 15:30:16.031 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   EPD_cardboard_redbox_cartonpallet.pdf: 11 chunks\n",
      "2026-02-21 15:30:16.856 | DEBUG    | sme_kt_zh_collaboration_rag.feature0_baseline_rag:load_chunks:205 -   EPD_pallet_CPR_noe.pdf: 11 chunks\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chunks = load_chunks(max_files=None)\n",
    "inspect_chunks(chunks)\n",
    "\n",
    "# Quick size distribution\n",
    "char_lengths = [len(c.content) for c in chunks]\n",
    "over_limit = sum(1 for n in char_lengths if n > 1024)\n",
    "print(f\"\\nChunks total       : {len(chunks)}\")\n",
    "print(f\"Mean length (chars): {sum(char_lengths) // len(char_lengths)}\")\n",
    "print(f\"Over 1024-char limit (≈256 tok embedding limit): {over_limit} / {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step1-inspect-md",
   "metadata": {},
   "source": [
    "### What a Chunk Looks Like\n",
    "\n",
    "Each chunk carries a `title` (the heading), the raw text `content`, and a `metadata` dict\n",
    "with the source file name. This metadata is returned alongside the answer so the user can\n",
    "trace every claim back to its origin document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step1-inspect-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [ART_customer_inquiry_frische_felder.md] ---\n",
      "Title  : '# Customer Inquiry — Sustainability Information Request'\n",
      "Length : 320 chars\n",
      "Preview: '# Customer Inquiry — Sustainability Information Request\\n\\n*Document type: Customer communication (incoming + internal draft response)* *Date received: 14 January 2025* *Customer: Frische Felder AG, Procurement Department* *Contact: Ms. Sabine Keller —'\n",
      "\n",
      "--- [ART_customer_inquiry_frische_felder.md] ---\n",
      "Title  : '## Incoming Customer Email'\n",
      "Length : 1662 chars\n",
      "Preview: '## Incoming Customer Email\\n\\n**Subject:** Request for Sustainability Documentation — Tape and Cardboard Products\\n\\nDear PrimePack AG Team,\\n\\nThank you for our ongoing partnership. As part of our updated supplier due diligence process in line with the EU'\n",
      "\n",
      "--- [ART_customer_inquiry_frische_felder.md] ---\n",
      "Title  : '## Internal Draft Response'\n",
      "Length : 374 chars\n",
      "Preview: '## Internal Draft Response\\n\\n*Status: DRAFT — not yet sent. Requires review and completion before sending.* *Prepared by: Sales Team, 20 January 2025* *Review required from: Procurement (PFAS status), Marketing (policy document)*\\n\\n---\\n\\nDear Ms. Keller'\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print 3 representative chunks\n",
    "for c in chunks[:3]:\n",
    "    print(f\"--- [{c.metadata.get('source_file', '?')}] ---\")\n",
    "    print(f\"Title  : {c.title!r}\")\n",
    "    print(f\"Length : {len(c.content)} chars\")\n",
    "    print(f\"Preview: {c.content[:250].strip()!r}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Embed Chunks and Build the Vector Store\n",
    "\n",
    "`SentenceTransformerEmbeddings` converts every chunk's `content` to a 384-dimensional vector using `all-MiniLM-L6-v2`. The resulting matrix (shape `[n_chunks, 384]`) is inserted into a persistent `ChromaDBVectorStore`.\n",
    "\n",
    "**On subsequent runs**, leave `reset=False` (the default) to skip re-embedding, it takes time and the store on disk is already correct. Pass `reset=True` only when the corpus or chunking strategy changes.\n",
    "\n",
    "> **Why 384 dimensions?** `all-MiniLM-L6-v2` is a distilled model: small enough to run on CPU in seconds but good enough for retrieval on short technical texts. OpenAI's `text-embedding-3-small` produces 1536-dimensional vectors with higher quality at the cost of an API call per chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-21 15:28:49.115 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-21 15:28:49.121 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_vector_store:253 - Vector store already contains 78 chunks — skipping embedding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector store ready.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "\n",
    "# Set reset=True to rebuild the store from scratch\n",
    "vector_store = await build_vector_store(\n",
    "    chunks, embedding_model, db_path=VS_PATH, reset=False\n",
    ")\n",
    "print(\"Vector store ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step2-embed-md",
   "metadata": {},
   "source": [
    "### Similarity in Embedding Space\n",
    "\n",
    "Embeddings that are close in vector space share semantic meaning. The cell below embeds several sentences and measures their cosine similarity: a value between -1 (opposite) and 1 (identical). You can change the sentences to see the impact on cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step2-embed-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = \"carbon footprint of a pallet\"\n",
    "sentence2 = \"GWP value for the Logypal 1\"\n",
    "sentence3 = \"PFAS-free tape declaration\"\n",
    "sentence4 = \"the annual report of a software firm\"\n",
    "\n",
    "\n",
    "async def cosine_similarity(a: str, b: str) -> float:\n",
    "    vecs = await embedding_model.get_embeddings([a, b])\n",
    "    return float(\n",
    "        np.dot(vecs[0], vecs[1]) / (np.linalg.norm(vecs[0]) * np.linalg.norm(vecs[1]))\n",
    "    )\n",
    "\n",
    "\n",
    "pairs = [\n",
    "    (sentence1, sentence2),\n",
    "    (sentence1, sentence3),\n",
    "    (sentence1, sentence4),\n",
    "]\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for a, b in pairs:\n",
    "    sim = await cosine_similarity(a, b)\n",
    "    print(f\"{sim:.3f}  -->  {a!r}  vs  {b!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Inspect Retrieval (Before the LLM Sees Anything)\n",
    "\n",
    "This is the **most important diagnostic step** in the whole pipeline:\n",
    "\n",
    "> If the retrieved chunks are wrong, the final answer will be wrong regardless of how good the LLM is.\n",
    "\n",
    "`inspect_retrieval()` runs the query through the embedding model, fetches the top-k most similar chunks from ChromaDB, and prints them with scores. Use this to:\n",
    "- Verify that relevant documents are in the index\n",
    "- Tune `top_k`\n",
    "- Compare different query phrasings\n",
    "- Identify retrieval gaps before blaming the LLM\n",
    "\n",
    "The **similarity score** is the L2 distance, range [0,4], lower = more similar. L2 distance is used becuase it works for any vectors, normalised or not. Cosine similarity only makes sense for direction (magnitude doesn't matter), so it requires that vectors be unit-length to be meaningful. L2 makes no such assumption, making it the safer general default. ChromaDB defaults to L2 because it's simpler to compute and works even if vector magnitudes vary. Since our embedding model always produces equal-length vectors, we get cosine-equivalent ranking. The score numbers look different, but the top-5 results would be identical either way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "results = await inspect_retrieval(QUERY, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-md",
   "metadata": {},
   "source": [
    "### Retrieval for a Product Outside the Portfolio\n",
    "\n",
    "The PrimePack AG product catalog defines the portfolio boundary. The **Lara Pallet** is not in the catalog, it does not exist. Watch which chunks are returned and what scores they have. A **higher** minimum score (large L2 distance) signals *weaker semantic match*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step3-fail-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "QUERY_OOK = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "results_ook = await inspect_retrieval(QUERY_OOK, vector_store, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step3-fail-obs-md",
   "metadata": {},
   "source": [
    "> **Observation:** The retriever always returns the *closest* chunks it can find, it has no concept of \"no match\". For an unknown product the L2 distances are **higher** (the closest chunks are still about other pallets), but without a score-threshold guard the LLM receives those chunks anyway and may silently answer about the wrong product.\n",
    "> **Phase 3** shows how to combat this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step4-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Build the RAG Agent\n",
    "\n",
    "`build_agent()` assembles the three components:\n",
    "\n",
    "```\n",
    "VectorStoreRetriever\n",
    "    └─ ChromaDBVectorStore (on disk, persists across runs)\n",
    "    └─ SentenceTransformerEmbeddings\n",
    "\n",
    "RAG Agent\n",
    "    ├─ LLM (Ollama / OpenAI / SDSC Qwen)\n",
    "    ├─ Retriever\n",
    "    └─ System prompt\n",
    "```\n",
    "\n",
    "### The System Prompt\n",
    "\n",
    "The system prompt is a very powerful lever for controlling LLM behaviour:\n",
    "\n",
    "```\n",
    "You are a helpful AI assistant specialised in sustainability and product compliance. Answer questions using the provided sources. If the information is not in the sources, say so clearly.\n",
    "```\n",
    "\n",
    "The instruction *\"If the information is not in the sources, say so clearly\"* should prevents hallucination about missing products and unverified claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step4-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vector_store,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    number_query_expansion=0,  # 0 = no expansion; will look at this more in feautre track 4\n",
    ")\n",
    "print(\"RAG agent assembled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-step5-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Ask a Question\n",
    "\n",
    "`ask()` sends the query to the agent and returns the answer string. The internal flow is:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks\n",
    "3. Build the prompt: `<system>` + `<sources>` XML block + user question\n",
    "4. Generate the answer with the LLM\n",
    "5. Return the answer and a list of cited source chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-step5-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "QUERY = \"What materials is the Logypal 1 pallet made from?\"\n",
    "\n",
    "answer = await ask(agent, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-queries-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Probing Failure Modes\n",
    "\n",
    "The corpus was designed with three deliberate challenges. Run the queries below and observe the answers.\n",
    "\n",
    "### 4a: Out-of-Portfolio Query\n",
    "\n",
    "The **Lara Pallet** does not exist. A good RAG must say so instead of describing a different pallet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-ook-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answer_ook = await ask(agent, \"What materials is the Lara pallet made from?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-gap-md",
   "metadata": {},
   "source": [
    "### 4b: Missing Data (LogyLight Pallet)\n",
    "\n",
    "The LogyLight datasheet marks all LCA fields as *\"not yet available\"*. The correct answer is that we don't have the data, not a fabricated figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-gap-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answer_gap = await ask(agent, \"What is the GWP of the LogyLight pallet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-conflict-md",
   "metadata": {},
   "source": [
    "### 4c: Conflicting Evidence (Relicyc GWP Figures)\n",
    "\n",
    "The 2021 Relicyc datasheet reports **4.1 kg CO₂e** per pallet. The 2023 EPD (third-party verified) reports a different, more recent figure. The RAG should flag the conflict and prefer the verified, more recent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-conflict-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answer_conflict = await ask(\n",
    "    agent, \"What is the GWP of the Logypal 1 pallet, and how reliable is the figure?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-query-unverified-md",
   "metadata": {},
   "source": [
    "### 4d: Unverified Supplier Claim (Tesa ECO Tape)\n",
    "\n",
    "The tesa supplier brochure claims **68% CO₂ reduction** compared to conventional tape. This is a self-declared marketing claim, there is no independent EPD. The RAG should report the claim but flag that it is unverified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-query-unverified-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "answer_claim = await ask(\n",
    "    agent,\n",
    "    \"How much lower is the carbon footprint of tesa ECO tape compared to standard tape?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5948f40",
   "metadata": {},
   "source": [
    "> Can you think of and find **other failure modes**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-multiturn-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multi-Turn Conversation\n",
    "\n",
    "The `ask()` function accepts a `history` argument, a list of prior `LLMMessage` objects. When history is provided the agent first **rewrites the query** to be self-contained (*\"it\"* becomes the actual product name) before retrieval.\n",
    "\n",
    "This prevents the retriever from embedding vague pronouns that match nothing in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-multiturn-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from conversational_toolkit.llms.base import LLMMessage, Roles\n",
    "\n",
    "history: list[LLMMessage] = []\n",
    "\n",
    "\n",
    "async def conversation_turn(query: str) -> str:\n",
    "    global history\n",
    "    answer = await agent.answer(QueryWithContext(query=query, history=history))\n",
    "    history.append(LLMMessage(role=Roles.USER, content=query))\n",
    "    history.append(LLMMessage(role=Roles.ASSISTANT, content=answer.content))\n",
    "    return answer.content\n",
    "\n",
    "\n",
    "# Turn 1: ask about a specific product\n",
    "reply1 = await conversation_turn(\n",
    "    \"Which pallets in our portfolio have a third-party verified EPD?\"\n",
    ")\n",
    "print(\"User: Which pallets in our portfolio have a third-party verified EPD?\")\n",
    "print(f\"Assistant: {reply1}\\n\")\n",
    "\n",
    "# Turn 2: follow-up using a pronoun — the agent should resolve \"it\" before retrieval\n",
    "reply2 = await conversation_turn(\n",
    "    \"What is the GWP figure reported in it for the Logypal 1?\"\n",
    ")\n",
    "print(\"User: What is the GWP figure reported in it for the Logypal 1?\")\n",
    "print(f\"Assistant: {reply2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-arch-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Running the Full Pipeline in One Call\n",
    "\n",
    "The `run_pipeline()` convenience function executes all five steps end-to-end. It is also\n",
    "what the `__main__` entry point calls.\n",
    "\n",
    "Use it for quick one-shot queries. Use the individual step functions above when you need\n",
    "to inspect intermediate results or iterate on a specific stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-arch-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import run_pipeline\n",
    "\n",
    "answer = await run_pipeline(\n",
    "    backend=BACKEND,\n",
    "    query=\"What sustainability certifications do the pallets in the portfolio have?\",\n",
    "    reset_vs=False,\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-backends-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Switching LLM Backends\n",
    "\n",
    "The pipeline abstracts the LLM behind a common interface. Only `build_llm()` needs to change.\n",
    "\n",
    "| Backend | `BACKEND=` | Prerequisite |\n",
    "|---|---|---|\n",
    "| Ollama (local) | `\"ollama\"` | `ollama serve` + `ollama pull mistral-nemo:12b` |\n",
    "| OpenAI | `\"openai\"` | `OPENAI_API_KEY` env variable |\n",
    "| SDSC Qwen | `\"qwen\"` | `SDSC_QWEN3_32B_AWQ` env variable |\n",
    "\n",
    "You can also override the model within a backend:\n",
    "\n",
    "```python\n",
    "llm = build_llm(backend=\"openai\", model_name=\"gpt-4o\") # stronger model\n",
    "llm = build_llm(backend=\"ollama\", model_name=\"llama3.2\") # smaller local model\n",
    "```\n",
    "\n",
    "The RAG pipeline is **backend-agnostic**, the retrieval step is identical regardless of which LLM is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-backends-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test openai\n",
    "\n",
    "QUERY = \"What materials is the Lara pallet made from?\"\n",
    "\n",
    "llm_openai = build_llm(backend=\"openai\", model_name=\"gpt-4o-mini\")\n",
    "agent_openai = build_agent(vector_store, embedding_model, llm_openai)\n",
    "answer_openai = await ask(agent_openai, QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-exercises-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Tasks\n",
    "\n",
    "1. **test set creation**: Go through the dataset and come up with questions and the corresponding correct answers that the RAG should give based on the query. ALso include trick questions, such as asking for information that is not in the data or for qhich contradicting infoormation exists. \n",
    "\n",
    "2. **Retrieval inspection** — Call `inspect_retrieval()` with different queries and inspect which files are returned? What do the scores tell you about how well the corpus covers this topic?\n",
    "\n",
    "3. **Top-k sensitivity**: Change `top_k` from 5 to 1. Does the answer to the the questions change? What about to 10? Is more always better?\n",
    "\n",
    "4. **System prompt ablation**: In `baseline_rag.py`, locate `SYSTEM_PROMPT`. Try changing it and then rebuild the agent and re-run the Lara Pallet query. Does the answer change?\n",
    "\n",
    "5. **Query phrasing**: The embedding model is sensitive to wording. Try `\"CO₂ footprint Logypal 1\"`, `\"carbon emissions recycled pallet\"`, and `\"GWP A1-A3 EPD pallet\"`. Do the top-1 chunk and score differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p0-exercises-code",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Scratch cell — run your experiments here\n",
    "async def quick_retrieve(query: str, top_k: int = 5):\n",
    "    retriever = VectorStoreRetriever(embedding_model, vector_store, top_k=top_k)\n",
    "    results = await retriever.retrieve(query)\n",
    "    print(f\"Query: {query!r}  (top_k={top_k})\")\n",
    "    for r in results:\n",
    "        src = r.metadata.get(\"source_file\", \"?\")\n",
    "        print(f\"  score={r.score:.4f}  {src}  {r.title!r}\")\n",
    "\n",
    "\n",
    "await quick_retrieve(\"PFAS-free tape declaration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p0-summary-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Step | Function | \n",
    "|---|---|\n",
    "| 1. Load & chunk | `load_chunks(max_files)` |\n",
    "| 2. Embed & index | `build_vector_store(chunks, emb, reset)` |\n",
    "| 3. Inspect retrieval | `inspect_retrieval(query, vs, emb)` | \n",
    "| 4. Build agent | `build_agent(vs, emb, llm, top_k)` |\n",
    "| 5. Generate answer | `ask(agent, query, history)` |\n",
    "\n",
    "### Three Core Failure Modes (Addressed in Later Feature Tracks)\n",
    "- Wrong entity\n",
    "- Missing data presented as fact\n",
    "- Low recall\n",
    "\n",
    "**Next — Feature 1:** Explore chunking strategies and understand how chunk size affects retrieval quality.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
