{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p1-title-md",
   "metadata": {},
   "source": [
    "# Feature Track 1: Evaluation & Validation\n",
    "\n",
    "---\n",
    "\n",
    "Shipping a RAG system without systematic evaluation is like navigating without instruments. The pipeline may *seem* to work on the queries you tested by hand, but you have no way to know where it breaks, how often, or whether a change you made helped or hurt.\n",
    "\n",
    "**Evaluation closes the feedback loop:**\n",
    "\n",
    "```\n",
    "Change a parameter  ──►  Measure quantitatively  ──►  Decide based on data\n",
    "```\n",
    "\n",
    "**Prerequisite:** Run `feature0_baseline_rag.ipynb` Steps 1–2 first to build the vector store.\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| Feature 0 | Working baseline prototype |\n",
    "| **Feature Track 1 (this notebook)** | Quantitative evaluation |\n",
    "| Feature Track 2 | Reliable, structured outputs |\n",
    "| Feature Track 3 | Better retrieval strategies |\n",
    "| Feature Track 4 | Multi-step agent workflows |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-why-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Foundation\n",
    "\n",
    "### Why Systematic Evaluation?\n",
    "Suppose you change the chunk size from 800 to 400 characters. Did that help? How would you know?\n",
    "\n",
    "Without metrics you are forced to manually re-read answers for a handful of test queries and guess. With metrics you run the evaluation suite and get a number -> a number you can track across changes and use to justify decisions.\n",
    "\n",
    "### A Concrete Example from Feature 0\n",
    "In Feature 0 we saw that the baseline RAG sometimes:\n",
    "- Described the **Lara Pallet** as if it exists (it doesn't)\n",
    "- Cited the **outdated 2021 GWP figure** even though a newer, verified EPD supersedes it\n",
    "- Reported the tesa **68% CO₂ reduction** without flagging it as unverified\n",
    "\n",
    "These are not edge cases -> they are exactly the queries that matter for compliance.\n",
    "How often does this happen? After every change to the pipeline, you need an answer.\n",
    "\n",
    "### The Four Questions Evaluation Answers\n",
    "| Question | Why it matters |\n",
    "|---|---|\n",
    "| **Is the retriever finding the right chunks?** | A perfect LLM cannot fix wrong retrieval |\n",
    "| **Is the LLM hallucinating?** | A fabricated GWP figure can be shared with clients |\n",
    "| **Is the answer complete?** | Missing \"this is unverified\" can mislead a user |\n",
    "| **Did my change help?** | Without a baseline metric you cannot tell improvement from regression |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-pipeline-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "Each arrow is a potential failure point. Evaluation targets a specific stage so you can isolate *where* the problem is.\n",
    "\n",
    "```\n",
    "**Ingestion** (run once)\n",
    "\n",
    "  Documents  ──►  [1] Chunker  ──►  [2] Embedder  ──►  [3] Vector DB\n",
    "\n",
    "\n",
    "**Querying** (every user question)\n",
    "\n",
    "  User query  ──►  [2] Embedder  ──►  [3] Retriever  ──►  Top-k Chunks\n",
    "                                                                 │\n",
    "                                                          [4] LLM + Prompt\n",
    "                                                                 │\n",
    "                                                          Answer + Sources\n",
    "```\n",
    "\n",
    "| Step | What it does | If it fails |\n",
    "|---|---|---|\n",
    "| [1] Chunking | Split documents into searchable units | Context split mid-fact; tables broken; information lost |\n",
    "| [2] Embedding | Convert text to vectors | Wrong chunks returned despite a matching query |\n",
    "| [3] Vector search | Find most similar chunks to query | Relevant chunks not returned |\n",
    "| [4] Generation | LLM answers using retrieved context | Hallucination; ignores context; incomplete answer |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-map-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stage-by-Stage Evaluation Map\n",
    "\n",
    "| Stage | What to measure |\n",
    "|---|---|\n",
    "| **Ingestion / Parsing** | Text completeness; table structure preserved; reading order correct in multi-column layouts |\n",
    "| **Chunking** | Chunk size distribution; percentage of chunks exceeding the embedding model's token limit |\n",
    "| **Embedding** | Similarity gap between a relevant and an irrelevant chunk for the same query |\n",
    "| **Vector search** | Fraction of queries where the correct chunk appears in the top-k results |\n",
    "| **Retrieved context** | Relevance of the retrieved chunks to the query |\n",
    "| **Faithfulness** | Fraction of answer claims that are directly supported by the retrieved context |\n",
    "| **Answer relevance** | Whether the answer addresses the actual question, not a related but different one |\n",
    "| **Answer correctness** | Factual accuracy of the answer compared to the known ground truth |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-ragas-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAGAS\n",
    "\n",
    "[RAGAS](https://docs.ragas.io) (*Retrieval Augmented Generation Assessment*) is an\n",
    "open-source Python library for evaluating RAG pipelines. It is a widely adopted evaluation framework in the LLM/RAG ecosystem.\n",
    "\n",
    "#### How it works internally\n",
    "Rather than asking a judge LLM \"rate this answer 0–5\", RAGAS decomposes the answer into individual atomic claims:\n",
    "\n",
    "```\n",
    "Answer: \"The Logypal 1 GWP is 3.2 kg CO₂e, verified by Bureau Veritas.\"\n",
    "\n",
    "  Claim 1: \"GWP is 3.2 kg CO₂e\"          → supported by context?  ✓\n",
    "  Claim 2: \"verified by Bureau Veritas\"  → supported by context?  ✓\n",
    "\n",
    "  Faithfulness = 2 supported / 2 total = 1.0\n",
    "```\n",
    "\n",
    "This is more rigorous than a holistic score: it catches partial hallucination, e.g. a correct figure with a fabricated verifier name.\n",
    "\n",
    "#### Metrics at a glance\n",
    "| Metric | Ground truth? | What it catches |\n",
    "|---|---|---|\n",
    "| `Faithfulness` | No | Claims not supported by the retrieved context |\n",
    "| `AnswerRelevancy` | No | Off-topic or evasive answers |\n",
    "| `AnswerCorrectness` | Yes | Wrong or missing facts vs. the reference answer |\n",
    "| `ContextPrecision` | Yes | Irrelevant chunks ranked above relevant ones |\n",
    "\n",
    "#### Strengths\n",
    "- Standardised, reproducible metrics widely used in industry\n",
    "- `Faithfulness` and `AnswerRelevancy` require zero labelling effort\n",
    "- Claim-level decomposition is more rigorous than holistic scoring\n",
    "\n",
    "#### Weaknesses\n",
    "- Needs a capable judge LLM -> RAGAS defaults to OpenAI (requires `OPENAI_API_KEY`)\n",
    "- LLM judge has its own biases; may be lenient on confident-sounding hallucinations\n",
    "- Slow and costly at scale: ~3 LLM calls per sample per metric\n",
    "- Metrics are proxies, not ground truth: score 0.9 ≠ 90% of answers are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### First Look at RAGAS\n",
    "\n",
    "#### Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` installed in editable mode. Vector store must already exist -> run `feature0_baseline_rag.ipynb` Steps 1–2 first.\n",
    "\n",
    "RAGAS uses OpenAI as its judge LLM by default ->`OPENAI_API_KEY` must be set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-24 00:53:33.228 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-24 00:53:33.240 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_llm:135 - LLM backend: OpenAI (gpt-4o-mini)\n",
      "2026-02-24 00:53:33.263 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.3; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\n",
      "2026-02-24 00:53:33.263 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_agent:332 - RAG agent ready (top_k=5  query_expansion=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model : sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector store    : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/backend/data_vs.db\n",
      "RAG agent LLM   : openai\n",
      "RAGAS judge LLM : gpt-4o-mini (OpenAI)\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings as LangChainOpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper  # type: ignore[import-untyped]\n",
    "from ragas.metrics import (  # type: ignore[attr-defined]\n",
    "    Faithfulness as RagasFaithfulness,\n",
    "    AnswerRelevancy as RagasAnswerRelevancy,\n",
    ")\n",
    "\n",
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.evaluation import Evaluator\n",
    "from conversational_toolkit.evaluation.adapters import evaluate_with_ragas\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    EMBEDDING_MODEL,\n",
    "    VS_PATH,\n",
    "    SYSTEM_PROMPT,\n",
    "    build_llm,\n",
    "    build_agent,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "_secret_path = pathlib.Path(\"/secrets/OPENAI_API_KEY\")\n",
    "if \"OPENAI_API_KEY\" not in os.environ and _secret_path.exists():\n",
    "    os.environ[\"OPENAI_API_KEY\"] = _secret_path.read_text().strip()\n",
    "\n",
    "RETRIEVER_TOP_K = 5\n",
    "BACKEND = \"openai\"  # \"ollama\"  or  \"openai\"\n",
    "# Note: RAGAS uses OpenAI for its judge LLM regardless of BACKEND above.\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError('Set BACKEND to \"ollama\" or \"openai\" before running.')\n",
    "\n",
    "# RAG pipeline\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vs = ChromaDBVectorStore(db_path=str(VS_PATH))\n",
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vs,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,\n",
    ")\n",
    "\n",
    "# AnswerRelevancy internally calls embed_query() / embed_documents() to compare generated questions against the original query. langchain_openai.OpenAIEmbeddings implements this interface and is accepted directly by ragas.evaluate().\n",
    "ragas_embeddings = LangChainOpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# RAGAS defaults to max_tokens=3072 for its judge LLM. Long answers with many atomic claims overflow this limit mid-JSON, causing \"output is incomplete\" errors.\n",
    "# Wrap ChatOpenAI with a higher limit and pass it explicitly to evaluate_with_ragas().\n",
    "ragas_llm = LangchainLLMWrapper(\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", max_completion_tokens=8192)\n",
    ")\n",
    "\n",
    "print(f\"Embedding model : {EMBEDDING_MODEL}\")\n",
    "print(f\"Vector store    : {VS_PATH}\")\n",
    "print(f\"RAG agent LLM   : {BACKEND}\")\n",
    "print(\"RAGAS judge LLM : gpt-4o-mini (OpenAI)\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d8fdd",
   "metadata": {},
   "source": [
    "We start with the two metrics that need no ground truth:\n",
    "\n",
    "- **Faithfulness**: are all claims in the answer supported by the retrieved context?\n",
    "- **AnswerRelevancy**: does the answer directly address the question?\n",
    "\n",
    "The `evaluate_with_ragas()` adapter converts our `EvaluationSample` objects to RAGAS format, calls the judge LLM, and returns an `EvaluationReport`.\n",
    "\n",
    "*Takes ~2–3 minutes: RAGAS makes multiple judge LLM calls per sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e5961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 5 evaluation samples (runs the RAG agent once per query)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-23 23:55:28.748 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:55:29.488 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:55:32.852 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:55:36.431 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:55:39.523 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 5 samples built.\n",
      "\n",
      "Running RAGAS: Faithfulness + AnswerRelevancy (~2-3 min)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7185551cf644657b447fc33ddd58b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────\n",
      "Samples evaluated : 5\n",
      "────────────────────────────────────────\n",
      "faithfulness            0.982\n",
      "answer_relevancy        0.380\n",
      "────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Does PrimePack AG offer a product called the Lara Pallet?\",\n",
    "    \"Which products in the portfolio have a third-party verified EPD?\",\n",
    "    \"Can the 68% CO2 reduction claim for tesapack ECO (product 50-102) be included in a customer sustainability response?\",\n",
    "    \"Are any tape products confirmed to be PFAS-free?\",\n",
    "    \"Which suppliers are not yet compliant with the EPD requirement by end of 2025?\",\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Building {len(queries)} evaluation samples (runs the RAG agent once per query)...\"\n",
    ")\n",
    "samples = await Evaluator.build_samples_from_agent(agent=agent, queries=queries)\n",
    "print(f\"Done. {len(samples)} samples built.\\n\")\n",
    "\n",
    "\n",
    "print(\"Running RAGAS: Faithfulness + AnswerRelevancy (~2-3 min)\\n\")\n",
    "\n",
    "report_basic = evaluate_with_ragas(\n",
    "    samples=samples,\n",
    "    metrics=[\n",
    "        RagasFaithfulness(),  # type: ignore[call-arg]\n",
    "        RagasAnswerRelevancy(strictness=1),  # type: ignore[call-arg]\n",
    "    ],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    ")\n",
    "\n",
    "print(\"─\" * 40)\n",
    "print(f\"Samples evaluated : {report_basic.num_samples}\")\n",
    "print(\"─\" * 40)\n",
    "for metric_name, score in report_basic.summary().items():\n",
    "    print(f\"{metric_name:<22}  {score:.3f}\")\n",
    "print(\"─\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07af8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample scores  (F = Faithfulness,  A = AnswerRelevancy)\n",
      "\n",
      "#      F    A      query                                     response\n",
      "──────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "1    1.00  0.99    Does PrimePack AG offer a product called..  No, PrimePack AG does not offer a produc..\n",
      "2    1.00  0.00    Which products in the portfolio have a t..  Based on the information provided, the p..\n",
      "3    1.00  0.00    Can the 68% CO2 reduction claim for tesa..  Based on the information provided, the 6..\n",
      "4    1.00  0.00    Are any tape products confirmed to be PF..  As of now, there are no confirmed tape p..\n",
      "5    0.91  0.91    Which suppliers are not yet compliant wi..  As of January 2025, the following suppli..\n"
     ]
    }
   ],
   "source": [
    "# Per-sample breakdown: find which queries score best / worst so we know where to focus improvement efforts.\n",
    "\n",
    "import math\n",
    "\n",
    "f_result = next(\n",
    "    (r for r in report_basic.results if \"faithfulness\" in r.metric_name.lower()), None\n",
    ")\n",
    "a_result = next(\n",
    "    (\n",
    "        r\n",
    "        for r in report_basic.results\n",
    "        if \"relevancy\" in r.metric_name.lower() or \"relevance\" in r.metric_name.lower()\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "\n",
    "f_scores: list[float] = (f_result.per_sample_scores if f_result else None) or []\n",
    "a_scores: list[float] = (a_result.per_sample_scores if a_result else None) or []\n",
    "\n",
    "\n",
    "def fmt(v: float) -> str:\n",
    "    return \"  N/A\" if math.isnan(v) else f\"{v:>5.2f}\"\n",
    "\n",
    "\n",
    "print(\"Per-sample scores  (F = Faithfulness,  A = AnswerRelevancy)\\n\")\n",
    "print(f\"{'#':<3} {'F':>4} {'A':>4}      {'query':<40}  response\")\n",
    "print(\"─\" * 110)\n",
    "for i, (sample, f, a) in enumerate(zip(samples, f_scores, a_scores), 1):\n",
    "    flag = \" ◄\" if (not math.isnan(f) and f < 0.7) else \"  \"\n",
    "    q = sample.query[:40] + \"..\" if len(sample.query) > 40 else sample.query\n",
    "    r = (\n",
    "        (sample.answer[:40] or \"\") + \"..\"\n",
    "        if len(sample.answer or \"\") > 60\n",
    "        else (sample.answer or \"\")\n",
    "    )\n",
    "    print(f\"{i:<3} {fmt(f)} {fmt(a)}{flag}  {q:<40}  {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f3a3b",
   "metadata": {},
   "source": [
    "## Brainstorming & Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be9f63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
