{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "p1-title-md",
   "metadata": {},
   "source": [
    "# Feature Track 1: Evaluation & Validation\n",
    "\n",
    "---\n",
    "\n",
    "Shipping a RAG system without systematic evaluation is like navigating without instruments. The pipeline may *seem* to work on the queries you tested by hand, but you have no way to know where it breaks, how often, or whether a change you made helped or hurt.\n",
    "\n",
    "**Evaluation closes the feedback loop:**\n",
    "\n",
    "```\n",
    "Change a parameter  ──►  Measure quantitatively  ──►  Decide based on data\n",
    "```\n",
    "\n",
    "**Prerequisite:** Run `feature0_baseline_rag.ipynb` Steps 1–2 first to build the vector store.\n",
    "\n",
    "| Notebook | Focus |\n",
    "|---|---|\n",
    "| Feature 0 | Working baseline prototype |\n",
    "| **Feature Track 1 (this notebook)** | Quantitative evaluation |\n",
    "| Feature Track 2 | Reliable, structured outputs |\n",
    "| Feature Track 3 | Better retrieval strategies |\n",
    "| Feature Track 4 | Multi-step agent workflows |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-why-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Foundation\n",
    "\n",
    "### Why Systematic Evaluation?\n",
    "Suppose you change the chunk size from 800 to 400 characters. Did that help? How would you know?\n",
    "\n",
    "Without metrics you are forced to manually re-read answers for a handful of test queries and guess. With metrics you run the evaluation suite and get a number -> a number you can track across changes and use to justify decisions.\n",
    "\n",
    "### A Concrete Example from Feature 0\n",
    "In Feature 0 we saw that the baseline RAG sometimes:\n",
    "- Described the **Lara Pallet** as if it exists (it doesn't)\n",
    "- Cited the **outdated 2021 GWP figure** even though a newer, verified EPD supersedes it\n",
    "- Reported the tesa **68% CO₂ reduction** without flagging it as unverified\n",
    "\n",
    "These are not edge cases -> they are exactly the queries that matter for compliance.\n",
    "How often does this happen? After every change to the pipeline, you need an answer.\n",
    "\n",
    "### The Four Questions Evaluation Answers\n",
    "| Question | Why it matters |\n",
    "|---|---|\n",
    "| **Is the retriever finding the right chunks?** | A perfect LLM cannot fix wrong retrieval |\n",
    "| **Is the LLM hallucinating?** | A fabricated GWP figure can be shared with clients |\n",
    "| **Is the answer complete?** | Missing \"this is unverified\" can mislead a user |\n",
    "| **Did my change help?** | Without a baseline metric you cannot tell improvement from regression |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-pipeline-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The RAG Pipeline\n",
    "\n",
    "Each arrow is a potential failure point. Evaluation targets a specific stage so you can isolate *where* the problem is.\n",
    "\n",
    "```\n",
    "**Ingestion** (run once)\n",
    "\n",
    "  Documents  ──►  [1] Chunker  ──►  [2] Embedder  ──►  [3] Vector DB\n",
    "\n",
    "\n",
    "**Querying** (every user question)\n",
    "\n",
    "  User query  ──►  [2] Embedder  ──►  [3] Retriever  ──►  Top-k Chunks\n",
    "                                                                 │\n",
    "                                                          [4] LLM + Prompt\n",
    "                                                                 │\n",
    "                                                          Answer + Sources\n",
    "```\n",
    "\n",
    "| Step | What it does | If it fails |\n",
    "|---|---|---|\n",
    "| [1] Chunking | Split documents into searchable units | Context split mid-fact; tables broken; information lost |\n",
    "| [2] Embedding | Convert text to vectors | Wrong chunks returned despite a matching query |\n",
    "| [3] Vector search | Find most similar chunks to query | Relevant chunks not returned |\n",
    "| [4] Generation | LLM answers using retrieved context | Hallucination; ignores context; incomplete answer |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-map-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Stage-by-Stage Evaluation Map\n",
    "\n",
    "| Stage | What to measure |\n",
    "|---|---|\n",
    "| **Ingestion / Parsing** | Text completeness; table structure preserved; reading order correct in multi-column layouts |\n",
    "| **Chunking** | Chunk size distribution; percentage of chunks exceeding the embedding model's token limit |\n",
    "| **Embedding** | Similarity gap between a relevant and an irrelevant chunk for the same query |\n",
    "| **Vector search** | Fraction of queries where the correct chunk appears in the top-k results |\n",
    "| **Retrieved context** | Relevance of the retrieved chunks to the query |\n",
    "| **Faithfulness** | Fraction of answer claims that are directly supported by the retrieved context |\n",
    "| **Answer relevance** | Whether the answer addresses the actual question, not a related but different one |\n",
    "| **Answer correctness** | Factual accuracy of the answer compared to the known ground truth |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1-ragas-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAGAS\n",
    "\n",
    "[RAGAS](https://docs.ragas.io) (*Retrieval Augmented Generation Assessment*) is an\n",
    "open-source Python library for evaluating RAG pipelines. It is a widely adopted evaluation framework in the LLM/RAG ecosystem.\n",
    "\n",
    "#### How it works internally\n",
    "Rather than asking a judge LLM \"rate this answer 0–5\", RAGAS decomposes the answer into individual atomic claims:\n",
    "\n",
    "```\n",
    "Answer: \"The Logypal 1 GWP is 3.2 kg CO₂e, verified by Bureau Veritas.\"\n",
    "\n",
    "  Claim 1: \"GWP is 3.2 kg CO₂e\"          → supported by context?  ✓\n",
    "  Claim 2: \"verified by Bureau Veritas\"  → supported by context?  ✓\n",
    "\n",
    "  Faithfulness = 2 supported / 2 total = 1.0\n",
    "```\n",
    "\n",
    "This is more rigorous than a holistic score: it catches partial hallucination, e.g. a correct figure with a fabricated verifier name.\n",
    "\n",
    "#### Metrics at a glance\n",
    "| Metric | Ground truth? | What it catches |\n",
    "|---|---|---|\n",
    "| `Faithfulness` | No | Claims not supported by the retrieved context |\n",
    "| `AnswerRelevancy` | No | Off-topic or evasive answers |\n",
    "| `AnswerCorrectness` | Yes | Wrong or missing facts vs. the reference answer |\n",
    "| `ContextPrecision` | Yes | Irrelevant chunks ranked above relevant ones |\n",
    "\n",
    "#### Strengths\n",
    "- Standardised, reproducible metrics widely used in industry\n",
    "- `Faithfulness` and `AnswerRelevancy` require zero labelling effort\n",
    "- Claim-level decomposition is more rigorous than holistic scoring\n",
    "\n",
    "#### Weaknesses\n",
    "- Needs a capable judge LLM -> RAGAS defaults to OpenAI (requires `OPENAI_API_KEY`)\n",
    "- LLM judge has its own biases; may be lenient on confident-sounding hallucinations\n",
    "- Slow and costly at scale: ~3 LLM calls per sample per metric\n",
    "- Metrics are proxies, not ground truth: score 0.9 ≠ 90% of answers are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p2-setup-md",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### First Look at RAGAS\n",
    "\n",
    "#### Setup\n",
    "\n",
    "**Prerequisites:** `conversational-toolkit` and `backend` installed in editable mode. Vector store must already exist -> run `feature0_baseline_rag.ipynb` Steps 1–2 first.\n",
    "\n",
    "RAGAS uses OpenAI as its judge LLM by default ->`OPENAI_API_KEY` must be set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/vr2d5w_x7z783sr5x3959rh80000gn/T/ipykernel_78672/1062740565.py:8: DeprecationWarning: Importing Faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import Faithfulness\n",
      "  from ragas.metrics import (\n",
      "/var/folders/wc/vr2d5w_x7z783sr5x3959rh80000gn/T/ipykernel_78672/1062740565.py:8: DeprecationWarning: Importing AnswerRelevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerRelevancy\n",
      "  from ragas.metrics import (\n",
      "/var/folders/wc/vr2d5w_x7z783sr5x3959rh80000gn/T/ipykernel_78672/1062740565.py:8: DeprecationWarning: Importing AnswerCorrectness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerCorrectness\n",
      "  from ragas.metrics import (\n",
      "2026-02-23 22:52:14.372 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:__init__:57 - Sentence Transformer embeddings model loaded: sentence-transformers/all-MiniLM-L6-v2 with kwargs: {}\n",
      "2026-02-23 22:52:14.439 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_llm:135 - LLM backend: OpenAI (gpt-4o-mini)\n",
      "2026-02-23 22:52:14.452 | DEBUG    | conversational_toolkit.llms.openai:__init__:63 - OpenAI LLM loaded: gpt-4o-mini; temperature: 0.3; seed: 42; tools: None; tool_choice: None; response_format: {'type': 'text'}\n",
      "2026-02-23 22:52:14.453 | INFO     | sme_kt_zh_collaboration_rag.feature0_baseline_rag:build_agent:332 - RAG agent ready (top_k=5  query_expansion=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model : sentence-transformers/all-MiniLM-L6-v2\n",
      "Vector store    : /Users/pkoerner/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/backend/data_vs.db\n",
      "RAG agent LLM   : openai\n",
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "from conversational_toolkit.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from conversational_toolkit.evaluation import Evaluator\n",
    "from conversational_toolkit.evaluation.adapters import evaluate_with_ragas\n",
    "from conversational_toolkit.vectorstores.chromadb import ChromaDBVectorStore\n",
    "from ragas.metrics import (\n",
    "    Faithfulness as RagasFaithfulness,\n",
    "    AnswerRelevancy as RagasAnswerRelevancy,\n",
    ")\n",
    "\n",
    "from sme_kt_zh_collaboration_rag.feature0_baseline_rag import (\n",
    "    EMBEDDING_MODEL,\n",
    "    VS_PATH,\n",
    "    SYSTEM_PROMPT,\n",
    "    build_llm,\n",
    "    build_agent,\n",
    ")\n",
    "from sme_kt_zh_collaboration_rag.feature1_evaluation import EVALUATION_QUERIES\n",
    "\n",
    "RETRIEVER_TOP_K = 5\n",
    "BACKEND = \"openai\"  # \"ollama\"  or  \"openai\"\n",
    "# Note: RAGAS uses OpenAI for its judge LLM regardless of BACKEND above.\n",
    "\n",
    "if not BACKEND:\n",
    "    raise ValueError('Set BACKEND to \"ollama\" or \"openai\" before running.')\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "vs = ChromaDBVectorStore(db_path=str(VS_PATH))\n",
    "llm = build_llm(backend=BACKEND)\n",
    "agent = build_agent(\n",
    "    vector_store=vs,\n",
    "    embedding_model=embedding_model,\n",
    "    llm=llm,\n",
    "    top_k=RETRIEVER_TOP_K,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    number_query_expansion=0,\n",
    ")\n",
    "print(f\"Embedding model : {EMBEDDING_MODEL}\")\n",
    "print(f\"Vector store    : {VS_PATH}\")\n",
    "print(f\"RAG agent LLM   : {BACKEND}\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d8fdd",
   "metadata": {},
   "source": [
    "We start with the two metrics that need no ground truth:\n",
    "\n",
    "- **Faithfulness**: are all claims in the answer supported by the retrieved context?\n",
    "- **AnswerRelevancy**: does the answer directly address the question?\n",
    "\n",
    "The `evaluate_with_ragas()` adapter converts our `EvaluationSample` objects to RAGAS format, calls the judge LLM, and returns an `EvaluationReport`.\n",
    "\n",
    "*Takes ~2–3 minutes: RAGAS makes multiple judge LLM calls per sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e46e5961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 5 evaluation samples (runs the RAG agent once per query)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-23 23:03:48.380 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:03:49.681 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:03:53.091 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:03:57.036 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n",
      "2026-02-23 23:04:01.352 | DEBUG    | conversational_toolkit.embeddings.sentence_transformer:get_embeddings:76 - sentence-transformers/all-MiniLM-L6-v2 embeddings size: (1, 384)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 5 samples built.\n",
      "\n",
      "Running RAGAS: Faithfulness + AnswerRelevancy  (~2-3 min)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49203d84262e47339a243bf51cc63f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[1]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[3]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[5]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[7]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[9]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot perform reduction 'mean' with string dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples built.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning RAGAS: Faithfulness + AnswerRelevancy  (~2-3 min)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m report_basic = \u001b[43mevaluate_with_ragas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m=\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mRagasFaithfulness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mRagasAnswerRelevancy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m─\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m48\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Samples evaluated : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_basic.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Kanton_Zurich/sme-kt-zh-collaboration-rag/conversational-toolkit/src/conversational_toolkit/evaluation/adapters/ragas_adapter.py:133\u001b[39m, in \u001b[36mevaluate_with_ragas\u001b[39m\u001b[34m(samples, metrics, llm, embeddings, multiturn)\u001b[39m\n\u001b[32m    127\u001b[39m ragas_result = ragas.evaluate(**kwargs)\n\u001b[32m    128\u001b[39m scores_df = ragas_result.to_pandas() \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    130\u001b[39m metric_results: \u001b[38;5;28mlist\u001b[39m[MetricResult] = [\n\u001b[32m    131\u001b[39m     MetricResult(\n\u001b[32m    132\u001b[39m         metric_name=col,\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         score=\u001b[38;5;28mfloat\u001b[39m(\u001b[43mscores_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[32m    134\u001b[39m         per_sample_scores=[\u001b[38;5;28mfloat\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m scores_df[col].tolist()],\n\u001b[32m    135\u001b[39m     )\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m scores_df.columns\n\u001b[32m    137\u001b[39m ]\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m EvaluationReport(results=metric_results, num_samples=\u001b[38;5;28mlen\u001b[39m(samples))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Kanton_Zurich/rag_venv/lib/python3.13/site-packages/pandas/util/_decorators.py:336\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    331\u001b[39m     warnings.warn(\n\u001b[32m    332\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    333\u001b[39m         klass,\n\u001b[32m    334\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    335\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Kanton_Zurich/rag_venv/lib/python3.13/site-packages/pandas/core/series.py:8113\u001b[39m, in \u001b[36mSeries.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m   8063\u001b[39m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(Pandas4Warning, allowed_args=[\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m], name=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   8064\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m   8065\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   8069\u001b[39m     **kwargs,\n\u001b[32m   8070\u001b[39m ) -> Any:\n\u001b[32m   8071\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   8072\u001b[39m \u001b[33;03m    Return the mean of the values over the requested axis.\u001b[39;00m\n\u001b[32m   8073\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   8111\u001b[39m \u001b[33;03m    2.0\u001b[39;00m\n\u001b[32m   8112\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m8113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   8114\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   8115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Kanton_Zurich/rag_venv/lib/python3.13/site-packages/pandas/core/generic.py:11831\u001b[39m, in \u001b[36mNDFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11823\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  11824\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  11825\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11829\u001b[39m     **kwargs,\n\u001b[32m  11830\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m11831\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  11832\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  11833\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Kanton_Zurich/rag_venv/lib/python3.13/site-packages/pandas/core/generic.py:11785\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11781\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  11783\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m11785\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  11786\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  11787\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Kanton_Zurich/rag_venv/lib/python3.13/site-packages/pandas/core/series.py:7480\u001b[39m, in \u001b[36mSeries._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m   7476\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_axis_number(axis)\n\u001b[32m   7478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delegate, ExtensionArray):\n\u001b[32m   7479\u001b[39m     \u001b[38;5;66;03m# dispatch to ExtensionArray interface\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m7480\u001b[39m     result = \u001b[43mdelegate\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7482\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7483\u001b[39m     \u001b[38;5;66;03m# dispatch to numpy arrays\u001b[39;00m\n\u001b[32m   7484\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m numeric_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype.kind \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miufcb\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   7485\u001b[39m         \u001b[38;5;66;03m# i.e. not is_numeric_dtype(self.dtype)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Kanton_Zurich/rag_venv/lib/python3.13/site-packages/pandas/core/arrays/string_arrow.py:556\u001b[39m, in \u001b[36mArrowStringArray._reduce\u001b[39m\u001b[34m(self, name, skipna, keepdims, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._reduce_calc(name, skipna=skipna, keepdims=keepdims, **kwargs)\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot perform reduction \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m with string dtype\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33margmin\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33margmax\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, pa.Array):\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._convert_int_result(result)\n",
      "\u001b[31mTypeError\u001b[39m: Cannot perform reduction 'mean' with string dtype"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"Does PrimePack AG offer a product called the Lara Pallet?\",\n",
    "    \"Which products in the portfolio have a third-party verified EPD?\",\n",
    "    \"Can the 68% CO2 reduction claim for tesapack ECO (product 50-102) be included in a customer sustainability response?\",\n",
    "    \"Are any tape products confirmed to be PFAS-free?\",\n",
    "    \"Which suppliers are not yet compliant with the EPD requirement by end of 2025?\",\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"Building {len(queries)} evaluation samples (runs the RAG agent once per query)...\"\n",
    ")\n",
    "samples = await Evaluator.build_samples_from_agent(agent=agent, queries=queries)\n",
    "print(f\"Done. {len(samples)} samples built.\\n\")\n",
    "\n",
    "\n",
    "print(\"Running RAGAS: Faithfulness + AnswerRelevancy  (~2-3 min)\\n\")\n",
    "\n",
    "report_basic = evaluate_with_ragas(\n",
    "    samples=samples,\n",
    "    metrics=[\n",
    "        RagasFaithfulness(),\n",
    "        RagasAnswerRelevancy(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"─\" * 48)\n",
    "print(f\"  Samples evaluated : {report_basic.num_samples}\")\n",
    "print(\"─\" * 48)\n",
    "for metric_name, score in report_basic.summary().items():\n",
    "    bar = \"█\" * int(score * 20) + \"░\" * (20 - int(score * 20))\n",
    "    print(f\"  {metric_name:<22}  {score:.3f}  {bar}\")\n",
    "print(\"─\" * 48)\n",
    "print()\n",
    "print(\"Faithfulness    < 0.7  → investigate hallucination\")\n",
    "print(\"AnswerRelevancy < 0.7  → answer drifts off-topic or evades the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-sample breakdown -> find which queries score worst\n",
    "from collections import defaultdict\n",
    "\n",
    "f_result = next(\n",
    "    (r for r in report_basic.results if \"faithfulness\" in r.metric_name.lower()), None\n",
    ")\n",
    "a_result = next(\n",
    "    (\n",
    "        r\n",
    "        for r in report_basic.results\n",
    "        if \"relevancy\" in r.metric_name.lower() or \"relevance\" in r.metric_name.lower()\n",
    "    ),\n",
    "    None,\n",
    ")\n",
    "\n",
    "f_scores = f_result.per_sample_scores or []\n",
    "a_scores = a_result.per_sample_scores or []\n",
    "\n",
    "print(\"Per-sample scores  (F = Faithfulness,  A = AnswerRelevancy)\\n\")\n",
    "print(f\"{'#':<3} {'F':>6} {'A':>6}  {'diff':<7} {'category':<22}  query\")\n",
    "print(\"─\" * 95)\n",
    "for i, (q_info, f, a) in enumerate(zip(EVALUATION_QUERIES, f_scores, a_scores), 1):\n",
    "    flag = \" ◄ low\" if f < 0.7 else \"\"\n",
    "    print(\n",
    "        f\"{i:<3} {f:>6.2f} {a:>6.2f}  {q_info['difficulty']:<7} {q_info['category']:<22}  {q_info['query'][:36]!r}{flag}\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "cat_scores: dict = defaultdict(list)\n",
    "for q_info, f, a in zip(EVALUATION_QUERIES, f_scores, a_scores):\n",
    "    cat_scores[q_info[\"category\"]].append((f, a))\n",
    "print(\"Category averages:\")\n",
    "for cat, scores_list in sorted(cat_scores.items()):\n",
    "    avg_f = sum(s[0] for s in scores_list) / len(scores_list)\n",
    "    avg_a = sum(s[1] for s in scores_list) / len(scores_list)\n",
    "    print(f\"  {cat:<22}  F={avg_f:.2f}  A={avg_a:.2f}  (n={len(scores_list)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f3a3b",
   "metadata": {},
   "source": [
    "## Brainstorming & (proposed) tasks\n",
    "\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
